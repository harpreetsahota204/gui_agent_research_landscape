# GUI Agent Research Landscape

A comprehensive analysis of the research landscape in GUI agents, OS agents, and visual agents, spanning from 2016 to 2025. This repository contains a complete research pipeline for extracting, analyzing, and understanding trends in this rapidly evolving field.

## Overview

This project provides a systematic approach to understanding the development and evolution of GUI agent research through data extraction, intelligent filtering, LLM-powered analysis, and comprehensive visualization. The analysis reveals key insights about research trends, influential papers, citation networks, and the field's trajectory.

## Data Sources

### Initial Dataset

The foundation of this analysis comes from the excellent work by the [LLM-Brained GUI Agents Survey](https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey) project. The initial dataset in the `data/` directory contains four categories of papers:

- **`benchmark.json`** - Papers introducing new evaluation benchmarks
- **`dataset.json`** - Papers contributing new datasets  
- **`models.json`** - Papers presenting new models and architectures
- **`survey.json`** - Survey and review papers

Each paper entry includes metadata such as title, platform, publication date, arXiv URL, key highlights, and code repository links.

**Credit**: We gratefully acknowledge [@vyokky](https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey) for curating and maintaining this valuable dataset of GUI agent research papers.

## Research Pipeline

### 1. Data Extraction and Expansion (`data_extractor.py`)

The initial dataset was significantly expanded through automated extraction from arXiv:

- **Multi-level Citation Mining**: Starting from the curated papers (Level 1), we extracted their references to identify relevant cited works (Level 2), and then extracted references from those papers (Level 3)

- **Content Extraction**: For each paper, we extracted full text content including abstracts, introductions, methodology sections, and conclusions from arXiv HTML pages

- **Metadata Enrichment**: Enhanced each paper with author information, publication dates, platform classifications, and research categories

- **Citation Network Construction**: Built comprehensive forward and backward citation relationships between papers

- **Special Paper Integration**: Included important non-arXiv papers like Rico and ERICA datasets that are frequently cited in the field

This process expanded the dataset from ~180 curated papers to over 9,000 papers with rich metadata and full content.

### 2. Intelligent Filtering (`filter_papers.py`)

To maintain focus on GUI/OS/Visual agent research, we implemented a sophisticated keyword-based filtering system:

- **Multi-tier Keyword Classification**: Used high-precision keywords (GUI agent, visual grounding, screenshot analysis) and medium-precision keywords (visual agent, computer control) with different scoring weights

- **Context-aware Filtering**: Enhanced relevance detection by considering keyword co-occurrence and context

- **Exclusion Patterns**: Filtered out papers from unrelated domains (medical, financial, pure NLP) to maintain research focus 

- **Level-based Strategy**: Kept all Level 1 (curated) papers while applying filtering to Level 2 and 3 papers to balance comprehensiveness with relevance

This reduced the dataset to approximately 600 highly relevant papers while maintaining the core research landscape.

### 3. LLM-Powered Content Analysis (`paper_analyzer.py`)

Each paper was analyzed using **Qwen3-8B** to extract structured insights:

- **Key Contributions Extraction**: Identified the main research themes and contributions from abstracts, introductions, and conclusions

- **Innovation Classification**: Automatically detected whether papers introduce new models, datasets, benchmarks, or frameworks

- **Contribution Analysis**: Analyzed how each work differs from and builds upon related work

- **Batch Processing**: Optimized GPU utilization through efficient batch processing for scalable analysis

- **Quality Assurance**: Implemented fallback mechanisms and error handling to ensure robust analysis

This enriched each paper with structured metadata about its contributions and innovations.

### 4. Temporal Trend Analysis (`research_trend_analyzer.py`)

Research trends were analyzed across different time periods using **Qwen3-14B**:

- **Time Period Segmentation**: Organized papers into meaningful periods (2016-2021 Early Era, 2022 Growth Year, 2023+ Quarterly analysis)

- **Trend Identification**: For each period, identified dominant themes, key innovations, emerging trends, and research gaps

- **Methodological Evolution**: Tracked changes in research approaches and methodologies over time

- **Future Direction Prediction**: Generated insights about where the field might be heading based on current trends

- **Cross-period Analysis**: Compared different eras to understand the field's evolution

This provided a comprehensive understanding of how GUI agent research has evolved and where it's headed.

### 5. Timeline and Temporal Analysis (`research_timeline_analysis.py`)

Comprehensive temporal analysis with multiple visualization approaches:

- **Publication Timeline**: Tracked paper publication rates over time with trend projections for future years

- **Platform Evolution**: Analyzed how research focus has shifted across different platforms (Web, Mobile, Desktop, etc.)

- **Innovation Trends**: Examined the temporal distribution of different types of contributions (models, datasets, benchmarks)

- **Research Acceleration**: Identified periods of rapid growth and analyzed sustainability indicators

- **Granular Analysis**: Provided quarterly-level analysis for recent years to capture fine-grained trends

Generated multiple visualizations showing the field's rapid growth, particularly accelerating after 2022.

### 6. Citation Network Analysis (`citation_network_analysis.py`)

Deep analysis of the citation network and research influence patterns:

- **Network Construction**: Built a comprehensive citation graph connecting papers through their references

- **Influence Metrics**: Calculated various influence measures including citation counts, PageRank scores, and network centrality

- **Foundation vs. Frontier Papers**: Distinguished between foundational works that established the field and frontier papers pushing current boundaries  

- **Citation Burst Detection**: Identified papers experiencing rapid citation growth, indicating emerging influence

- **Research Bridge Analysis**: Found papers that connect different research communities or approaches

- **Temporal Dynamics**: Analyzed how influence patterns change over time and identified paradigm shifts

This revealed the field's key influential works, emerging stars, and the structure of research communities.

## Key Findings

- **Rapid Growth**: The field has experienced exponential growth, particularly since 2022, with 2024 seeing unprecedented research activity

- **Platform Diversification**: Research has expanded from early web-focused work to encompass mobile, desktop, and cross-platform approaches

- **Innovation Patterns**: Strong emphasis on new models and architectures, with increasing focus on comprehensive benchmarks and datasets

- **Emerging Trends**: Growing interest in multimodal approaches, real-world deployment, and foundation models for GUI understanding

- **Research Maturation**: Evolution from proof-of-concept work to production-ready systems and comprehensive evaluation frameworks


## Citation Network Analysis Results

The `citation_network_analysis.py` script performs comprehensive analysis of the citation network among GUI agent research papers, revealing influential works, research patterns, and field evolution. The analysis generates multiple specialized tables and visualizations.

### üìä Analysis Outputs

The citation network analysis produces the following key tables and insights:

#### **Core Influence Rankings**
- **[Most Cited Papers](citation_analysis/most_cited_papers.md)** - Top papers by raw citation count, showing the most referenced works in the field

- **[Influential Papers Ranking](citation_analysis/influential_papers_ranking.md)** - PageRank-based influence ranking that considers network effects and citation quality

- **[Foundation Papers](citation_analysis/foundation_papers.md)** - Papers that established key concepts and methodologies in GUI agent research

- **[Frontier Papers](citation_analysis/frontier_papers.md)** - Recent papers pushing the boundaries of current research

#### **Research Infrastructure Analysis**

- **[Benchmark Papers](citation_analysis/benchmarks_papers.md)** - Papers introducing evaluation benchmarks and testing frameworks

- **[Dataset Papers](citation_analysis/datasets_papers.md)** - Papers contributing datasets for training and evaluation

- **[Model Papers](citation_analysis/models_papers.md)** - Papers introducing new models and architectures (GUI agent-specific)

#### **Temporal Dynamics**

- **[Temporal Analysis](citation_analysis/temporal_analysis_simple.md)** - Papers with dynamic citation patterns and recent momentum

- **[Citation Velocity](citation_analysis/citation_velocity_papers.md)** - Papers with high citation rates and sustained impact

- **[Future Impact Signals](citation_analysis/future_impact_signals_papers.md)** - Papers showing early indicators of potential high impact

#### **Visualizations**
- **[Research Timeline](citation_analysis/influential_papers_timeline.png)** - Visual timeline showing the evolution of influential papers over time

### üîç Key Insights from Citation Analysis

The citation network analysis reveals several important patterns:

#### **Field Evolution**

- **Foundation Era (2016-2021)**: Established by datasets like Rico and ERICA, and early GUI understanding frameworks

- **Growth Phase (2022-2023)**: Rapid expansion with new benchmarks like Mind2Web, WebArena, and WebShop

- **Current Wave (2024-2025)**: Focus on production-ready agents, multimodal approaches, and comprehensive evaluation

#### **Most Influential Works**

The analysis identifies papers with the highest network influence (combining citations and network centrality):

- **Rico Dataset (2017)**: Foundational mobile app dataset enabling data-driven GUI research

- **Mind2Web (2023)**: Breakthrough web agent benchmark that catalyzed current research wave

- **WebArena (2023)**: Comprehensive web environment for autonomous agent evaluation

- **Recent Multimodal Models**: Growing influence of vision-language models adapted for GUI tasks

#### **Research Patterns**

- **Citation Bursts**: Certain papers experience rapid citation growth following major conferences or breakthroughs

- **Cross-Platform Influence**: Papers spanning multiple platforms (web, mobile, desktop) tend to have higher influence

- **Benchmark-Driven Growth**: Introduction of new benchmarks consistently drives citation activity and follow-up research

#### **Emerging Trends**

- **Multimodal Integration**: Increasing focus on combining vision and language for GUI understanding

- **Real-world Deployment**: Shift from proof-of-concept to production-ready agent systems

- **Evaluation Sophistication**: More comprehensive and realistic evaluation frameworks

- **Foundation Model Adaptation**: Leveraging and fine-tuning large language models for GUI tasks

### üìà Network Statistics

The citation network analysis processes:

- **Total Papers Analyzed**: 600+ filtered and enriched papers

- **Citation Relationships**: Comprehensive forward and backward citation links

- **Time Span**: Research from 2016 to 2025

- **Platform Coverage**: Web, Mobile, Desktop, Android, iOS, and cross-platform research

### üéØ How to Use These Results

- **For Literature Review**: Start with Foundation Papers and Most Cited Papers for comprehensive background

- **For Current Trends**: Focus on Frontier Papers and Temporal Analysis for cutting-edge research

- **For Benchmarking**: Reference Benchmark Papers for evaluation frameworks and comparison baselines

- **For Implementation**: Check Model Papers and Dataset Papers for practical resources

- **For Future Research**: Review Future Impact Signals for emerging opportunities

### üîÑ Analysis Methodology

The citation network analysis employs several sophisticated metrics:

- **PageRank Algorithm**: Measures influence based on citation network structure

- **Temporal Pattern Detection**: Identifies citation bursts and momentum changes

- **Cross-Platform Analysis**: Evaluates papers spanning multiple research domains

- **Foundation vs. Frontier Classification**: Distinguishes established works from cutting-edge research

All tables include arXiv badges for direct access to papers, along with key contributions and innovation summaries extracted via LLM analysis.

---
## Repository Structure

```
‚îú‚îÄ‚îÄ data/                          # Initial curated dataset
‚îÇ   ‚îú‚îÄ‚îÄ benchmark.json            # Benchmark papers
‚îÇ   ‚îú‚îÄ‚îÄ dataset.json             # Dataset papers  
‚îÇ   ‚îú‚îÄ‚îÄ models.json              # Model papers
‚îÇ   ‚îî‚îÄ‚îÄ survey.json              # Survey papers
‚îú‚îÄ‚îÄ src/                         # Source code directory
‚îÇ   ‚îú‚îÄ‚îÄ data_extractor.py        # ArXiv data extraction and expansion
‚îÇ   ‚îú‚îÄ‚îÄ filter_papers.py         # Intelligent paper filtering
‚îÇ   ‚îú‚îÄ‚îÄ paper_analyzer.py        # LLM-powered content analysis  
‚îÇ   ‚îú‚îÄ‚îÄ research_trend_analyzer.py # Temporal trend analysis
‚îÇ   ‚îú‚îÄ‚îÄ research_timeline_analysis.py # Timeline visualization
‚îÇ   ‚îî‚îÄ‚îÄ citation_network_analysis.py # Citation network analysis
‚îî‚îÄ‚îÄ README.md                    # This file
```


## Usage

1. **Data Extraction**: Run `data_extractor.py` to expand the dataset from arXiv
2. **Filtering**: Use `filter_papers.py` to focus on relevant GUI agent research
3. **Content Analysis**: Apply `paper_analyzer.py` for LLM-powered insights
4. **Trend Analysis**: Execute `research_trend_analyzer.py` for temporal trends
5. **Visualization**: Run timeline and network analysis scripts for comprehensive insights

Each script includes detailed command-line options and can be run independently or as part of the full pipeline.

## Requirements

- Python 3.8+
- PyTorch with CUDA support (for LLM analysis)
- Transformers library (HuggingFace)
- NetworkX, Matplotlib, Seaborn (for analysis and visualization)
- BeautifulSoup, Requests (for web scraping)

## Citation

If you find this research landscape analysis helpful for your work, please consider citing:

```bibtex
@misc{sahota2025gui,
  title={GUI Agent Research Landscape: A Comprehensive Analysis of Trends, Networks, and Evolution},
  author={Sahota, Harpreet},
  year={2025},
  url={https://github.com/harpreetsahota204/gui_agent_research_landscape},
  note={Comprehensive analysis of GUI agent research from 2016-2025}
}
```

## Acknowledgments

- **Original Dataset**: [@vyokky](https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey) for the foundational curated dataset

- **LLM Analysis**: Powered by Qwen3-8B and Qwen3-14B models from Alibaba Cloud

- **Research Community**: All the researchers whose work is analyzed in this landscape study

## License

This project is licensed under the Apache 2.0 License - see the LICENSE file for details.

## Papers by Time Period

The following tables organize all papers in the dataset by time periods, showing their key contributions and innovations.

**Total Papers**: 609

**Papers by Period**:
- 2016-2021: Early Era: 28 papers
- 2022: Growth Year: 39 papers
- 2023: Q1: 20 papers
- 2023: Q2: 36 papers
- 2023: Q3: 29 papers
- 2023: Q4: 62 papers
- 2024: Q1: 71 papers
- 2024: Q2: 84 papers
- 2024: Q3: 56 papers
- 2024: Q4: 74 papers
- 2025: Q1: 71 papers
- 2025: Q2: 39 papers


### 2016-2021: Early Era

| arXiv | Title | Summary | Contributions |
|-------|-------|---------|---------------|
| [![arXiv](https://img.shields.io/badge/arXiv-2112.09332-b31b1b.svg)](https://arxiv.org/abs/2112.09332) | WebGPT: Browser-assisted question-answering with human feedback | The paper introduces a browser-assisted question-answering system that integrates web browsing with GPT-3, utilizing imitation learning and human feed... | This work differs from related work by explicitly combining web-browsing capabilities with large language models, leveraging human feedback for iterat... |
| [![arXiv](https://img.shields.io/badge/arXiv-2010.03768-b31b1b.svg)](https://arxiv.org/abs/2010.03768) | ALFWorld: Aligning Text and Embodied Environments for Interactive Learning | ALFWorld bridges abstract text-based policy learning and concrete visual execution, introducing the BUTLER agent that leverages pre-learned abstract k... | Unlike prior work focused on either abstract reasoning or visual execution, ALFWorld combines both through a unified simulator, enabling agents to tra... |
| [![arXiv](https://img.shields.io/badge/arXiv-2005.03776v2-b31b1b.svg)](https://arxiv.org/abs/2005.03776) | Mapping Natural Language Instructions to Mobile UI Action Sequences | The paper introduces a novel problem of grounding natural language instructions to mobile UI actions, develops three new datasets including PIXELHELP,... | This work differs from related work by focusing specifically on mobile UI interaction, creating task-specific datasets with grounded action sequences,... |
| [![arXiv](https://img.shields.io/badge/arXiv-1802.08802v1-b31b1b.svg)](https://arxiv.org/abs/1802.08802) | Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration | The paper introduces workflow-guided exploration for reinforcement learning on web interfaces, using high-level workflows derived from expert demonstr... | This work differs from related work by using workflow-guided exploration to constrain action sequences with expert demonstrations, rather than relying... |
| [![arXiv](https://img.shields.io/badge/arXiv-2105.13231v1-b31b1b.svg)](https://arxiv.org/abs/2105.13231) | AndroidEnv: A Reinforcement Learning Platform for Android | The paper introduces AndroidEnv, a reinforcement learning platform for Android that enables agents to interact with real-world apps via a touchscreen... | AndroidEnv differs from related work by providing a realistic Android simulation environment tailored for RL research, enabling direct interaction wit... |
| [![arXiv](https://img.shields.io/badge/arXiv-2107.13731-b31b1b.svg)](https://arxiv.org/abs/2107.13731) | UIBert: Learning Generic Multimodal Representations for UI Understanding | The paper introduces UIBert, a transformer-based model for UI understanding that leverages self-aligned multimodal features (image, text, structural m... | Unlike prior work, UIBert utilizes self-alignment between UI component features and proposes five pre-training tasks to learn generic representations... |
| [![arXiv](https://img.shields.io/badge/arXiv-2101.09465-b31b1b.svg)](https://arxiv.org/abs/2101.09465) | WebSRC: A Dataset for Web-Based Structural Reading Comprehension | The paper introduces WebSRC, a novel dataset for structural reading comprehension on web pages, emphasizing the need for systems to understand both se... | This work differs from related work by focusing on structural understanding of web pages, providing a comprehensive dataset with HTML, screenshots, an... |
| [![arXiv](https://img.shields.io/badge/arXiv-2111.15664-b31b1b.svg)](https://arxiv.org/abs/2111.15664) | OCR-free Document Understanding Transformer | The paper introduces an OCR-free approach for document understanding using a Transformer model (Donut), addressing limitations of OCR-based methods by... | The work differs from related OCR-based VDU methods by eliminating reliance on OCR engines, using a Transformer with cross-entropy loss for end-to-end... |
| [![arXiv](https://img.shields.io/badge/arXiv-2009.12293-b31b1b.svg)](https://arxiv.org/abs/2009.12293) | robosuite: A Modular Simulation Framework and Benchmark for Robot Learning | The paper introduces robosuite, a modular simulation framework and benchmark for robot learning, leveraging the MuJoCo physics engine. Key contributio... | robosuite differs from related work by providing a modular framework with standardized benchmarks and task environments, enabling rigorous evaluation... |
| [![arXiv](https://img.shields.io/badge/arXiv-2007.00398-b31b1b.svg)](https://arxiv.org/abs/2007.00398) | DocVQA: A Dataset for VQA on Document Images | The paper introduces DocVQA, a specialized dataset for Visual Question Answering on document images, emphasizing structural understanding challenges.... | DocVQA differs from related work by focusing specifically on document images, requiring models to understand document structure, and providing a bench... |
| [![arXiv](https://img.shields.io/badge/arXiv-2109.08238-b31b1b.svg)](https://arxiv.org/abs/2109.08238) | Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embod... | The paper introduces HM3D, a large-scale 3D dataset with 1,000 building-scale environments, emphasizing its superior physical scale, visual fidelity,... | HM3D differs from related work by offering significantly larger navigable space (112.5k m¬≤), higher visual fidelity (20-85% improvement over Replica/M... |
| [![arXiv](https://img.shields.io/badge/arXiv-2007.04954-b31b1b.svg)](https://arxiv.org/abs/2007.04954) | ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation | ThreeDWorld introduces a platform for interactive multi-modal physical simulation with high-fidelity rendering, customizable agents, generative enviro... | Unlike prior work, ThreeDWorld provides a unified platform with real-time physics simulation, multi-modal sensory data, and customizable agents, enabl... |
| [![arXiv](https://img.shields.io/badge/arXiv-2109.11797-b31b1b.svg)](https://arxiv.org/abs/2109.11797) | CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models | The paper introduces Cross-modal Prompt Tuning (CPT) to address the gap between pre-training and fine-tuning objectives in Vision-Language Pre-trained... | Unlike traditional fine-tuning approaches that require large labeled datasets, CPT leverages color-based prompt tuning to bridge the pre-training and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2011.01975-b31b1b.svg)](https://arxiv.org/abs/2011.01975) | Rearrangement: A Challenge for Embodied AI | The paper introduces a framework for Embodied AI research centered on the rearrangement task, emphasizing standardized metrics, scenario characterizat... | This work differs from related work by establishing a standardized rearrangement task with formal metrics and simulation testbeds, enabling reproducib... |
| [![arXiv](https://img.shields.io/badge/arXiv-2103.16057v2-b31b1b.svg)](https://arxiv.org/abs/2103.16057) | Grounding Open-Domain Instructions to Automate Web Support Tasks | The paper introduces RUSS, a system that grounds open-domain natural language instructions into web actions using a domain-specific language (ThingTal... | This work differs from related work by introducing ThingTalk as an intermediate domain-specific language for grounding instructions, enabling more pre... |
| [![arXiv](https://img.shields.io/badge/arXiv-1812.11470-b31b1b.svg)](https://arxiv.org/abs/1812.11470) | A Systematic Literature Review of Automated Techniques for Functional GUI Testin... | The paper provides a systematic review of automated GUI testing techniques for mobile applications, analyzing their effectiveness, efficiency, and pra... | This work differs from related studies by conducting a comprehensive systematic review of 25 primary studies to evaluate and compare existing automate... |
| [![arXiv](https://img.shields.io/badge/arXiv-2110.08518-b31b1b.svg)](https://arxiv.org/abs/2110.08518) | MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Un... | The paper introduces MarkupLM, a pre-training model that jointly learns text and markup language information for document understanding, addressing th... | This work differs from related work by using markup languages as the backbone for pre-training, rather than relying on fixed layouts or images. It exp... |
| [![arXiv](https://img.shields.io/badge/arXiv-2110.08653-b31b1b.svg)](https://arxiv.org/abs/2110.08653) | Learning UI Navigation through Demonstrations composed of Macro Actions | The paper introduces a framework for UI navigation that simplifies state and action spaces using UI elements and macro actions. It proposes demo augme... | This work differs from related work by focusing on macro-action-based navigation with demo augmentation and customized DQfD, enabling efficient traini... |
| [![arXiv](https://img.shields.io/badge/arXiv-2108.03272-b31b1b.svg)](https://arxiv.org/abs/2108.03272) | iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household... | iGibson 2.0 introduces object-centric simulation with enhanced object states (temperature, wetness, cleanliness, toggled/sliced), predicate logic func... | Unlike prior simulation environments focused on motion and physics, iGibson 2.0 extends capabilities to handle complex object states and semantic task... |
| [![arXiv](https://img.shields.io/badge/arXiv-1909.01871-b31b1b.svg)](https://arxiv.org/abs/1909.01871) | Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospecti... | The paper introduces HANNA, a photo-realistic simulator for visual navigation tasks with human-like assistance. It proposes a memory-augmented neural... | This work differs from related work by integrating multimodal assistance (language + vision) with retrospective curiosity-driven imitation learning, h... |
| [![arXiv](https://img.shields.io/badge/arXiv-1705.07962-b31b1b.svg)](https://arxiv.org/abs/1705.07962) | pix2code: Generating Code from a Graphical User Interface Screenshot | The paper introduces an end-to-end deep learning model that generates code from GUI screenshots with high accuracy across multiple platforms. It addre... | This work differs from related work by leveraging deep learning for direct code generation from screenshots, whereas prior methods often relied on man... |
| [![arXiv](https://img.shields.io/badge/arXiv-2105.11941-b31b1b.svg)](https://arxiv.org/abs/2105.11941) | Understanding Mobile GUI: from Pixel-Words to Screen-Sentences | This paper introduces a vision-based approach for mobile GUI understanding by abstracting visual elements into 'Pixel-Words' and structuring them into... | Unlike prior works dependent on human-created metadata (e.g., View Hierarchy), this work defines atomic visual components (Pixel-Words) and employs a... |
| [![arXiv](https://img.shields.io/badge/arXiv-2108.03353-b31b1b.svg)](https://arxiv.org/abs/2108.03353) | Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning | The paper introduces Screen2Words, a multi-modal approach for mobile UI summarization that integrates text, images, UI structures, and semantics. It p... | The work differs from related work by combining multi-modal data (text, images, UI structure, semantics) for UI summarization and introducing a large-... |
| [![arXiv](https://img.shields.io/badge/arXiv-2103.14025-b31b1b.svg)](https://arxiv.org/abs/2103.14025) | The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion Planning... | Introduces a physically realistic benchmark for task-and-motion planning in embodied AI, emphasizing visual guidance and physics-driven interactions.... | This work differs by creating a comprehensive benchmark that integrates visual task understanding, physics-based object manipulation, and realistic en... |
| [![arXiv](https://img.shields.io/badge/arXiv-2008.05132-b31b1b.svg)](https://arxiv.org/abs/2008.05132) | Object Detection for Graphical User Interface: Old Fashioned or Deep Learning or... | The paper conducts a large-scale empirical study on GUI element detection methods, identifies limitations of existing approaches, and proposes a novel... | The work differs by introducing a GUI-specific old-fashioned method with a top-down coarse-to-fine strategy, integrating it with deep learning for tex... |
| [![arXiv](https://img.shields.io/badge/arXiv-1902.07257-b31b1b.svg)](https://arxiv.org/abs/1902.07257) | DOM-Q-NET: Grounded RL on Structured Language | The paper addresses challenges in web navigation for RL agents by introducing DOM-Q-NET, which handles large discrete action spaces and varying action... | DOM-Q-NET differs from related work by using a structured representation of HTML via graph neural networks and separating action-specific Q-networks f... |
| [![arXiv](https://img.shields.io/badge/arXiv-2008.08899-b31b1b.svg)](https://arxiv.org/abs/2008.08899) | Document Visual Question Answering Challenge 2020 | The paper introduces a new problem in visual question answering (VQA) focused on document images, proposing two tasks: single-image question answering... | This work differs from related work by specifically addressing VQA on document images, which is distinct from general-purpose VQA tasks. It introduces... |
| [![arXiv](https://img.shields.io/badge/arXiv-2101.11272-b31b1b.svg)](https://arxiv.org/abs/2101.11272) | VisualMRC: Machine Reading Comprehension on Document Images | Introduces VisualMRC, a dataset focused on document images with abstractive answers, and a new model integrating visual layout and content for machine... | Differently focuses on document-specific reading comprehension with abstractive answers, integrating visual layout understanding, whereas related work... |


### 2022: Growth Year

| arXiv | Title | Summary | Contributions |
|-------|-------|---------|---------------|
| [![arXiv](https://img.shields.io/badge/arXiv-2201.07207-b31b1b.svg)](https://arxiv.org/abs/2201.07207) | Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embod... | The paper explores using large language models (LLMs) to decompose high-level tasks into actionable steps for embodied agents without additional train... | This work differs from prior methods by leveraging pre-trained LLMs for zero-shot task decomposition, avoiding explicit step-by-step training. It focu... |
| [![arXiv](https://img.shields.io/badge/arXiv-2210.08402-b31b1b.svg)](https://arxiv.org/abs/2210.08402) | LAION-5B: An open large-scale dataset for training next generation image-text mo... | The paper introduces LAION-5B, a large-scale image-text dataset enabling training of advanced language-vision models. It emphasizes the importance of... | LAION-5B differs from prior work by offering the first openly available dataset of this scale, enabling broader research on multi-modal models without... |
| [![arXiv](https://img.shields.io/badge/arXiv-2205.11029v2-b31b1b.svg)](https://arxiv.org/abs/2205.11029) | META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI | The paper introduces a GUI-based task-oriented dialogue system (GUI-TOD) that directly interacts with mobile app GUIs without relying on backend APIs,... | This work differs from related work by eliminating reliance on task-oriented dialogue-specific backend APIs, enabling direct GUI interaction for task... |
| [![arXiv](https://img.shields.io/badge/arXiv-2207.01206v4-b31b1b.svg)](https://arxiv.org/abs/2207.01206) | WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agen... | The paper introduces WebShop, a large-scale simulated e-commerce environment with real-world products and crowd-sourced instructions, addressing scala... | This work differs from related work by creating a scalable, real-world benchmark (WebShop) with extensive product and instruction data, enabling evalu... |
| [![arXiv](https://img.shields.io/badge/arXiv-2210.03945-b31b1b.svg)](https://arxiv.org/abs/2210.03945) | Understanding HTML with Large Language Models | The paper explores the application of large language models (LLMs) to HTML understanding tasks, demonstrating their effectiveness in semantic classifi... | This work differs from related work by demonstrating that pre-trained LLMs, when fine-tuned, outperform task-specific models in HTML understanding tas... |
| [![arXiv](https://img.shields.io/badge/arXiv-2209.08199-b31b1b.svg)](https://arxiv.org/abs/2209.08199) | ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots | Introduces ScreenQA, a large-scale benchmark dataset for screen content understanding through question-answering, bridging low-level structural analys... | ScreenQA is the largest QA dataset for mobile screenshots, using full screenshots rather than cropped regions, and includes unanswerable questions and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2212.04088-b31b1b.svg)](https://arxiv.org/abs/2212.04088) | LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language... | The paper introduces LLM-Planner, a novel method leveraging large language models (LLMs) for few-shot planning in embodied agents. It addresses high d... | LLM-Planner differs from related work by integrating physical grounding into LLM planning, enabling environment-aware few-shot task execution without... |
| [![arXiv](https://img.shields.io/badge/arXiv-2209.14927-b31b1b.svg)](https://arxiv.org/abs/2209.14927) | Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus | The paper introduces a vision-only approach for mobile UI understanding, emphasizing the use of screenshots and a region of interest (focus) to bypass... | Unlike prior work reliant on view hierarchies, Spotlight uses a vision-language model with screenshot and focus inputs, achieving state-of-the-art res... |
| [![arXiv](https://img.shields.io/badge/arXiv-2206.08853-b31b1b.svg)](https://arxiv.org/abs/2206.08853) | MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge | MineDojo introduces a framework for building generalist embodied agents by integrating an open-ended simulation environment, an internet-scale multimo... | Unlike prior work focused on isolated environments with limited tasks, MineDojo combines a diverse simulation suite, large-scale knowledge integration... |
| [![arXiv](https://img.shields.io/badge/arXiv-2206.11795-b31b1b.svg)](https://arxiv.org/abs/2206.11795) | Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos | The paper introduces Video PreTraining (VPT), a method for training agents in sequential decision domains by leveraging unlabeled online videos throug... | This work differs from related work by extending internet-scale pretraining to sequential decision domains using unlabeled videos, enabling zero-shot... |
| [![arXiv](https://img.shields.io/badge/arXiv-2210.03347-b31b1b.svg)](https://arxiv.org/abs/2210.03347) | Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding | The paper introduces Pix2Struct, a pretrained image-to-text model for visual language understanding, focusing on parsing screenshots into HTML. It add... | Pix2Struct differs from prior work by using HTML structure as a pretraining target for visual parsing, enabling OCR-free end-to-end understanding. It... |
| [![arXiv](https://img.shields.io/badge/arXiv-2202.03052-b31b1b.svg)](https://arxiv.org/abs/2202.03052) | OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-... | The paper introduces OFA, a unified framework that unifies cross-modal and unimodal tasks through a sequence-to-sequence learning paradigm. It emphasi... | OFA differs from related work by eliminating task-specific layers through a simple sequence-to-sequence framework, achieving competitive performance w... |
| [![arXiv](https://img.shields.io/badge/arXiv-2212.09662-b31b1b.svg)](https://arxiv.org/abs/2212.09662) | MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Dere... | The paper introduces MatCha, a pretraining framework that enhances visual language models by integrating math reasoning and chart derendering tasks. I... | Unlike prior work, MatCha focuses on explicit plot deconstruction and numerical reasoning, addressing limitations in existing vision-language models f... |
| [![arXiv](https://img.shields.io/badge/arXiv-2211.03267-b31b1b.svg)](https://arxiv.org/abs/2211.03267) | Prompter: Utilizing Large Language Model Prompting for a Data Efficient Embodied... | The paper introduces a modular approach for Embodied Instruction Following (EIF) by integrating physical constraints of robots and leveraging Large La... | Prompter differs from prior work by incorporating physical constraints into modular design and replacing trained object search models with LLM-based p... |
| [![arXiv](https://img.shields.io/badge/arXiv-2202.02312v3-b31b1b.svg)](https://arxiv.org/abs/2202.02312) | A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasib... | The paper introduces MoTIF, a novel dataset for vision-language navigation (VLN) in mobile apps that addresses unknown command feasibility. It provide... | This work differs from related work by introducing the first dataset with feasibility annotations for VLN, addressing task uncertainty through follow-... |
| [![arXiv](https://img.shields.io/badge/arXiv-2204.08387-b31b1b.svg)](https://arxiv.org/abs/2204.08387) | LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking | The paper introduces LayoutLMv3, a multimodal pre-trained model for Document AI that unifies text and image masking with a word-patch alignment object... | Unlike prior multimodal models that used disparate objectives for text and image modalities, LayoutLMv3 introduces a unified masking approach and a cr... |
| [![arXiv](https://img.shields.io/badge/arXiv-2210.07474-b31b1b.svg)](https://arxiv.org/abs/2210.07474) | SQA3D: Situated Question Answering in 3D Scenes | Introduces SQA3D, a new task for evaluating embodied agents' scene understanding through situated question answering in 3D environments. Provides a co... | This work differs from related work by focusing on situated question answering in 3D scenes, emphasizing spatial reasoning, multi-hop logic, and commo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2207.00221-b31b1b.svg)](https://arxiv.org/abs/2207.00221) | VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attrib... | The paper introduces VL-CheckList, a framework to evaluate vision-language pretraining (VLP) models by analyzing their capabilities in objects, attrib... | Unlike prior work focused on downstream task performance, VL-CheckList provides an explainable, taxonomy-driven benchmark that decomposes VLP model ca... |
| [![arXiv](https://img.shields.io/badge/arXiv-2212.04732-b31b1b.svg)](https://arxiv.org/abs/2212.04732) | Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI... | The paper introduces QTypist, a context-aware text input generation framework leveraging pre-trained LLMs for mobile GUI testing. It addresses the cha... | Unlike prior work, QTypist innovatively applies LLMs to GUI testing with a tailored prompt engineering approach for text generation, achieving signifi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2211.11602-b31b1b.svg)](https://arxiv.org/abs/2211.11602) | Improving Multimodal Interactive Agents with Reinforcement Learning from Human F... | The paper introduces a method to enhance embodied agents using reinforcement learning from human feedback (RLHF) with a novel 'Inter-temporal Bradley-... | This work differs from related work by employing IBT modeling to capture human judgment dynamics for reward learning, enabling effective RLHF in compl... |
| [![arXiv](https://img.shields.io/badge/arXiv-2210.06849-b31b1b.svg)](https://arxiv.org/abs/2210.06849) | Retrospectives on the Embodied AI Workshop | The paper provides a retrospective analysis of Embodied AI research, focusing on three themes: visual navigation, rearrangement, and embodied vision-a... | This work differs from related research by offering a comprehensive retrospective analysis of challenges and progress in Embodied AI, emphasizing cros... |
| [![arXiv](https://img.shields.io/badge/arXiv-2210.13435-b31b1b.svg)](https://arxiv.org/abs/2210.13435) | Dichotomy of Control: Separating What You Can Control from What You Cannot | The paper introduces the Dichotomy of Control (DoC) framework for future-conditioned supervised learning in offline reinforcement learning, addressing... | This work differs from related work by explicitly separating policy control mechanisms from environmental stochasticity through latent variable condit... |
| [![arXiv](https://img.shields.io/badge/arXiv-2212.02623-b31b1b.svg)](https://arxiv.org/abs/2212.02623) | Unifying Vision, Text, and Layout for Universal Document Processing | The paper introduces UDOP, a unified model for document AI that integrates vision, text, and layout modalities. It proposes a novel Vision-Text-Layout... | UDOP differs from related work by unifying vision, text, and layout modalities into a single model with a novel architecture, enabling both document u... |
| [![arXiv](https://img.shields.io/badge/arXiv-2209.15099-b31b1b.svg)](https://arxiv.org/abs/2209.15099) | MUG: Interactive Multimodal Grounding on User Interfaces | The paper introduces MUG, an interactive multimodal grounding task for UI interactions that enables iterative user-agent collaboration. It emphasizes... | Unlike prior works that focused on single-round UI grounding, this work introduces iterative interaction frameworks, a dedicated dataset for multi-rou... |
| [![arXiv](https://img.shields.io/badge/arXiv-2203.07828-b31b1b.svg)](https://arxiv.org/abs/2203.07828) | Do BERTs Learn to Use Browser User Interface? Exploring Multi-Step Tasks with Un... | The paper explores the use of pre-trained BERT models for multi-step tasks involving GUI interaction, specifically through web browsers. It introduces... | This work differs from related work by explicitly integrating pre-trained BERTs with GUI interaction for multi-step tasks, whereas prior research prim... |
| [![arXiv](https://img.shields.io/badge/arXiv-2212.08051-b31b1b.svg)](https://arxiv.org/abs/2212.08051) | Objaverse: A Universe of Annotated 3D Objects | Analysis failed: Batch generation error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 896.00 KiB i... | Analysis failed: Batch generation error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 896.00 KiB i... |
| [![arXiv](https://img.shields.io/badge/arXiv-2209.08655-b31b1b.svg)](https://arxiv.org/abs/2209.08655) | Enabling Conversational Interaction with Mobile UI using Large Language Models | The paper explores using pre-trained large language models (LLMs) with prompting techniques to enable versatile conversational interactions with mobil... | This work differs from related work by avoiding the need for task-specific datasets or models, leveraging general-purpose LLMs with carefully designed... |
| [![arXiv](https://img.shields.io/badge/arXiv-2211.16649-b31b1b.svg)](https://arxiv.org/abs/2211.16649) | CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation | The paper explores zero-shot Vision-and-Language Navigation (VLN) using CLIP, demonstrating its capability to navigate environments via natural langua... | This work differs from related work by applying CLIP for zero-shot VLN using natural language referring expressions (rather than object class template... |
| [![arXiv](https://img.shields.io/badge/arXiv-2210.09263-b31b1b.svg)](https://arxiv.org/abs/2210.09263) | Vision-Language Pre-training: Basics, Recent Advances, and Future Trends | The paper provides a comprehensive survey of vision-language pre-training (VLP) methods, categorizing them into image-text, core computer vision, and... | This work differs from related work by offering a structured survey of VLP methods across diverse modalities, emphasizing challenges and advanced rese... |
| [![arXiv](https://img.shields.io/badge/arXiv-2205.12005-b31b1b.svg)](https://arxiv.org/abs/2205.12005) | mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-conn... | The paper introduces mPLUG, a vision-language foundation model addressing computational inefficiency and information asymmetry in cross-modal alignmen... | mPLUG differs from prior work by introducing cross-modal skip-connections to reduce computational costs of long visual sequences and mitigate informat... |
| [![arXiv](https://img.shields.io/badge/arXiv-2202.08137-b31b1b.svg)](https://arxiv.org/abs/2202.08137) | A data-driven approach for learning to control computers | The paper introduces a data-driven approach combining reinforcement learning with behavioral priors derived from human-computer interactions to achiev... | This work differs from related work by focusing on scalable reinforcement learning with behavioral priors from real human interactions, avoiding hand-... |
| [![arXiv](https://img.shields.io/badge/arXiv-2206.10352-b31b1b.svg)](https://arxiv.org/abs/2206.10352) | Psychologically-Inspired, Unsupervised Inference of Perceptual Groups of GUI Wid... | The paper introduces a psychologically-inspired, unsupervised method for perceptual grouping of GUI widgets using Gestalt principles, addressing the c... | This work differs from related work by leveraging Gestalt theory for unsupervised perceptual grouping without supervision or runtime data, outperformi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2208.09116-b31b1b.svg)](https://arxiv.org/abs/2208.09116) | Effective, Platform-Independent GUI Testing via Image Embedding and Reinforcemen... | The paper introduces PIRLTest, a platform-independent GUI testing framework combining image embedding and reinforcement learning. It addresses limitat... | Unlike prior work relying on static models or random exploration, PIRLTest innovates by embedding GUI images as states for reinforcement learning, usi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2208.00932-b31b1b.svg)](https://arxiv.org/abs/2208.00932) | Masader Plus: A New Interface for Exploring +500 Arabic NLP Datasets | The paper introduces Masader Plus, a web interface designed to enhance the exploration of Arabic NLP datasets by addressing UX challenges, enabling da... | This work differs from related work by providing a user-centric web interface for dataset exploration, emphasizing usability improvements and interact... |
| [![arXiv](https://img.shields.io/badge/arXiv-2211.00688-b31b1b.svg)](https://arxiv.org/abs/2211.00688) | Learning to Solve Voxel Building Embodied Tasks from Pixels and Natural Language... | The paper introduces a method combining language models and reinforcement learning for voxel building tasks in Minecraft-like environments, emphasizin... | This work differs by integrating language model-generated sub-goals with reinforcement learning for task execution, improving feasibility verification... |
| [![arXiv](https://img.shields.io/badge/arXiv-2206.00142-b31b1b.svg)](https://arxiv.org/abs/2206.00142) | IGLU Gridworld: Simple and Fast Environment for Embodied Dialog Agents | The paper introduces IGLU Gridworld, a reinforcement learning environment focused on embodied dialog agents with visual embodiment, language-condition... | Unlike prior work, IGLU Gridworld provides a unified benchmark combining visual agent embodiment, language conditioning, and complex task spaces, enab... |
| [![arXiv](https://img.shields.io/badge/arXiv-2211.06552-b31b1b.svg)](https://arxiv.org/abs/2211.06552) | Collecting Interactive Multi-modal Datasets for Grounded Language Understanding | The paper formalizes a collaborative embodied agent using natural language tasks, develops a scalable data collection tool, and introduces the first d... | This work differs from related work by providing the first interactive grounded language understanding dataset, along with a formal framework for embo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2207.01821-b31b1b.svg)](https://arxiv.org/abs/2207.01821) | Toward Explainable and Fine-Grained 3D Grounding through Referring Textual Phras... | The paper advances 3D scene understanding by introducing a fine-grained task (3DPAG) that explicitly handles phrase-level object relationships in visu... | This work differs from related work by focusing on fine-grained phrase-aware grounding rather than coarse-grained object localization, introducing a l... |
| [![arXiv](https://img.shields.io/badge/arXiv-2209.00353-b31b1b.svg)](https://arxiv.org/abs/2209.00353) | AccoMontage2: A Complete Harmonization and Accompaniment Arrangement System | Analysis failed: Batch generation error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 896.00 KiB i... | Analysis failed: Batch generation error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 896.00 KiB i... |


### 2023: Q1

| arXiv | Title | Summary | Contributions |
|-------|-------|---------|---------------|
| [![arXiv](https://img.shields.io/badge/arXiv-2303.08774-b31b1b.svg)](https://arxiv.org/abs/2303.08774) | GPT-4 Technical Report | GPT-4 is a large-scale multimodal model with human-level performance on professional and academic benchmarks, improved post-training alignment for fac... | Differs from prior work by achieving human-level performance on complex benchmarks (e.g., simulated bar exams), demonstrating multimodal capabilities... |
| [![arXiv](https://img.shields.io/badge/arXiv-2303.18223-b31b1b.svg)](https://arxiv.org/abs/2303.18223) | A Survey of Large Language Models | The paper provides a comprehensive survey of large language models (LLMs), focusing on their evolution from statistical to neural models, the impact o... | This work differs from related work by offering a structured review of LLMs' advancements, emphasizing scaling laws, emergent capabilities, and practi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2303.11366-b31b1b.svg)](https://arxiv.org/abs/2303.11366) | Reflexion: Language Agents with Verbal Reinforcement Learning | Reflexion introduces a novel framework for language agents that uses verbal reinforcement learning through self-reflective feedback. It leverages epis... | Reflexion differentiates from related work by using verbal feedback as a 'semantic gradient' instead of traditional reinforcement learning, enabling p... |
| [![arXiv](https://img.shields.io/badge/arXiv-2302.01560-b31b1b.svg)](https://arxiv.org/abs/2302.01560) | Describe, Explain, Plan and Select: Interactive Planning with Large Language Mod... | The paper introduces DEPS, an interactive planning framework leveraging Large Language Models (LLMs) to address long-term reasoning and sub-task effic... | DEPS differs from prior work by integrating self-explanation and feedback loops for error correction, introducing a trainable goal selector to optimiz... |
| [![arXiv](https://img.shields.io/badge/arXiv-2303.17491-b31b1b.svg)](https://arxiv.org/abs/2303.17491) | Language Models can Solve Computer Tasks | The paper introduces a novel prompting method (Recursively Criticizes and Improves, RCI) enabling pre-trained LLMs to execute computer tasks via natur... | This work differs by using RCI prompting with minimal demonstrations and no task-specific reward functions, achieving state-of-the-art results on Mini... |
| [![arXiv](https://img.shields.io/badge/arXiv-2303.18240-b31b1b.svg)](https://arxiv.org/abs/2303.18240) | Where are we in the search for an Artificial Visual Cortex for Embodied Intellig... | The paper presents CortexBench, a comprehensive benchmark for evaluating pre-trained visual representations (PVRs) across 17 EAI tasks. It evaluates e... | This work unifies fragmented EAI research by introducing CortexBench, a broad benchmark for evaluating PVRs across diverse tasks. It refutes prior ass... |
| [![arXiv](https://img.shields.io/badge/arXiv-2303.00855-b31b1b.svg)](https://arxiv.org/abs/2303.00855) | Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Age... | The paper addresses the challenge of integrating semantic knowledge from large language models (LLMs) with grounded environmental understanding for em... | This work differs from related work by combining the semantic capabilities of LLMs with grounded models of the environment through a novel decoding fr... |
| [![arXiv](https://img.shields.io/badge/arXiv-2301.12050-b31b1b.svg)](https://arxiv.org/abs/2301.12050) | Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Lang... | The paper introduces a novel approach combining large language models (LLMs) with reinforcement learning (RL) for embodied agents, focusing on languag... | This work differs from related work by integrating LLMs to hypothesize abstract world models (AWMs) for planning, followed by environment-driven verif... |
| [![arXiv](https://img.shields.io/badge/arXiv-2302.00763-b31b1b.svg)](https://arxiv.org/abs/2302.00763) | Collaborating with language models for embodied reasoning | This work explores the integration of large language models (LLMs) with embodied agents to enhance reasoning in complex environments. It introduces a... | This work differs from related work by proposing a novel architecture that synergizes pre-trained language models with embodied agents through a colla... |
| [![arXiv](https://img.shields.io/badge/arXiv-2302.04659-b31b1b.svg)](https://arxiv.org/abs/2302.04659) | ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills | ManiSkill2 introduces a comprehensive benchmark addressing limitations in existing manipulation benchmarks by incorporating diverse object-level varia... | Unlike prior benchmarks, ManiSkill2 offers a unified framework with extensive object diversity, dynamic simulations, and scalable infrastructure, enab... |
| [![arXiv](https://img.shields.io/badge/arXiv-2301.10165-b31b1b.svg)](https://arxiv.org/abs/2301.10165) | Lexi: Self-Supervised Learning of the UI Language | The paper introduces Lexi, a self-supervised model for learning visio-linguistic representations of UIs without relying on metadata like UI trees or a... | This work differs from related work by avoiding reliance on UI metadata, leveraging self-supervised learning with a novel dataset, and focusing on UI-... |
| [![arXiv](https://img.shields.io/badge/arXiv-2301.13166-b31b1b.svg)](https://arxiv.org/abs/2301.13166) | ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigati... | The paper introduces ESC, a zero-shot object navigation framework that leverages pre-trained vision-language models and commonsense reasoning from lar... | ESC differs from prior work by using pre-trained commonsense knowledge from LLMs for zero-shot object navigation without requiring environment-specifi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2303.10571-b31b1b.svg)](https://arxiv.org/abs/2303.10571) | Reinforcement Learning Friendly Vision-Language Model for Minecraft | The paper introduces CLIP4MC, a cross-modal contrastive learning framework for training RL-friendly vision-language models (VLMs) in open-ended tasks.... | Unlike prior work, CLIP4MC explicitly incorporates task completion degree into the VLM training objective to provide more instructive reward signals f... |
| [![arXiv](https://img.shields.io/badge/arXiv-2301.12507-b31b1b.svg)](https://arxiv.org/abs/2301.12507) | Distilling Internet-Scale Vision-Language Models into Embodied Agents | The paper introduces a method to distill internet-scale vision-language models (VLMs) into embodied agents for language grounding. It combines model d... | This work differs from related work by repurposing pre-trained VLMs for offline supervision of embodied agents through prompt-based distillation and H... |
| [![arXiv](https://img.shields.io/badge/arXiv-2303.03480-b31b1b.svg)](https://arxiv.org/abs/2303.03480) | Can an Embodied Agent Find Your "Cat-shaped Mug"? LLM-Guided Exploration for Zer... | Analysis failed: Batch generation error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 896.00 KiB i... | Analysis failed: Batch generation error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 896.00 KiB i... |
| [![arXiv](https://img.shields.io/badge/arXiv-2301.05223-b31b1b.svg)](https://arxiv.org/abs/2301.05223) | NOPA: Neurally-guided Online Probabilistic Assistance for Building Socially Inte... | The paper introduces NOPA, a method for socially intelligent home assistants that addresses adaptivity and scalability in online goal inference. Key c... | NOPA differs from prior work by integrating neural and probabilistic methods for adaptive goal inference and uncertainty-aware helping strategies, alo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2303.01586-b31b1b.svg)](https://arxiv.org/abs/2303.01586) | Alexa Arena: A User-Centric Interactive Platform for Embodied AI | The paper introduces Alexa Arena, a user-centric simulation platform for Embodied AI (EAI) research, enabling human-robot interaction (HRI) missions t... | Alexa Arena differs from related work by offering a user-centric, gamified platform that bridges simulation and real-world HRI, along with a dialog-en... |
| [![arXiv](https://img.shields.io/badge/arXiv-2303.16894-b31b1b.svg)](https://arxiv.org/abs/2303.16894) | ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and P... | ViewRefer addresses 3D visual grounding by integrating multi-view knowledge from both text and 3D modalities. It leverages large-scale language models... | Unlike prior works focusing solely on 3D modality or manual alignment, ViewRefer innovates by extracting view knowledge from both text and 3D inputs.... |
| [![arXiv](https://img.shields.io/badge/arXiv-2303.12998-b31b1b.svg)](https://arxiv.org/abs/2303.12998) | The Universal NFT Vector Database: A Scaleable Vector Database for NFT Similarit... | The paper introduces a scalable vector database for NFT similarity matching, a modular cloud-centered NFT processing system compliant with ERC-721 sta... | This work differs from related work by proposing an off-chain, hardware-agnostic architecture for NFT data aggregation based on vector similarity, com... |
| [![arXiv](https://img.shields.io/badge/arXiv-2303.03565-b31b1b.svg)](https://arxiv.org/abs/2303.03565) | CLIP-Layout: Style-Consistent Indoor Scene Synthesis with Semantic Furniture Emb... | Analysis failed: Batch generation error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 896.00 KiB i... | Analysis failed: Batch generation error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 896.00 KiB i... |


### 2023: Q2

| arXiv | Title | Summary | Contributions |
|-------|-------|---------|---------------|
| [![arXiv](https://img.shields.io/badge/arXiv-2305.16291-b31b1b.svg)](https://arxiv.org/abs/2305.16291) | Voyager: An Open-Ended Embodied Agent with Large Language Models | Voyager introduces an LLM-powered embodied lifelong learning agent for open-ended environments like Minecraft, featuring an automatic curiosity-driven... | Unlike prior works that rely on fixed curricula or require human interaction, Voyager enables open-ended exploration through a bottom-up curiosity-dri... |
| [![arXiv](https://img.shields.io/badge/arXiv-2306.06070v3-b31b1b.svg)](https://arxiv.org/abs/2306.06070) | Mind2Web: Towards a Generalist Agent for the Web | The paper introduces Mind2Web, the first dataset for generalist web agents, emphasizing real-world websites, diverse domains/tasks, and user interacti... | Differs from prior work by using real-world websites instead of simulations, providing diverse tasks across 31 domains, and integrating LLMs with a fi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.11854v4-b31b1b.svg)](https://arxiv.org/abs/2305.11854) | Multimodal Web Navigation with Instruction-Finetuned Foundation Models | This work introduces WebGUM, a multimodal agent for web navigation that combines vision-language foundation models with instruction-finetuning. It add... | Unlike prior works reliant on domain-specific architectures and online RL, this work leverages instruction-finetuned vision-language foundation models... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.15021-b31b1b.svg)](https://arxiv.org/abs/2305.15021) | EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought | The paper introduces EmbodiedGPT, a multi-modal foundation model for embodied AI that integrates vision-language pre-training with embodied planning a... | This work differs from related work by explicitly combining Chain-of-Thought planning with vision-language pre-training, enabling more effective task... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.16653-b31b1b.svg)](https://arxiv.org/abs/2305.16653) | AdaPlanner: Adaptive Planning from Feedback with Language Models | AdaPlanner introduces adaptive planning for LLM agents through feedback, combining in-plan and out-of-plan refinement strategies. It mitigates halluci... | Unlike prior methods that use static plans or greedy actions, AdaPlanner introduces closed-loop adaptive planning with feedback, reducing hallucinatio... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.17066-b31b1b.svg)](https://arxiv.org/abs/2305.17066) | Mindstorms in Natural Language-Based Societies of Mind | The paper introduces the concept of Natural Language-Based Societies of Mind (NLSOMs), where diverse neural networks communicate via natural language... | This work differs from related work by focusing on the societal organization of agents rather than individual models, leveraging natural language as a... |
| [![arXiv](https://img.shields.io/badge/arXiv-2306.00245-b31b1b.svg)](https://arxiv.org/abs/2306.00245) | From Pixels to UI Actions: Learning to Follow Instructions via Graphical User In... | This paper introduces a pixel-based approach for GUI agents, demonstrating that agents can outperform humans on instruction-following tasks using only... | The work differs from prior methods by using purely pixel-based inputs and generic action spaces instead of structured representations (e.g., DOM tree... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.08144v4-b31b1b.svg)](https://arxiv.org/abs/2305.08144) | Mobile-Env: Building Qualified Evaluation Benchmarks for LLM-GUI Interaction | The paper introduces Mobile-Env, a comprehensive toolkit for creating qualified GUI interaction benchmarks in Android environments. It addresses limit... | Mobile-Env differs from existing benchmarks by offering a controllable and isolated environment with support for intermediate rewards and instructions... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.09434-b31b1b.svg)](https://arxiv.org/abs/2305.09434) | Chatting with GPT-3 for Zero-Shot Human-Like Mobile Automated GUI Testing | This paper introduces GPTDroid, a novel approach leveraging large language models (LLMs) like GPT-3 for zero-shot human-like mobile GUI testing. It fo... | GPTDroid differs from related work by combining LLM-based natural language understanding with GUI testing, using dynamic context iteration and neural... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.14761-b31b1b.svg)](https://arxiv.org/abs/2305.14761) | UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension a... | UniChart introduces a pretrained vision-language model specialized for chart comprehension and reasoning, incorporating chart-specific pretraining tas... | UniChart differs from related work by explicitly modeling chart structures (e.g., visual elements, data relationships) through task-specific pretraini... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.02412-b31b1b.svg)](https://arxiv.org/abs/2305.02412) | Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Age... | The paper introduces the PET framework, which leverages pre-trained LLMs to simplify control tasks for embodied agents by decomposing tasks into sub-t... | This work differs by using LLMs as teachers to simplify control problems rather than directly solving them, employing modular components (Plan, Elimin... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.11172-b31b1b.svg)](https://arxiv.org/abs/2305.11172) | ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalitie... | The paper introduces a scalable general representation model (ONE-PEACE) capable of handling unlimited modalities through modality adapters, shared se... | Unlike prior work that often relies on pre-trained models for specific modalities, ONE-PEACE introduces a unified architecture with modality-agnostic... |
| [![arXiv](https://img.shields.io/badge/arXiv-2306.07863-b31b1b.svg)](https://arxiv.org/abs/2306.07863) | Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control | Synapse introduces state abstraction, trajectory-as-exemplar prompting, and exemplar memory to enhance computer control agents. It addresses limitatio... | Unlike prior work relying on task-specific exemplars or incomplete prompts, Synapse uses abstracted state trajectories and memory-based retrieval to e... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.10626-b31b1b.svg)](https://arxiv.org/abs/2305.10626) | Language Models Meet World Models: Embodied Experiences Enhance Language Models | The paper introduces a framework that enhances language models (LMs) with embodied experiences from world models to improve reasoning and planning in... | This work differs from related work by explicitly combining world models with LMs to bridge the gap in embodied knowledge, while preserving the LMs' g... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.06849-b31b1b.svg)](https://arxiv.org/abs/2305.06849) | WebCPM: Interactive Web Search for Chinese Long-form Question Answering | The paper introduces WebCPM, the first Chinese long-form question answering (LFQA) dataset that leverages interactive web search for information retri... | This work differs from related work by introducing WebCPM, a novel dataset collected via interactive web search, and by emphasizing the synthesis of h... |
| [![arXiv](https://img.shields.io/badge/arXiv-2304.09349-b31b1b.svg)](https://arxiv.org/abs/2304.09349) | LLM as A Robotic Brain: Unifying Egocentric Memory and Control | The paper introduces LLM-Brain, a framework that unifies egocentric memory and control in embodied AI using large language models (LLMs). It emphasize... | This work differs from related work by unifying memory and control through an embodied LLM, leveraging zero-shot learning for robotic tasks, and integ... |
| [![arXiv](https://img.shields.io/badge/arXiv-2306.01987-b31b1b.svg)](https://arxiv.org/abs/2306.01987) | Prompting Is All You Need: Automated Android Bug Replay with Large Language Mode... | This paper introduces AdbGPT, a lightweight LLM-based approach for automated Android bug replay through prompt engineering. It leverages few-shot lear... | Unlike prior work relying on S2R extraction and predefined patterns, AdbGPT uses LLMs with prompt engineering to infer bug reproduction steps without... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.16986-b31b1b.svg)](https://arxiv.org/abs/2305.16986) | NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language... | The paper introduces NavGPT, a purely LLM-based navigation agent that demonstrates explicit reasoning capabilities in vision-and-language navigation (... | This work differs from related work by leveraging large language models (LLMs) for zero-shot VLN without additional training, emphasizing explicit rea... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.10455-b31b1b.svg)](https://arxiv.org/abs/2305.10455) | Towards Generalist Robots: A Promising Paradigm via Generative Simulation | The paper proposes a generative simulation paradigm for robotics research, leveraging large-scale foundation models to automate task and scene generat... | This work differs from related work by advocating a fully automated generative pipeline (generative simulation) that uses foundation models to create... |
| [![arXiv](https://img.shields.io/badge/arXiv-2306.07906-b31b1b.svg)](https://arxiv.org/abs/2306.07906) | WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human P... | WebGLM introduces a web-enhanced question-answering system that integrates a retriever, bootstrapped generator, and human preference-aware scorer to i... | WebGLM differs by combining retrieval augmentation with human preference learning, creating a novel architecture for efficient web QA. It introduces a... |
| [![arXiv](https://img.shields.io/badge/arXiv-2306.03604-b31b1b.svg)](https://arxiv.org/abs/2306.03604) | Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement L... | This paper addresses efficient interaction between agents and large language models (LLMs) using reinforcement learning. It introduces When2Ask, an RL... | Unlike prior work that relies on hard-coded rules or frequent LLM querying, this paper proposes a learned interaction policy (When2Ask) that dynamical... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.01795-b31b1b.svg)](https://arxiv.org/abs/2305.01795) | Multimodal Procedural Planning via Dual Text-Image Prompting | The paper introduces the Multimodal Procedural Planning (MPP) task, which generates paired text-image steps for task execution. It proposes Text-Image... | This work differs from related work by explicitly addressing cross-modal informativeness, temporal coherence, and accuracy in procedural planning thro... |
| [![arXiv](https://img.shields.io/badge/arXiv-2306.04356-b31b1b.svg)](https://arxiv.org/abs/2306.04356) | Fine-Grained Visual Prompting | The paper introduces Fine-Grained Visual Prompting (FGVP) to enhance zero-shot instance-level tasks for Vision-Language Models (VLMs). It proposes the... | This work differs from related work by introducing fine-grained visual prompting using segmentation masks (e.g., from SAM) instead of coarse cues like... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.15765-b31b1b.svg)](https://arxiv.org/abs/2305.15765) | Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving | This paper introduces a novel multi-modal visual grounding task (LiDAR Grounding) tailored for autonomous driving scenarios, proposing the Multi-modal... | This work differs from related work by focusing on 3D outdoor autonomous driving scenarios rather than 2D or indoor 3D domains. It introduces a novel... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.14218-b31b1b.svg)](https://arxiv.org/abs/2305.14218) | DUBLIN -- Document Understanding By Language-Image Network | The paper introduces DUBLIN, a model pretrained on web pages with novel objectives for visual document understanding. It achieves state-of-the-art res... | DUBLIN differs from related work by employing three novel training objectives (Masked Document Text Generation, Bounding Box Task, Rendered Question A... |
| [![arXiv](https://img.shields.io/badge/arXiv-2304.09012-b31b1b.svg)](https://arxiv.org/abs/2304.09012) | GUILGET: GUI Layout GEneration with Transformer | The paper introduces GUILGET, a transformer-based model for generating GUI layouts from positional constraints (GUI-AGs), emphasizing constraint adher... | GUILGET differs from related work by explicitly addressing GUI design constraints through a transformer architecture that captures semantic relationsh... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.10912-b31b1b.svg)](https://arxiv.org/abs/2305.10912) | A Generalist Dynamics Model for Control | This paper introduces transformer sequence models as dynamics models (TDMs) for control, demonstrating their strong generalization capabilities in bot... | The work differs from related work by focusing on using transformers as dynamics models rather than policies, emphasizing generalization across unseen... |
| [![arXiv](https://img.shields.io/badge/arXiv-2304.13653-b31b1b.svg)](https://arxiv.org/abs/2304.13653) | Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learnin... | The paper advances deep reinforcement learning (Deep RL) for bipedal robots, demonstrating agile soccer skills through zero-shot sim-to-real transfer.... | This work differs by focusing on full-body control of bipedal robots for long-horizon, multi-agent competitive tasks, whereas prior research primarily... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.07498-b31b1b.svg)](https://arxiv.org/abs/2305.07498) | Visual Information Extraction in the Wild: Practical Dataset and End-to-end Solu... | The paper introduces a large-scale, diverse dataset (POIE) for Visual Information Extraction (VIE) in real-world scenarios, along with an end-to-end f... | The work differs by proposing a more challenging and diverse dataset (POIE) compared to existing benchmarks like SROIE and EPHOIE, and introduces a no... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.08455-b31b1b.svg)](https://arxiv.org/abs/2305.08455) | Document Understanding Dataset and Evaluation (DUDE) | The paper introduces a novel dataset (DUDE) for visually-rich documents, emphasizing multi-industry, multi-domain, and multi-page layouts. It proposes... | This work differs from related work by focusing on practical, real-world applicable benchmarks through diverse datasets and multi-task evaluations, ad... |
| [![arXiv](https://img.shields.io/badge/arXiv-2306.06770-b31b1b.svg)](https://arxiv.org/abs/2306.06770) | Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysi... | The paper introduces the STARS framework, which enhances LLM-based task learning for embodied agents by addressing limitations of prompt engineering.... | STARS differs from related work by combining online task learning, multi-source knowledge exploitation, and proactive LLM response evaluation without... |
| [![arXiv](https://img.shields.io/badge/arXiv-2304.01192-b31b1b.svg)](https://arxiv.org/abs/2304.01192) | Navigating to Objects Specified by Images | The paper presents a modular system for embodied agents to navigate to objects specified by images, combining semantic visual reasoning with explorati... | This work differs from related work by using a modular approach with off-the-shelf components for sub-tasks (exploration, re-identification, localizat... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.07019-b31b1b.svg)](https://arxiv.org/abs/2305.07019) | Musketeer: Joint Training for Multi-task Vision Language Model with Task Explana... | The paper introduces Musketeer, a vision-language model trained jointly across multiple heterogeneous tasks with fully shared parameters. It proposes... | Unlike prior work with separate task-specific heads or adapters, Musketeer employs a unified encoder-decoder architecture with fully shared parameters... |
| [![arXiv](https://img.shields.io/badge/arXiv-2304.02639-b31b1b.svg)](https://arxiv.org/abs/2304.02639) | ENTL: Embodied Navigation Trajectory Learner | ENTL introduces a unified approach for embodied navigation by integrating world modeling, localization, and imitation learning into a single sequence... | ENTL differs from related work by unifying world modeling and localization into a sequence prediction framework without explicit rewards, using a tran... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.10783-b31b1b.svg)](https://arxiv.org/abs/2305.10783) | Transforming Human-Centered AI Collaboration: Redefining Embodied Agents Capabil... | The paper focuses on developing embodied agents capable of human-like collaboration through grounded language instructions. Key contributions include... | This work differs from related research by providing a comprehensive framework for collecting and utilizing grounded language instructions, offering a... |
| [![arXiv](https://img.shields.io/badge/arXiv-2305.04183-b31b1b.svg)](https://arxiv.org/abs/2305.04183) | OpenViVQA: Task, Dataset, and Multimodal Fusion Models for Visual Question Answe... | The paper introduces the first large-scale open-domain Vietnamese VQA dataset (OpenViVQA) with 11,000+ images and 37,000+ question-answer pairs, along... | This work differs from related work by focusing on open-ended answer generation in Vietnamese, providing a new benchmark for low-resource languages, a... |


### 2023: Q3

| arXiv | Title | Summary | Contributions |
|-------|-------|---------|---------------|
| [![arXiv](https://img.shields.io/badge/arXiv-2308.12966-b31b1b.svg)](https://arxiv.org/abs/2308.12966) | Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text... | The paper introduces Qwen-VL, a series of vision-language models with enhanced visual perception capabilities through a visual receptor, 3-stage train... | The work differs from related work by integrating a meticulously designed 3-stage training pipeline, visual receptor, and input-output interface, alon... |
| [![arXiv](https://img.shields.io/badge/arXiv-2307.13854v4-b31b1b.svg)](https://arxiv.org/abs/2307.13854) | WebArena: A Realistic Web Environment for Building Autonomous Agents | The paper introduces WebArena, a highly realistic and reproducible web environment for autonomous agents, featuring functional websites from four doma... | WebArena differs from prior work by providing a realistic web environment with dynamic, functional websites and a benchmark focused on functional corr... |
| [![arXiv](https://img.shields.io/badge/arXiv-2309.07864-b31b1b.svg)](https://arxiv.org/abs/2309.07864) | The Rise and Potential of Large Language Model Based Agents: A Survey | The paper provides a comprehensive survey of LLM-based agents, tracing their conceptual origins, presenting a general framework (brain, perception, ac... | This work differs by offering a holistic framework for LLM-based agents, systematically categorizing applications, and analyzing societal implications... |
| [![arXiv](https://img.shields.io/badge/arXiv-2307.15818-b31b1b.svg)](https://arxiv.org/abs/2307.15818) | RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control | The paper introduces RT-2, a vision-language-action (VLA) model that integrates large-scale web data with robotic control through co-fine-tuning. It e... | This work differs by co-fine-tuning vision-language models on both robotic trajectory data and internet-scale vision-language tasks, treating actions... |
| [![arXiv](https://img.shields.io/badge/arXiv-2307.12856-b31b1b.svg)](https://arxiv.org/abs/2307.12856) | A Real-World WebAgent with Planning, Long Context Understanding, and Program Syn... | The paper introduces WebAgent, an LLM-driven autonomous agent for real-world web automation that addresses open-domainness, long-context HTML understa... | Unlike prior works relying on simulated environments or single LLMs, WebAgent combines HTML-T5 (specialized for HTML with novel attention mechanisms)... |
| [![arXiv](https://img.shields.io/badge/arXiv-2309.11436-b31b1b.svg)](https://arxiv.org/abs/2309.11436) | You Only Look at Screens: Multimodal Chain-of-Action Agents | The paper introduces Auto-GUI, a multimodal GUI agent that directly interacts with interfaces without environment parsing or application-specific APIs... | Auto-GUI differs from prior work by eliminating reliance on external tools (e.g., OCR) and application APIs, directly interacting with GUIs through mu... |
| [![arXiv](https://img.shields.io/badge/arXiv-2307.01952-b31b1b.svg)](https://arxiv.org/abs/2307.01952) | SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis | The paper introduces SDXL, a latent diffusion model for text-to-image synthesis with enhanced performance through a larger UNet backbone, novel condit... | SDXL improves upon existing latent diffusion models by scaling the UNet architecture, introducing novel conditioning techniques, and incorporating a r... |
| [![arXiv](https://img.shields.io/badge/arXiv-2307.10088v2-b31b1b.svg)](https://arxiv.org/abs/2307.10088) | Android in the Wild: A Large-Scale Dataset for Android Device Control | The paper introduces a large-scale dataset (AITW) for device-control research, emphasizing visual and language interaction, multi-step tasks, and robu... | This work differs from related work by providing an order-of-magnitude larger dataset with diverse Android versions, device types, and complex multi-s... |
| [![arXiv](https://img.shields.io/badge/arXiv-2308.15272v4-b31b1b.svg)](https://arxiv.org/abs/2308.15272) | AutoDroid: LLM-powered Task Automation in Android | AutoDroid integrates large language models (LLMs) with domain-specific app knowledge through dynamic analysis to enable scalable, hands-free mobile ta... | AutoDroid differs from related work by combining LLM commonsense knowledge with app-specific domain knowledge through automated dynamic analysis, elim... |
| [![arXiv](https://img.shields.io/badge/arXiv-2307.08581-b31b1b.svg)](https://arxiv.org/abs/2307.08581) | BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs | BuboGPT enhances multi-modal LLMs with fine-grained visual grounding, enabling precise cross-modal interactions between vision, audio, and language. I... | Unlike prior works that rely on coarse-grained mappings, BuboGPT achieves fine-grained visual grounding by integrating a SAM-based module and a novel... |
| [![arXiv](https://img.shields.io/badge/arXiv-2307.02485-b31b1b.svg)](https://arxiv.org/abs/2307.02485) | Building Cooperative Embodied Agents Modularly with Large Language Models | The paper introduces a modular framework integrating Large Language Models (LLMs) into embodied agents for decentralized multi-agent cooperation. It p... | This work differs from related research by employing a decentralized, modular cognitive-inspired framework that leverages LLMs for reasoning, language... |
| [![arXiv](https://img.shields.io/badge/arXiv-2309.11419-b31b1b.svg)](https://arxiv.org/abs/2309.11419) | KOSMOS-2.5: A Multimodal Literate Model | KOSMOS-2.5 introduces a multimodal literate model for document-level text recognition and structured output generation, combining spatial-aware text b... | Unlike prior work focused on line-level OCR or domain-specific structured parsing, KOSMOS-2.5 unifies spatial and structural document understanding th... |
| [![arXiv](https://img.shields.io/badge/arXiv-2308.02151-b31b1b.svg)](https://arxiv.org/abs/2308.02151) | Retroformer: Retrospective Large Language Agents with Policy Gradient Optimizati... | Introduces Retroformer, a framework for reinforcing large language agents using policy gradient optimization with a retrospective model that automatic... | Proposes a policy gradient approach for language agents, differentiating from prior work by leveraging environment rewards directly rather than relyin... |
| [![arXiv](https://img.shields.io/badge/arXiv-2307.01848-b31b1b.svg)](https://arxiv.org/abs/2307.01848) | Embodied Task Planning with Large Language Models | This paper introduces TaPA, an embodied task planning agent that integrates large language models (LLMs) with visual perception for grounded planning.... | The work differs by combining LLM-generated plans with scene-grounded object detection for executable actions, using a custom multimodal dataset for t... |
| [![arXiv](https://img.shields.io/badge/arXiv-2309.02561-b31b1b.svg)](https://arxiv.org/abs/2309.02561) | Physically Grounded Vision-Language Models for Robotic Manipulation | The paper introduces PhysObjects, a large-scale dataset annotating physical concepts of household objects, and demonstrates how fine-tuning vision-lan... | This work differs from prior work by focusing on physical concept annotations (e.g., material, fragility) rather than visual attributes, and by integr... |
| [![arXiv](https://img.shields.io/badge/arXiv-2309.14365-b31b1b.svg)](https://arxiv.org/abs/2309.14365) | An In-depth Survey of Large Language Model-based Artificial Intelligence Agents | The paper provides a comprehensive comparison between LLM-based AI agents and traditional agents, analyzing core components (planning, memory, tool us... | This work differs from related work by offering a systematic survey of LLM-based agents, emphasizing their unique strengths in natural language proces... |
| [![arXiv](https://img.shields.io/badge/arXiv-2308.15452-b31b1b.svg)](https://arxiv.org/abs/2308.15452) | When Do Program-of-Thoughts Work for Reasoning? | The paper investigates the effectiveness of program-of-thought prompting for reasoning, introducing the Complexity-Impacted Reasoning Score (CIRS) to... | This work differs from related studies by introducing CIRS, a metric that evaluates code complexity through both structural (AST) and logical (difficu... |
| [![arXiv](https://img.shields.io/badge/arXiv-2309.12311-b31b1b.svg)](https://arxiv.org/abs/2309.12311) | LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as a... | LLM-Grounder introduces a zero-shot, open-vocabulary 3D visual grounding approach using LLMs as agents to decompose queries, reason about spatial rela... | Unlike prior CLIP-based methods that suffer from 'bag-of-words' limitations, LLM-Grounder leverages LLMs for compositional semantic decomposition and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2307.00329-b31b1b.svg)](https://arxiv.org/abs/2307.00329) | DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution... | The paper introduces DoReMi, a framework for grounding language models in robotic tasks by detecting and recovering from plan-execution misalignment.... | Unlike prior work that assumes perfect execution of low-level skills or relies on manual feedback, DoReMi introduces a hierarchical framework with aut... |
| [![arXiv](https://img.shields.io/badge/arXiv-2308.01399-b31b1b.svg)](https://arxiv.org/abs/2308.01399) | Learning to Model the World with Language | The paper introduces Dynalang, a multimodal agent that unifies language understanding with future prediction as a self-supervised learning objective.... | Unlike prior work that relies on task-specific language instructions or supervised learning, Dynalang uses a world model to predict future text/image... |
| [![arXiv](https://img.shields.io/badge/arXiv-2308.01552-b31b1b.svg)](https://arxiv.org/abs/2308.01552) | InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent | The paper explores integrating ChatGPT into embodied agent systems through role-based prompt engineering, demonstrating high success rates in simulate... | Unlike prior work, InterAct leverages ChatGPT's versatility through role assignment (e.g., checker, sorter) and prompt engineering to enhance cooperat... |
| [![arXiv](https://img.shields.io/badge/arXiv-2307.08962-b31b1b.svg)](https://arxiv.org/abs/2307.08962) | REX: Rapid Exploration and eXploitation for AI Agents | The paper introduces REX, a framework enhancing AI agents' exploration and exploitation by integrating UCB-inspired rewards and offline behavior utili... | REX differs from related work by incorporating UCB-based reward structures and try-and-fail procedures akin to RL, enabling efficient offline behavior... |
| [![arXiv](https://img.shields.io/badge/arXiv-2308.05221-b31b1b.svg)](https://arxiv.org/abs/2308.05221) | Alexa, play with robot: Introducing the First Alexa Prize SimBot Challenge on Em... | The paper introduces the SimBot Challenge as a new competition for developing embodied AI agents, emphasizing the integration of vision, language, and... | This work differs from related work by establishing the SimBot Challenge as a benchmark for embodied AI, focusing on task completion in simulated phys... |
| [![arXiv](https://img.shields.io/badge/arXiv-2308.13782-b31b1b.svg)](https://arxiv.org/abs/2308.13782) | Planning with Logical Graph-based Language Model for Instruction Generation | The paper introduces Logical-GLM, a graph-based language model that integrates logical Bayesian graphs with language models to enhance instruction gen... | Unlike prior work that relies solely on pre-trained LLMs or static knowledge graphs, Logical-GLM combines AI planning with dynamic logical graph struc... |
| [![arXiv](https://img.shields.io/badge/arXiv-2308.02223-b31b1b.svg)](https://arxiv.org/abs/2308.02223) | ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation | The paper introduces two-stage sampling and dynamic sampling approaches to enhance the efficiency of reinforcement learning (RL) in sequence generatio... | This work differs from related work by focusing on optimizing sampling efficiency in RL for sequence generation, rather than proposing new architectur... |
| [![arXiv](https://img.shields.io/badge/arXiv-2308.10088-b31b1b.svg)](https://arxiv.org/abs/2308.10088) | PACE: Improving Prompt with Actor-Critic Editing for Large Language Model | The paper introduces PACE, a novel prompt editing framework leveraging actor-critic reinforcement learning to enhance LLM performance. It addresses pr... | PACE differs from related work by using actor-critic within LLMs themselves for prompt optimization, avoiding external reward models or training. It e... |
| [![arXiv](https://img.shields.io/badge/arXiv-2307.08699-b31b1b.svg)](https://arxiv.org/abs/2307.08699) | Pair then Relation: Pair-Net for Panoptic Scene Graph Generation | This paper addresses the challenge of Panoptic Scene Graph (PSG) generation by introducing Pair-Net, a novel framework that improves inter-object pair... | The work introduces Pair-Net, a novel architecture that directly learns sparse pairwise relationships via a Matrix Learner within the PPN, improving u... |
| [![arXiv](https://img.shields.io/badge/arXiv-2307.06082-b31b1b.svg)](https://arxiv.org/abs/2307.06082) | VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation... | VELMA introduces a novel approach to urban Vision and Language Navigation (VLN) by leveraging large language models (LLMs) with verbalization of traje... | VELMA differs from prior work by employing a verbalization-based embodiment strategy, where navigation instructions, trajectories, and visual observat... |
| [![arXiv](https://img.shields.io/badge/arXiv-2308.15962-b31b1b.svg)](https://arxiv.org/abs/2308.15962) | WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model | Analysis failed: Batch generation error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 896.00 KiB i... | Analysis failed: Batch generation error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 896.00 KiB i... |


### 2023: Q4

| arXiv | Title | Summary | Contributions |
|-------|-------|---------|---------------|
| [![arXiv](https://img.shields.io/badge/arXiv-2310.09478-b31b1b.svg)](https://arxiv.org/abs/2310.09478) | MiniGPT-v2: large language model as a unified interface for vision-language mult... | The paper introduces MiniGPT-v2, a unified interface leveraging large language models for vision-language tasks. Key contributions include task-specif... | Unlike prior work, MiniGPT-v2 introduces task-specific identifiers during training to enhance task distinction and learning efficiency, combined with... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.08914v2-b31b1b.svg)](https://arxiv.org/abs/2312.08914) | CogAgent: A Visual Language Model for GUI Agents | CogAgent introduces a specialized visual language model (VLM) for GUI agents, achieving state-of-the-art performance on VQA benchmarks and GUI navigat... | CogAgent differs by directly processing GUI screenshots (not HTML/OCR) with a high-resolution VLM architecture, enabling human-level GUI understanding... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.11441-b31b1b.svg)](https://arxiv.org/abs/2310.11441) | Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V | The paper introduces Set-of-Mark (SoM), a novel visual prompting method that enhances visual grounding capabilities of large multimodal models like GP... | This work differs from related work by focusing on prompt engineering rather than model architecture or training methods, enabling zero-shot visual gr... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.07562-b31b1b.svg)](https://arxiv.org/abs/2311.07562) | GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navig... | The paper introduces MM-Navigator, a GPT-4V-based agent for zero-shot smartphone GUI navigation, demonstrating high accuracy in action description and... | This work differs from related work by leveraging GPT-4V's advanced screen interpretation and action reasoning capabilities for zero-shot GUI navigati... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.07704-b31b1b.svg)](https://arxiv.org/abs/2310.07704) | Ferret: Refer and Ground Anything Anywhere at Any Granularity | Ferret introduces a unified framework for referring and grounding in images, utilizing a hybrid region representation and spatial-aware visual sampler... | Ferret differs from related work by unifying referring and grounding in a single framework, supporting diverse region inputs (points, boxes, shapes),... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.06607-b31b1b.svg)](https://arxiv.org/abs/2311.06607) | Monkey: Image Resolution and Text Label Are Important Things for Large Multi-mod... | The paper introduces Monkey, a method to enhance Large Multimodal Models (LMMs) by addressing high-resolution image processing and detailed scene unde... | Monkey differs from related work by introducing a patch-based processing module with sliding window and LoRA adjustments for efficient high-resolution... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.16886-b31b1b.svg)](https://arxiv.org/abs/2312.16886) | MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices | MobileVLM introduces mobile-optimized multimodal vision language models with efficient cross-modality interaction via a lightweight projector, achievi... | MobileVLM differs from prior work by emphasizing mobile-specific optimizations (e.g., efficient projector, compact language models) and achieving comp... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.03716-b31b1b.svg)](https://arxiv.org/abs/2310.03716) | A Long Way to Go: Investigating Length Correlations in RLHF | The paper highlights that optimizing response length is a critical, underappreciated factor in RLHF, demonstrating that length-based rewards can repli... | This work differs from related work by systematically analyzing length as a core feature in RLHF, challenging the assumption that length increases are... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.05126-b31b1b.svg)](https://arxiv.org/abs/2310.05126) | UReader: Universal OCR-free Visually-situated Language Understanding with Multim... | URender introduces an OCR-free approach for visually-situated language understanding using a Multimodal Large Language Model (MLLM). Key contributions... | This work differs from related work by eliminating the need for domain-specific pretraining and fine-tuning, leveraging a minimal parameter fine-tunin... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.06114-b31b1b.svg)](https://arxiv.org/abs/2310.06114) | Learning Interactive Real-World Simulators | The paper introduces UniSim, a universal simulator that integrates diverse datasets (image, video, robotics, navigation) to simulate real-world intera... | Unlike prior work focused on domain-specific simulations or limited data, UniSim unifies heterogeneous datasets in a conditional video generation fram... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.03356-b31b1b.svg)](https://arxiv.org/abs/2311.03356) | GLaMM: Pixel Grounding Large Multimodal Model | GLaMM introduces the first model capable of generating natural language responses with object segmentation masks, enabling visually grounded conversat... | GLaMM unifies region understanding, pixel-level grounding, and conversational abilities through end-to-end training, differing from prior works that e... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.13108-b31b1b.svg)](https://arxiv.org/abs/2312.13108) | ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation | This paper introduces AssistGUI, a novel benchmark for evaluating GUI automation on Windows desktop environments, along with an Actor-Critic Embodied... | Unlike prior work on mobile/web or terminal automation, this paper focuses on desktop GUI automation with a comprehensive benchmark (AssistGUI) and a... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.14135-b31b1b.svg)](https://arxiv.org/abs/2312.14135) | V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs | The paper introduces V*, an LLM-guided visual search mechanism to enhance multimodal LLMs (MLLMs) with efficient visual querying, leading to the SEAL... | The work differs from related work by leveraging rich common sense knowledge from LLMs for dynamic visual search, incorporating both top-down feature... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.04498-b31b1b.svg)](https://arxiv.org/abs/2311.04498) | NExT-Chat: An LMM for Chat, Detection and Segmentation | The paper introduces the pix2emb paradigm for region-level visual understanding, enabling multiple location formats (bounding boxes, masks) through em... | Unlike prior pix2seq methods limited to discrete coordinates, pix2emb uses embeddings decoded into diverse formats via specialized decoders. This arch... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.08588-b31b1b.svg)](https://arxiv.org/abs/2310.08588) | Octopus: Embodied Vision-Language Programmer from Environmental Feedback | Introduces Octopus, an embodied vision-language programmer that bridges high-level planning and real-world manipulation via executable code generation... | Octopus differs by integrating code generation as a medium between planning and manipulation, addressing gaps in prior works that either output low-le... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.03720v2-b31b1b.svg)](https://arxiv.org/abs/2310.03720) | SteP: Stacked LLM Policies for Web Actions | SteP introduces a dynamic policy composition framework for web tasks using stacked LLM policies, addressing challenges in handling combinatorial web i... | SteP differs from related work by enabling dynamic policy invocation and stacking, allowing adaptive control over task complexity. Unlike static hiera... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.17842-b31b1b.svg)](https://arxiv.org/abs/2311.17842) | Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language P... | The paper introduces ViLa, a novel robotic planning framework that integrates vision-language models (VLMs) for grounded task planning. It emphasizes... | Unlike prior work relying on external affordance models or LLMs with limited environmental grounding, ViLa unifies vision and language processing with... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.08740-b31b1b.svg)](https://arxiv.org/abs/2310.08740) | A Zero-Shot Language Agent for Computer Control with Structured Reflection | The paper introduces a zero-shot language agent for computer control that leverages structured reflection to autonomously learn from mistakes without... | This work differs from related work by eliminating the reliance on expert traces or additional screen information, instead using structured self-refle... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.16862-b31b1b.svg)](https://arxiv.org/abs/2312.16862) | TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones | Introduces TinyGPT-V, an efficient multimodal large language model (MLLM) with a compact architecture, enabling low-resource training (24GB) and infer... | Differently from prior MLLMs relying on large backbones (e.g., LLaMA2-7B, Vicuna-13B), TinyGPT-V uses a smaller Phi-2 backbone with optimized architec... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.04716v1-b31b1b.svg)](https://arxiv.org/abs/2310.04716) | Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API | The paper introduces a multimodal model for grounding natural language instructions in UI screenshots, leveraging a visual encoder and language decode... | This work differs by integrating reinforcement learning with visual and language components for spatial decoding, proposing a metadata-free grounding... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.02949-b31b1b.svg)](https://arxiv.org/abs/2312.02949) | LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models | The paper introduces LLaVA-Grounding, a model that combines visual chat and grounding capabilities. It creates a new dataset (GVC) for grounded visual... | This work differs from related work by integrating chat and grounding tasks into a single model, creating a dedicated dataset and benchmark for ground... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.13724-b31b1b.svg)](https://arxiv.org/abs/2310.13724) | Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots | Habitat 3.0 introduces a simulation platform for collaborative human-robot tasks in home environments, focusing on accurate humanoid simulation, human... | Habitat 3.0 differs from related work by integrating humanoid and robot simulation in the same environment, supporting HITL control via VR/mouse/keybo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.16042-b31b1b.svg)](https://arxiv.org/abs/2310.16042) | WebWISE: Web Interface Control and Sequential Exploration with Large Language Mo... | The paper introduces a method for web interface control using LLMs with in-context learning, leveraging filtered DOM elements as observations and sequ... | WebWISE differs from related work by using DOM observations and in-context learning with minimal examples (single manual or auto-generated example) in... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.04869-b31b1b.svg)](https://arxiv.org/abs/2310.04869) | ILuvUI: Instruction-tuned LangUage-Vision modeling of UIs from Machine Conversat... | The paper addresses UI task challenges in Vision-Language Models (VLMs) by generating a large-scale conversational dataset without human annotations.... | Unlike prior work requiring human annotations, this study generates UI training data automatically using machine conversations, enabling VLMs to handl... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.03815-b31b1b.svg)](https://arxiv.org/abs/2312.03815) | LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem | The paper introduces the AIOS-Agent ecosystem, positioning Large Language Models (LLMs) as the core of an intelligent operating system (AIOS) and agen... | This work differs from related work by proposing a conceptual framework where LLMs function as the system-level OS (LLMOS), enabling agents as applica... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.07472-b31b1b.svg)](https://arxiv.org/abs/2312.07472) | MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception | The paper introduces MP5, a multi-modal embodied system for open-ended tasks in Minecraft, emphasizing active perception, modular architecture, and co... | MP5 differs from prior work by introducing a modular architecture with active perception and context-aware planning, addressing limitations of previou... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.02684-b31b1b.svg)](https://arxiv.org/abs/2311.02684) | Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE | The paper introduces Octavius, a framework addressing task interference in Multimodal Large Language Models (MLLMs) by integrating Mixture-of-Experts... | The work combines MoE with LoRA to create a novel decoder (LoRA-MoE) for MLLMs, addressing task interference through specialized learning paths. This... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.12871-b31b1b.svg)](https://arxiv.org/abs/2311.12871) | An Embodied Generalist Agent in 3D World | The paper introduces LEO, an embodied multi-modal generalist agent designed for 3D world interaction, addressing limitations in 3D perception, groundi... | LEO extends beyond existing 2D-focused generalist models by introducing a unified 3D vision-language-action architecture, a novel 3D dataset with obje... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.15209-b31b1b.svg)](https://arxiv.org/abs/2311.15209) | See and Think: Embodied Agent in Virtual Environment | The paper introduces STEVE, an embodied agent combining vision perception, language instruction, and code action for Minecraft. It proposes a multi-mo... | STEVE differs from prior work by integrating vision perception with LLM-based reasoning and code action generation, enabling more precise and autonomo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.13255-b31b1b.svg)](https://arxiv.org/abs/2310.13255) | Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Wo... | The paper introduces Steve-Eye, a multimodal model integrating LLMs with visual perception for open-world embodied agents. It addresses limitations in... | Unlike prior work focusing on text-based interactions, Steve-Eye integrates visual perception with LLMs, introduces a large-scale open-world instructi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.08235-b31b1b.svg)](https://arxiv.org/abs/2310.08235) | GROOT: Learning to Follow Instructions by Watching Gameplay Videos | This work introduces GROOT, a novel agent that learns to follow open-ended instructions by watching gameplay videos, addressing limitations in existin... | GROOT differs from prior work by using reference gameplay videos as goal specifications instead of text or static images, enabling self-imitation lear... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.10170v4-b31b1b.svg)](https://arxiv.org/abs/2312.10170) | UINav: A Practical Approach to Train On-Device Automation Agents | UINav introduces a demonstration-based approach for training on-device UI automation agents with high accuracy using minimal human demonstrations. It... | UINav differs from prior work by combining demonstration-based training with a referee model for real-time feedback and data augmentation, achieving h... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.08649-b31b1b.svg)](https://arxiv.org/abs/2311.08649) | Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing | The paper introduces DroidAgent, an autonomous GUI testing agent for Android that leverages Large Language Models (LLMs) and memory mechanisms for int... | This work differs from related work by integrating LLMs with memory systems to enable high-level semantic testing, moving beyond traditional code cove... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.11190-b31b1b.svg)](https://arxiv.org/abs/2312.11190) | VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM... | The paper introduces VisionTasker, a two-stage framework for mobile task automation combining vision-based UI understanding and LLM-driven step-by-ste... | VisionTasker differs from related work by replacing view hierarchies with vision-based UI interpretation and integrating LLMs for dynamic, step-by-ste... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.10505-b31b1b.svg)](https://arxiv.org/abs/2310.10505) | ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Alig... | ReMax introduces a reinforcement learning method tailored for aligning large language models (LLMs) by leveraging RLHF properties (fast simulation, de... | ReMax differs from prior work by exploiting RLHF-specific properties and using a greedy baseline in REINFORCE for variance reduction, eliminating the... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.11444-b31b1b.svg)](https://arxiv.org/abs/2312.11444) | An In-depth Look at Gemini's Language Abilities | The paper provides a third-party, reproducible comparison of Google's Gemini and OpenAI's GPT models across 10 language tasks, identifies performance... | This work differs from related work by offering an independent, transparent evaluation of Gemini's language capabilities through reproducible experime... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.02519-b31b1b.svg)](https://arxiv.org/abs/2312.02519) | Creative Agents: Empowering Agents with Imagination for Creative Tasks | The paper introduces creative agents enhanced with an 'imaginator' component to generate task outcomes from language instructions, enabling novel solu... | This work differs from related work by integrating an imaginator for generating task-specific imaginations, enabling creativity in agents. It also int... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.05861-b31b1b.svg)](https://arxiv.org/abs/2310.05861) | Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Mod... | The paper addresses underspecification in vision-language tasks by introducing RepARe, a framework that rephrases and augments questions with visually... | Unlike prior work focusing on image captioning or CoT reasoning, RepARe explicitly leverages visual grounding in question rephrasing, utilizing LVLMs... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.18751v2-b31b1b.svg)](https://arxiv.org/abs/2311.18751) | Exposing Limitations of Language Model Agents in Sequential-Task Compositions on... | The paper introduces a new benchmark (CompWoB) for evaluating compositional web automation tasks, highlights the limitations of language model agents... | This work differs from related work by focusing on compositional task compositionality rather than isolated tasks, introducing a new benchmark (CompWo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.02071-b31b1b.svg)](https://arxiv.org/abs/2310.02071) | Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model... | The paper introduces PCA-EVAL, a benchmark for evaluating embodied decision-making in perception, cognition, and action. It proposes HOLMES, a multi-a... | This work differs by introducing PCA-EVAL as a comprehensive benchmark and HOLMES as a multi-agent framework that directly integrates MLLMs for end-to... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.08825-b31b1b.svg)](https://arxiv.org/abs/2310.08825) | From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models | The paper investigates the effectiveness of different visual encoders in MLLMs, highlighting the advantages of CLIP's shallow layers for fine-grained... | This work differs from related work by analyzing the role of visual encoder depth and introducing COMM, a multi-level feature merging architecture tha... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.06147-b31b1b.svg)](https://arxiv.org/abs/2312.06147) | "What's important here?": Opportunities and Challenges of Using LLMs in Retrievi... | The paper investigates LLMs' ability to retrieve important UI elements from web pages based on user queries, focusing on prompting strategies (example... | Unlike prior works focused on autonomous web navigation, this study decomposes the problem into atomic operations, analyzing how prompting strategies... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.02976-b31b1b.svg)](https://arxiv.org/abs/2312.02976) | SPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Ma... | The paper introduces SPOC, a transformer-based architecture that imitates shortest-path planners in simulation to enable real-world navigation and man... | This work differs by using shortest-path expert trajectories instead of human demonstrations or dense rewards, combining a transformer architecture wi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.13435-b31b1b.svg)](https://arxiv.org/abs/2311.13435) | PG-Video-LLaVA: Pixel Grounding Large Video-Language Models | PG-Video-LLaVA introduces the first video-based Large Multimodal Model (LMM) with pixel-level grounding capabilities, integrating audio cues via trans... | This work differs from related work by introducing pixel-level visual grounding in videos, integrating audio context through transcription, proposing... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.06791-b31b1b.svg)](https://arxiv.org/abs/2311.06791) | InfMLLM: A Unified Framework for Visual-Language Tasks | The paper introduces InfMLLM, a unified framework for visual-language tasks, employing a three-stage training scheme (lightweight alignment pretrainin... | InfMLLM differs from prior MLLMs by introducing a progressive training strategy with stage-specific objectives and a novel pool-adapter architecture t... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.13518-b31b1b.svg)](https://arxiv.org/abs/2310.13518) | Vision-Based Mobile App GUI Testing: A Survey | The paper provides a comprehensive survey of vision-based mobile app GUI testing techniques, highlighting their advantages over traditional code/layou... | This work differs from related work by offering a holistic survey of 271 papers (92 vision-based), systematically categorizing GUI testing topics, and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.16714-b31b1b.svg)](https://arxiv.org/abs/2311.16714) | Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld | The paper introduces EMMA, an Embodied Multi-Modal Agent trained via cross-modality imitation learning between a text-world LLM and a visual-world VLM... | This work differs from related work by combining cross-modal distillation from an LLM expert with a novel DAgger-DPO algorithm, enabling embodied VLM... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.17722-b31b1b.svg)](https://arxiv.org/abs/2310.17722) | Large Language Models as Generalizable Policies for Embodied Tasks | The paper introduces LLaRP, a method that adapts large language models (LLMs) as generalizable policies for embodied visual tasks through reinforcemen... | LLaRP differs from prior work by using reinforcement learning directly on pre-trained LLMs for embodied tasks without requiring task-specific training... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.07263-b31b1b.svg)](https://arxiv.org/abs/2310.07263) | CoPAL: Corrective Planning of Robot Actions with Large Language Models | The paper introduces CoPAL, a hierarchical architecture for robot task planning that integrates multi-level feedback loops with Large Language Models... | This work differs from related work by proposing a closed-loop task planning architecture (CoPAL) that systematically integrates geometric constraints... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.18651-b31b1b.svg)](https://arxiv.org/abs/2311.18651) | LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasonin... | LL3DA introduces a novel approach for 3D scene understanding, reasoning, and planning by directly processing point cloud inputs, enabling interaction... | LL3DA differs from prior work by directly encoding 3D point clouds instead of using multi-view projections, employing an attention mechanism to integr... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.03602-b31b1b.svg)](https://arxiv.org/abs/2310.03602) | Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraint... | Ctrl-Room introduces a two-stage framework for text-driven 3D room generation with layout constraints, enabling structurally plausible layouts, high-f... | Unlike prior methods that struggle with global layout consistency or lack editability, Ctrl-Room decouples layout and appearance generation, employs s... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.07968-b31b1b.svg)](https://arxiv.org/abs/2310.07968) | Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation | The paper introduces Zero-shot Interactive Personalized Object Navigation (ZIPON), a task that combines zero-shot object navigation with natural langu... | This work differs from related work by introducing ORION, a framework that leverages LLMs for sequential decision-making in interactive navigation, in... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.02219-b31b1b.svg)](https://arxiv.org/abs/2312.02219) | Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Langu... | The paper introduces MERLIM, a multi-modal benchmark for evaluating Instruction Tuning Large Vision and Language Models (IT-LVLMs) on fundamental comp... | This work differs from related work by introducing MERLIM, a standardized benchmark specifically designed to evaluate IT-LVLMs for fundamental vision... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.02046-b31b1b.svg)](https://arxiv.org/abs/2310.02046) | Improving web element localization by using a large language model | The paper introduces VON Similo LLM, an enhanced web element localization approach leveraging Large Language Models (LLMs) to improve accuracy by inco... | Unlike prior post-repair approaches (e.g., WATER, GPT-2-based methods) that rely on attribute comparisons or heuristic repairs, this work integrates L... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.07937-b31b1b.svg)](https://arxiv.org/abs/2310.07937) | Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation Using Vision Langu... | The paper introduces Co-NavGPT, a framework that integrates Vision Language Models (VLMs) as global planners for multi-robot cooperative visual target... | Co-NavGPT differs from related work by combining VLMs with multi-robot systems for semantic reasoning and global planning, enabling efficient collabor... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.10763-b31b1b.svg)](https://arxiv.org/abs/2312.10763) | M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts | The paper introduces M3DBench, a comprehensive multi-modal 3D instruction-following dataset that supports diverse 3D tasks, interleaved visual prompts... | Unlike prior 3D datasets focused on specific tasks (e.g., object detection, navigation), M3DBench unifies region and scene-level 3D tasks with multi-m... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.02635-b31b1b.svg)](https://arxiv.org/abs/2310.02635) | Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficientl... | The paper introduces RLFP framework with FAC algorithm, leveraging foundation models for policy, value, and reward priors to enhance sample efficiency... | This work differs from related work by integrating foundation priors (policy, value, success-reward) into RL, enabling automatic reward functions and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.13373-b31b1b.svg)](https://arxiv.org/abs/2311.13373) | Large Language Model as a Policy Teacher for Training Reinforcement Learning Age... | This paper introduces a framework that combines Large Language Models (LLMs) with reinforcement learning (RL) to train specialized student agents. The... | This work differs from related work by proposing a knowledge distillation framework where an LLM acts as a teacher to guide a specialized RL student a... |
| [![arXiv](https://img.shields.io/badge/arXiv-2310.05136-b31b1b.svg)](https://arxiv.org/abs/2310.05136) | InstructDET: Diversifying Referring Object Detection with Generalized Instructio... | The paper introduces InstructDET, a data-centric approach to referring object detection (ROD) that generates diverse instructions for object localizat... | This work differs from related work by (1) generating diverse, human-like instructions via foundation models rather than manual annotation, (2) creati... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.10103-b31b1b.svg)](https://arxiv.org/abs/2312.10103) | GSVA: Generalized Segmentation via Multimodal Large Language Models | The paper introduces GSVA, a novel approach to Generalized Referring Expression Segmentation (GRES) that addresses multi-target and empty-target scena... | GSVA differs from prior work by introducing the [REJ] token for explicit rejection of empty targets and enabling simultaneous segmentation of multiple... |
| [![arXiv](https://img.shields.io/badge/arXiv-2311.03783-b31b1b.svg)](https://arxiv.org/abs/2311.03783) | Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI | The paper introduces a scene-driven multimodal knowledge graph (Scene-MMKG) construction method for embodied AI, combining knowledge engineering with... | The work differs from related work by proposing a unified scene knowledge injection framework that integrates conventional knowledge bases with large... |
| [![arXiv](https://img.shields.io/badge/arXiv-2312.15820v1-b31b1b.svg)](https://arxiv.org/abs/2312.15820) | WebVLN: Vision-and-Language Navigation on Websites | Introduces the WebVLN task for navigating websites using natural language instructions, incorporates HTML content alongside visual and textual data, a... | Differs from related work by explicitly incorporating HTML content (non-visual web-specific data) and introducing a new task and benchmark focused on... |


### 2024: Q1

| arXiv | Title | Summary | Contributions |
|-------|-------|---------|---------------|
| [![arXiv](https://img.shields.io/badge/arXiv-2401.01614-b31b1b.svg)](https://arxiv.org/abs/2401.01614) | GPT-4V(ision) is a Generalist Web Agent, if Grounded | This work introduces SEEACT, a generalist web agent leveraging large multimodal models (LMMs) like GPT-4V for integrated visual understanding and web... | This work differs from related work by proposing a novel grounding strategy that integrates HTML structure and visuals for web agents, rather than rel... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.10935v2-b31b1b.svg)](https://arxiv.org/abs/2401.10935) | SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents | The paper introduces SeeClick, a visual GUI agent that automates tasks using screenshots instead of structured data, addresses the GUI grounding chall... | SeeClick differs from prior work by eliminating reliance on structured text (e.g., HTML) and GUI metadata, leveraging LVLMs for direct screenshot-base... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.13649v2-b31b1b.svg)](https://arxiv.org/abs/2401.13649) | VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks | Introduces VisualWebArena, a benchmark for evaluating multimodal agents on visually grounded web tasks, emphasizing integration of visual and textual... | VisualWebArena fills the gap in evaluating multimodal agents on visually grounded tasks, offering a comprehensive benchmark with real-world tasks and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.16158-b31b1b.svg)](https://arxiv.org/abs/2401.16158) | Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception | The paper introduces Mobile-Agent, a multi-modal agent that uses visual perception to operate mobile apps without relying on XML metadata. It presents... | Mobile-Agent differs from prior work by employing a vision-centric approach without requiring XML or system metadata, introducing Mobile-Eval as a new... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.05525-b31b1b.svg)](https://arxiv.org/abs/2403.05525) | DeepSeek-VL: Towards Real-World Vision-Language Understanding | DeepSeek-VL introduces a hybrid vision encoder for efficient high-resolution image processing, a comprehensive real-world dataset with diverse modalit... | DeepSeek-VL differentiates from prior work by combining vision and language pretraining with a hybrid vision encoder, addressing limitations of projec... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.13919-b31b1b.svg)](https://arxiv.org/abs/2401.13919) | WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models | This paper introduces WebVoyager, a multimodal web agent leveraging large multimodal models (LMMs) to interact with real-world websites end-to-end. It... | WebVoyager differs from prior work by enabling real-world web navigation through multimodal inputs (screenshots and text), utilizing a novel benchmark... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.07939-b31b1b.svg)](https://arxiv.org/abs/2402.07939) | UFO: A UI-Focused Agent for Windows OS Interaction | UFO introduces a dual-agent framework for Windows OS interaction, leveraging GPT-Vision to analyze GUIs and controls, enabling automated task executio... | Unlike prior LLM agents focused on general task planning or code-centric approaches, UFO specifically targets Windows OS UI automation with a dual-age... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.11684-b31b1b.svg)](https://arxiv.org/abs/2402.11684) | ALLaVA: Harnessing GPT4V-Synthesized Data for Lite Vision-Language Models | The paper introduces a synthetic dataset (ALLaVA) generated via GPT4V to enhance lite vision-language models (VLMs), achieving performance comparable... | This work differs by focusing on high-quality synthetic data generation (ALLaVA) to bridge performance gaps between lite and large VLMs, rather than r... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.07456-b31b1b.svg)](https://arxiv.org/abs/2402.07456) | OS-Copilot: Towards Generalist Computer Agents with Self-Improvement | The paper introduces OS-Copilot, a framework for building generalist computer agents capable of interacting with diverse OS elements. It presents FRID... | This work differs from related work by focusing on generalist agents for OS interactions, introducing self-improvement mechanisms, and demonstrating s... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.05930v2-b31b1b.svg)](https://arxiv.org/abs/2402.05930) | WebLINX: Real-World Website Navigation with Multi-Turn Dialogue | The paper introduces WEBLINX, a large-scale benchmark for conversational web navigation, and proposes a retrieval-inspired model to address the challe... | This work differs from related work by introducing a novel benchmark (WEBLINX) and a retrieval-inspired architecture tailored for web navigation tasks... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.02713v2-b31b1b.svg)](https://arxiv.org/abs/2403.02713) | Android in the Zoo: Chain-of-Action-Thought for GUI Agents | This work introduces Chain-of-Action-Thought (CoAT) for GUI agents, emphasizing semantic reasoning through screen context, action thinking, targets, a... | Unlike prior works focusing solely on coordinate-based actions or separating element recognition from action inference, CoAT integrates semantic reaso... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.05935-b31b1b.svg)](https://arxiv.org/abs/2402.05935) | SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language... | SPHINX-X introduces a scalable family of multi-modal large language models (MLLMs) with architectural optimizations, a comprehensive multimodal datase... | This work differs by scaling data and parameters, modifying the SPHINX architecture with skip tokens and one-stage training, and curating a diverse da... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.17553v3-b31b1b.svg)](https://arxiv.org/abs/2402.17553) | OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous A... | Introduces OmniACT, the first dataset and benchmark for evaluating autonomous agents' ability to generate executable scripts for both desktop and web... | OmniACT differs from prior work by combining desktop and web tasks, requiring executable script generation rather than just action prediction, and emp... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.06209-b31b1b.svg)](https://arxiv.org/abs/2401.06209) | Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs | Identifies systematic visual shortcomings in multimodal LLMs (MLLMs) stemming from CLIP-based visual encoders, introduces the MMVP benchmark using CLI... | Systematically exposes the limitations of CLIP-based visual encoders in MLLMs, highlights the disconnect between CLIP's embedding space and vision-onl... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.04473-b31b1b.svg)](https://arxiv.org/abs/2403.04473) | TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document | TextMonkey introduces an OCR-Free large multimodal model for document understanding, enhancing text-centric tasks through Shifted Window Attention wit... | Unlike OCR-Model-Driven methods, TextMonkey eliminates reliance on external OCR systems by integrating text detection, layout understanding, and visua... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.05459-b31b1b.svg)](https://arxiv.org/abs/2401.05459) | Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Se... | The paper provides a comprehensive survey of Personal LLM Agents, focusing on their architecture, capabilities, efficiency, and security. It highlight... | This work differs from related work by offering a systematic survey and analysis of Personal LLM Agents, emphasizing their integration with personal d... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.04615v3-b31b1b.svg)](https://arxiv.org/abs/2402.04615) | ScreenAI: A Vision-Language Model for UI and Infographics Understanding | ScreenAI introduces a vision-language model specialized in understanding UIs and infographics, combining PaLI architecture with Pix2Struct's flexible... | ScreenAI differs from related work by unifying UI and infographic understanding through a novel visual language model architecture, leveraging a hybri... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.03186-b31b1b.svg)](https://arxiv.org/abs/2403.03186) | Cradle: Empowering Foundation Agents Towards General Computer Control | The paper introduces the General Computer Control (GCC) setting to standardize interaction with software via screenshots and low-level actions, propos... | This work differs from related work by introducing a standardized interface (screenshots + keyboard/mouse actions), a modular framework (Cradle) that... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.04706-b31b1b.svg)](https://arxiv.org/abs/2403.04706) | Common 7B Language Models Already Possess Strong Math Capabilities | The paper demonstrates that LLaMA-2 7B models possess strong mathematical capabilities without specialized pre-training, achieving high accuracy on GS... | This work differs from related work by focusing on enhancing small language models' mathematical abilities through efficient SFT and synthetic data sc... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.07718v5-b31b1b.svg)](https://arxiv.org/abs/2403.07718) | WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks? | The paper introduces WorkArena, a benchmark for evaluating web agents on enterprise software tasks, and BrowserGym, a unified environment for agent ev... | WorkArena focuses on enterprise software workflows (ServiceNow) unlike prior benchmarks (e.g., WebArena, MiniWoB), and introduces BrowserGym as a flex... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.02716-b31b1b.svg)](https://arxiv.org/abs/2402.02716) | Understanding the planning of LLM agents: A survey | The paper provides the first systematic taxonomy of LLM-based agent planning, categorizing existing works into Task Decomposition, Plan Selection, Ext... | This work differs from related work by offering the first comprehensive survey and structured taxonomy of LLM-Agent planning, synthesizing existing re... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.01680-b31b1b.svg)](https://arxiv.org/abs/2402.01680) | Large Language Model based Multi-Agents: A Survey of Progress and Challenges | The paper provides a comprehensive survey of LLM-based multi-agent systems, focusing on their applications in complex problem-solving and world simula... | This work differentiates itself by offering a holistic survey of LLM-based multi-agent systems, emphasizing collective intelligence, inter-agent inter... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.12895-b31b1b.svg)](https://arxiv.org/abs/2403.12895) | mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding | The paper introduces Unified Structure Learning for OCR-free Visual Document Understanding, emphasizing structure-aware parsing tasks and multi-graine... | Unlike prior OCR-dependent or limited-domain approaches, this work unifies structure learning across diverse text-rich images (documents, webpages, ta... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.07945v1-b31b1b.svg)](https://arxiv.org/abs/2402.07945) | ScreenAgent: A Vision Language Model-driven Computer Control Agent | The paper introduces ScreenAgent, a Vision Language Model (VLM)-driven computer control agent that interacts with real computer screens via GUI manipu... | Unlike prior work, ScreenAgent combines VLM with a task-specific control pipeline and dataset for real-world GUI interaction, achieving superior UI po... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.02330-b31b1b.svg)](https://arxiv.org/abs/2401.02330) | LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model | The paper introduces LLaVA-Phi, a compact multi-modal assistant leveraging the Phi-2 small language model for efficient visual-dialogue tasks. It demo... | LLaVA-Phi differs from related work by combining a small language model (Phi-2) with LLaVA-1.5 training methodology, achieving efficiency without sacr... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.17918v2-b31b1b.svg)](https://arxiv.org/abs/2403.17918) | AgentStudio: A Toolkit for Building General Virtual Agents | AgentStudio introduces a comprehensive toolkit for building general virtual agents, featuring a lightweight interactive environment with generic obser... | Unlike prior domain-specific simulators (e.g., WebShop, AndroidEnv) or narrow benchmarks, AgentStudio provides a generic, real-world compatible enviro... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.01622-b31b1b.svg)](https://arxiv.org/abs/2402.01622) | TravelPlanner: A Benchmark for Real-World Planning with Language Agents | Introduces TravelPlanner, a benchmark for evaluating language agents in complex real-world planning tasks. Highlights challenges in multi-constraint p... | TravelPlanner differs from prior work by focusing on real-world, multi-constraint travel planning scenarios with a rich sandbox environment, diverse t... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.03163-b31b1b.svg)](https://arxiv.org/abs/2403.03163) | Design2Code: Benchmarking Multimodal Code Generation for Automated Front-End Eng... | The paper introduces Design2Code, the first real-world benchmark for multimodal code generation in front-end engineering. It evaluates current MLLMs o... | This work differs from related studies by creating the first benchmark specifically for visual-to-code generation, providing detailed evaluation metri... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.09029-b31b1b.svg)](https://arxiv.org/abs/2403.09029) | Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dat... | The paper introduces WebSight, a large-scale synthetic dataset of 2 million HTML-code-to-screenshot pairs, and Sightseer, a vision-language model with... | Unlike prior work that relied on small datasets or heuristic-based approaches, this work introduces WebSight, a synthetic dataset with 2.5x larger siz... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.15116-b31b1b.svg)](https://arxiv.org/abs/2402.15116) | Large Multimodal Agents: A Survey | The paper provides a systematic review of Large Multimodal Agents (LMAs), categorizing research into four types, compiling evaluation methodologies, p... | This work differs from related work by offering a comprehensive survey of LMAs, unifying diverse research categories, standardizing evaluation framewo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.03568-b31b1b.svg)](https://arxiv.org/abs/2401.03568) | Agent AI: Surveying the Horizons of Multimodal Interaction | The paper defines 'Agent AI' as a framework for embodied multimodal systems that integrate visual, linguistic, and environmental data to enable contex... | This work differs from related research by systematically framing Agent AI as a class of systems that prioritize environmental grounding and multimoda... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.04124-b31b1b.svg)](https://arxiv.org/abs/2401.04124) | MobileAgent: enhancing mobile control via human-machine interaction and SOP inte... | The paper introduces MobileAgent, an LLM-based agent that enhances mobile device control through human-machine interaction and integration of Standard... | This work differs from related work by integrating SOP information into in-context learning for better task comprehension and introducing interactive... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.11941-b31b1b.svg)](https://arxiv.org/abs/2402.11941) | CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation | The paper introduces CoCo-Agent, a multimodal large language model (MLLM) agent for smartphone GUI automation, focusing on comprehensive environment p... | Unlike prior work relying on strong pre-trained models or black-box APIs, CoCo-Agent introduces a trainable architecture with CEP and CAP to systemati... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.09631-b31b1b.svg)](https://arxiv.org/abs/2403.09631) | 3D-VLA: A 3D Vision-Language-Action Generative World Model | The paper introduces 3D-VLA, a generative world model that integrates 3D perception, reasoning, and action for embodied agents. It addresses limitatio... | 3D-VLA differs from prior work by explicitly modeling 3D dynamics and using 3D features for action generation, introducing a novel dataset for 3D embo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.12185-b31b1b.svg)](https://arxiv.org/abs/2402.12185) | ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Ch... | The paper introduces ChartX, a comprehensive multi-modal benchmark for evaluating chart understanding capabilities, and ChartVLM, a new model designed... | This work differs from related work by introducing ChartX as a comprehensive benchmark with diverse chart types and tasks, and ChartVLM with a cascade... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.06596v1-b31b1b.svg)](https://arxiv.org/abs/2402.06596) | Understanding the Weakness of Large Language Model Agents within a Complex Andro... | The paper identifies critical weaknesses of LLM agents in complex Android environments, introduces AndroidArena as a benchmark for evaluating LLM agen... | This work differs from related research by introducing AndroidArena, the first benchmark specifically designed to evaluate LLM agents in complex OS en... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.03428-b31b1b.svg)](https://arxiv.org/abs/2401.03428) | Exploring Large Language Model based Intelligent Agents: Definitions, Methods, a... | The paper provides a comprehensive survey of LLM-based intelligent agents, covering definitions, research frameworks, foundational components (plannin... | This work differs from related research by offering an in-depth synthesis of LLM-based agents' methodologies, multi-agent system integration strategie... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.09712-b31b1b.svg)](https://arxiv.org/abs/2401.09712) | SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning... | Introduces SkyEyeGPT, a unified multi-modal large language model for remote sensing (RS) vision-language tasks. Develops a large-scale RS instruction-... | Unlike prior RS models like RSGPT that handle single tasks with separate models, SkyEyeGPT unifies diverse RS tasks through instruction tuning and a s... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.19322-b31b1b.svg)](https://arxiv.org/abs/2403.19322) | Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models | The paper addresses limitations in Multimodal Large Language Models (MLLMs) by introducing P2G, a framework for plug-and-play grounding of visual reas... | The work differs from related work by leveraging external agents for on-the-fly grounding instead of relying on training data or existing modules, and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.04236-b31b1b.svg)](https://arxiv.org/abs/2402.04236) | CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning | Introduces Chain of Manipulations (CoM) for step-by-step visual reasoning in VLMs, proposes the CogCoM model with 17B parameters, annotates 6K graphic... | Differ from prior work by emphasizing step-by-step visual manipulation reasoning (CoM) instead of direct alignment, introducing a multi-turn multi-ima... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.15057v1-b31b1b.svg)](https://arxiv.org/abs/2402.15057) | On the Multi-turn Instruction Following for Conversational Web Agents | This work introduces the Conversational Web Navigation task, a novel framework (Self-MAP) for handling multi-turn interactions with users and environm... | Unlike prior work focusing on single-turn web navigation tasks, this paper emphasizes multi-turn conversational interactions, introducing a dataset (M... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.11905-b31b1b.svg)](https://arxiv.org/abs/2403.11905) | Tur[k]ingBench: A Challenge Benchmark for Web Agents | Introduces TurkingBench, a benchmark for web-based agents using real HTML pages from crowdsourcing platforms, evaluates multi-modal models on complex... | TurkingBench differs from prior work by using naturally occurring crowdsourcing HTML pages rather than synthesized environments, offering more realist... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.00908-b31b1b.svg)](https://arxiv.org/abs/2401.00908) | DocLLM: A layout-aware generative language model for multimodal document underst... | DocLLM introduces a layout-aware generative language model for multimodal document understanding, focusing on spatial-textual cross-alignment through... | DocLLM differs from related work by avoiding expensive image encoders, focusing exclusively on bounding box-based spatial layouts, and introducing dis... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.08282-b31b1b.svg)](https://arxiv.org/abs/2403.08282) | Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation | The paper introduces a hierarchical auto-organizing system (HAS) for multi-agent navigation in dynamic environments like Minecraft. Key contributions... | Unlike prior work focusing on single-agent reinforcement learning or pre-trained LLMs for task planning, HAS introduces a novel hierarchical architect... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.15300-b31b1b.svg)](https://arxiv.org/abs/2402.15300) | Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models vi... | This paper addresses object hallucination in Large Vision-Language Models (LVLMs) by introducing CLIP-Guided Decoding (CGD), a training-free method th... | Unlike prior work relying on internal model information (e.g., token likelihoods, hidden states) or complex external tools, this work proposes a train... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.04476-b31b1b.svg)](https://arxiv.org/abs/2402.04476) | Dual-View Visual Contextualization for Web Navigation | The paper introduces a method to enhance HTML element representations for web navigation by leveraging dual views (textual and visual context from scr... | Unlike prior work relying solely on HTML documents or simplified representations, this work integrates visual context from webpage screenshots, contex... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.08140-b31b1b.svg)](https://arxiv.org/abs/2403.08140) | BAGEL: Bootstrapping Agents by Guiding Exploration with Language | BAGEL introduces a method to bootstrap language model agents for digital environment interaction without human supervision. It uses iterative round-tr... | BAGEL differs from prior work by eliminating the need for human demonstrations or reward functions, instead leveraging synthetic demonstrations genera... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.12037-b31b1b.svg)](https://arxiv.org/abs/2403.12037) | MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simula... | Introduces MineDreamer, an embodied agent in Minecraft that uses Chain-of-Imagination (CoI) to translate natural language instructions into precise vi... | Proposes CoI mechanism to address sequential instruction-following challenges by breaking tasks into stages and generating state-aware visual prompts,... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.11401-b31b1b.svg)](https://arxiv.org/abs/2403.11401) | Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning | Scene-LLM integrates 3D visual understanding with LLMs for interactive indoor environments, combining scene-level and ego-centric 3D information. It i... | Scene-LLM differs from prior work by jointly leveraging scene-level and ego-centric 3D information, employing a hybrid feature representation for dyna... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.15847-b31b1b.svg)](https://arxiv.org/abs/2401.15847) | Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipane... | The paper introduces MultipanelVQA, a novel benchmark for evaluating Multimodal Large Language Models (MLLMs) on multipanel image understanding. It hi... | This work differs from related studies by focusing specifically on multipanel image understanding, introducing a synthetic benchmark to isolate layout... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.20213-b31b1b.svg)](https://arxiv.org/abs/2403.20213) | VHM: Versatile and Honest Vision Language Model for Remote Sensing Image Analysi... | The paper introduces VHM, a vision language model tailored for remote sensing image analysis, emphasizing versatility and honesty. It proposes two nov... | Unlike prior work focused on factual questions and sparse captions, VHM introduces comprehensive captions (VersaD) and deceptive question datasets (Hn... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.06118-b31b1b.svg)](https://arxiv.org/abs/2402.06118) | ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grai... | The paper addresses visual grounding issues in LVLMs by introducing ViGoR, a framework that leverages fine-grained reward modeling with human evaluati... | ViGoR differs from related work by focusing on reward modeling with fine-grained human feedback to enhance visual grounding, rather than relying solel... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.03578-b31b1b.svg)](https://arxiv.org/abs/2402.03578) | LLM Multi-Agent Systems: Challenges and Open Problems | The paper identifies key challenges in multi-agent systems (MAS) involving LLMs, such as optimizing task allocation, fostering collaborative reasoning... | This work differs from related studies by focusing on systemic challenges in multi-agent collaboration (e.g., context management, memory, and layered... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.12451-b31b1b.svg)](https://arxiv.org/abs/2402.12451) | The Revolution of Multimodal Large Language Models: A Survey | The paper provides a comprehensive survey of Multimodal Large Language Models (MLLMs), focusing on architectural designs, multimodal alignment strateg... | This work differentiates from related studies by offering an exhaustive review of visual-based MLLMs, emphasizing their integration with visual modali... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.08392-b31b1b.svg)](https://arxiv.org/abs/2401.08392) | DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models (Exe... | DoraemonGPT introduces a novel LLM-driven video agent for dynamic scene understanding, addressing spatial-temporal reasoning, large planning spaces, a... | Unlike prior multimodal systems, DoraemonGPT specifically targets dynamic video tasks with a modular architecture combining symbolic memory, sub-task... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.06089-b31b1b.svg)](https://arxiv.org/abs/2402.06089) | AI Assistance for UX: A Literature Review Through Human-Centered AI | The paper provides a systematic literature review of 359 papers on AI-enabled UX tools, highlighting gaps in empathy-building, multi-screen user exper... | This work differs from related studies by focusing on UX practitioners' unmet needs through a Human-Centered AI lens, mapping findings onto the Double... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.12482-b31b1b.svg)](https://arxiv.org/abs/2403.12482) | Embodied LLM Agents Learn to Cooperate in Organized Teams | The paper introduces a framework for organizing LLM agents using prompt-based structures to enhance cooperation in multi-agent systems. It emphasizes... | This work differs from related work by proposing a novel multi-LLM-agent architecture with hierarchical organization structures and a Criticize-Reflec... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.20271-b31b1b.svg)](https://arxiv.org/abs/2403.20271) | Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend Wha... | The paper introduces the Draw-and-Understand framework for integrating visual prompting into MLLMs, supports multiple visual prompt types (points, box... | Unlike prior ROI-based methods that rely on segmentation models or ground truth masks, this work proposes a general architecture adaptable to diverse... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.00290-b31b1b.svg)](https://arxiv.org/abs/2402.00290) | MEIA: Multimodal Embodied Perception and Interaction in Unknown Environments | The paper introduces MEIA, a multimodal embodied agent that integrates visual and linguistic memory for task planning in unknown environments. It prop... | MEIA differs from prior work by combining visual-language memory (MEM) for multimodal scene understanding, enabling real-time environmental grounding... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.17766-b31b1b.svg)](https://arxiv.org/abs/2402.17766) | ShapeLLM: Universal 3D Object Understanding for Embodied Interaction | This paper introduces ShapeLLM, the first 3D Multimodal Large Language Model (LLM) for embodied interaction, combining 3D geometry understanding with... | Unlike prior work focusing on 3D representation learning or vision-language models, ShapeLLM unifies 3D geometry understanding (via ReCon++) with lang... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.08577-b31b1b.svg)](https://arxiv.org/abs/2401.08577) | MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D Worl... | The paper introduces MultiPLY, a multisensory embodied large language model that actively interacts with 3D environments by integrating visual, audio,... | This work differs from related work by enabling active 3D environment interaction through multisensory data integration, introducing object-centric re... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.11075-b31b1b.svg)](https://arxiv.org/abs/2403.11075) | GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alig... | The paper introduces GOMA, a framework for proactive verbal communication in embodied agents to align mental states toward shared goals. It addresses... | GOMA differs from prior work by explicitly modeling mental state alignment through goal-oriented planning, enabling proactive and contextually relevan... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.05468-b31b1b.svg)](https://arxiv.org/abs/2403.05468) | Will GPT-4 Run DOOM? | Demonstrates GPT-4's ability to play Doom through reasoning and planning without reinforcement learning, evaluates LLM capabilities in complex environ... | This work differs from related work by demonstrating LLM-based planning in a video game without reinforcement learning or fine-tuning, using Doom as a... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.13307-b31b1b.svg)](https://arxiv.org/abs/2401.13307) | ChatterBox: Multi-round Multimodal Referring and Grounding | The paper introduces the MRG task for instance-level multimodal dialogues, a new benchmark (CB-300K) with multi-round dialogue and complex spatial cha... | This work differs by addressing multi-round dialogues and complex spatial relationships in MRG, which existing benchmarks lack. It introduces CB-300K... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.06071-b31b1b.svg)](https://arxiv.org/abs/2401.06071) | GroundingGPT:Language Enhanced Multi-modal Grounding Model | GroundingGPT introduces a language-enhanced multi-modal grounding model focused on fine-grained understanding of local information across modalities (... | GroundingGPT differs from prior work by explicitly targeting fine-grained multi-modal grounding through modality-specific adapters, coordinate-based s... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.15842-b31b1b.svg)](https://arxiv.org/abs/2401.15842) | LCV2: An Efficient Pretraining-Free Framework for Grounded Visual Question Answe... | The paper introduces LCV2, a modular, pretraining-free framework for grounded visual question answering (VQA) that leverages a frozen large language m... | LCV2 differs from related work by eliminating the need for pre-training, using a modular architecture with a frozen LLM as a mediator, and enabling ef... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.03346-b31b1b.svg)](https://arxiv.org/abs/2403.03346) | Enhancing Vision-Language Pre-training with Rich Supervisions | The paper introduces S4, a pre-training paradigm for Vision-Language Models (VLMs) using web screenshots and rich supervisions. It leverages HTML elem... | Unlike prior work focusing on image-text pairs or object detection tasks, S4 innovates by utilizing web screenshots with structured HTML annotations a... |
| [![arXiv](https://img.shields.io/badge/arXiv-2403.09333-b31b1b.svg)](https://arxiv.org/abs/2403.09333) | Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Vis... | Griffon v2 enhances multimodal perception by addressing image resolution limitations in LVLMs, enabling high-resolution scaling (up to 1K) and visual-... | Unlike prior methods that rely on division-based resolution enhancement or low-resolution image encoding, Griffon v2 employs a lightweight projector t... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.16354-b31b1b.svg)](https://arxiv.org/abs/2402.16354) | Language-guided Skill Learning with Temporal Variational Inference | The paper introduces a framework for language-guided skill discovery using hierarchical variational inference and an auxiliary objective based on Mini... | The work differs from prior methods by integrating LLM-generated semantic segmentation with variational inference, avoiding language-only skill repres... |
| [![arXiv](https://img.shields.io/badge/arXiv-2402.05889-b31b1b.svg)](https://arxiv.org/abs/2402.05889) | CREMA: Generalizable and Efficient Video-Language Reasoning via Multimodal Modul... | CREMA introduces a modular, parameter-efficient multimodal fusion framework for video-language reasoning, enabling seamless integration of diverse mod... | Unlike prior work, CREMA achieves scalability and efficiency by decoupling modality processing from the LLM, using parameter-efficient modules and a n... |
| [![arXiv](https://img.shields.io/badge/arXiv-2401.02814-b31b1b.svg)](https://arxiv.org/abs/2401.02814) | Object-Centric Instruction Augmentation for Robotic Manipulation | The paper introduces the Object-Centric Instruction Augmentation (OCI) framework to enhance robotic manipulation by integrating object positions into... | Unlike prior work focusing on task planning or vision-language model integration, this work emphasizes augmenting language instructions with explicit... |


### 2024: Q2

| arXiv | Title | Summary | Contributions |
|-------|-------|---------|---------------|
| [![arXiv](https://img.shields.io/badge/arXiv-2404.16821-b31b1b.svg)](https://arxiv.org/abs/2404.16821) | How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with O... | The paper introduces three key improvements: (1) a strong vision encoder with continuous learning for better visual understanding, (2) dynamic high-re... | InternVL 1.5 addresses gaps in parameter scale, image resolution, and multilingual capability compared to proprietary models by integrating a continuo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.07972v2-b31b1b.svg)](https://arxiv.org/abs/2404.07972) | OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer En... | The paper introduces OSWorld, a real computer environment for evaluating multimodal agents in open-ended tasks across multiple operating systems. It p... | OSWorld differs from prior work by providing a scalable, real-world interactive environment and benchmark that captures the diversity and complexity o... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.14573v3-b31b1b.svg)](https://arxiv.org/abs/2405.14573) | AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents | The paper introduces AndroidWorld, a dynamic benchmarking environment for autonomous agents on Android, featuring 116 programmatic tasks across 20 rea... | AndroidWorld differs from related work by providing the first comprehensive mobile benchmark with dynamically generated, parameterized tasks across di... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.16860-b31b1b.svg)](https://arxiv.org/abs/2406.16860) | Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs | The paper introduces Cambrian-1, a vision-centric multimodal LLM family that evaluates diverse visual representations through visual instruction tunin... | Unlike prior work, Cambrian-1 focuses on vision-centric design and integrates spatial awareness via SVA to enhance visual grounding. It introduces CV-... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.05719v1-b31b1b.svg)](https://arxiv.org/abs/2404.05719) | Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs | Ferret-UI introduces a specialized multimodal large language model (MLLM) for mobile UI understanding, addressing limitations in existing models throu... | This work differs from related work by explicitly addressing UI-specific challenges (e.g., elongated aspect ratios, small objects) through a dual-subi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.06512-b31b1b.svg)](https://arxiv.org/abs/2404.06512) | InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Reso... | The paper introduces InternLM-XComposer2-4KHD, a large vision-language model capable of handling resolutions from 336 pixels to 4K HD. It addresses re... | Unlike prior work that uses fixed resolutions or separate HR/LR encoders, this work proposes dynamic resolution scaling with automatic patch configura... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.02246-b31b1b.svg)](https://arxiv.org/abs/2405.02246) | What matters when building vision-language models? | The paper systematically evaluates critical design choices in vision-language models (VLMs), including architecture, data, and training methods. It id... | This work differs from related research by rigorously ablation-studying key design decisions in VLMs through controlled experiments, providing empiric... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.11896v1-b31b1b.svg)](https://arxiv.org/abs/2406.11896) | DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement... | The paper introduces DigiRL, an autonomous reinforcement learning framework for training device-control agents in real-world GUI environments. Key con... | DigiRL differs from prior work by combining autonomous offline-to-online RL with pre-trained VLMs, addressing real-world stochasticity and non-station... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.01014-b31b1b.svg)](https://arxiv.org/abs/2406.01014) | Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via... | The paper introduces a multi-agent architecture (planning, decision, reflection agents) for mobile device operation tasks, addressing navigation chall... | This work differs from related work by proposing a specialized multi-agent collaboration framework tailored for mobile device GUI operations, incorpor... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.03648-b31b1b.svg)](https://arxiv.org/abs/2404.03648) | AutoWebGLM: A Large Language Model-based Web Navigating Agent | The paper introduces AutoWebGLM, a large language model-based web navigating agent designed to overcome challenges in real-world web navigation. Key c... | This work differs from related work by addressing HTML complexity and open-domain task challenges through HTML simplification, curriculum learning, an... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.03679-b31b1b.svg)](https://arxiv.org/abs/2406.03679) | On the Effects of Data Scale on UI Control Agents | The paper investigates the scalability of fine-tuning LLMs for UI control agents, introduces the AndroidControl dataset with high/low-level instructio... | The work introduces AndroidControl, the most diverse UI control dataset with both high/low-level instructions, and provides systematic analysis of how... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.08451v1-b31b1b.svg)](https://arxiv.org/abs/2406.08451) | GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devi... | This work introduces GUI Odyssey, a comprehensive dataset for cross-app GUI navigation, and develops OdysseyAgent, a multimodal navigation agent. It a... | This work differs from related work by focusing on cross-app navigation tasks, which previous datasets and models primarily ignored. It introduces GUI... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.11317v1-b31b1b.svg)](https://arxiv.org/abs/2406.11317) | GUICourse: From General Vision Language Models to Versatile GUI Agents | The paper introduces GUICourse, a comprehensive suite of datasets (GUIEnv, GUIAct, GUIChat) to enhance Vision Language Models (VLMs) for GUI agent tas... | This work differs from related work by explicitly addressing VLM limitations in OCR, grounding, and GUI-specific knowledge through purpose-built datas... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.01483-b31b1b.svg)](https://arxiv.org/abs/2405.01483) | MANTIS: Interleaved Multi-Image Instruction Tuning | The paper introduces Mantis, a family of large multimodal models trained via instruction tuning on a curated dataset (Mantis-Instruct) to excel in mul... | Unlike prior work that relies on pre-training on large noisy interleaved data, Mantis achieves strong multi-image capabilities through instruction tun... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.06474-b31b1b.svg)](https://arxiv.org/abs/2404.06474) | Autonomous Evaluation and Refinement of Digital Agents | The paper introduces domain-general automatic evaluators for digital agents, demonstrating their effectiveness in improving web navigation and device... | This work differs from related work by proposing automated evaluators that operate without expert demonstrations or supervision, leveraging vision-lan... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.12358-b31b1b.svg)](https://arxiv.org/abs/2404.12358) | From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function | The paper establishes DPO as a token-level Q-learning algorithm within the RLHF framework, bridging the gap between contextual bandit-based DPO and cl... | The work theoretically frames DPO as an inverse Q-learning algorithm in token-level MDPs, resolving discrepancies with classical RLHF by satisfying Be... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.09246-b31b1b.svg)](https://arxiv.org/abs/2406.09246) | OpenVLA: An Open-Source Vision-Language-Action Model | This paper introduces OpenVLA, an open-source vision-language-action (VLA) model trained on 970k real-world robot demonstrations. It combines Llama 2... | OpenVLA differs from prior work by integrating Internet-scale vision-language foundation models with robot demonstrations, enabling generalization to... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.05955v1-b31b1b.svg)](https://arxiv.org/abs/2404.05955) | VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding a... | The paper introduces VisualWebBench, a comprehensive benchmark for evaluating multimodal large language models (MLLMs) in web page understanding and g... | VisualWebBench differs from related work by providing a holistic evaluation framework tailored to web contexts, encompassing perception, comprehension... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.13501-b31b1b.svg)](https://arxiv.org/abs/2404.13501) | A Survey on the Memory Mechanism of Large Language Model based Agents | The paper provides a comprehensive survey on memory mechanisms in LLM-based agents, systematically reviewing existing designs, evaluating their effect... | This work differs from related studies by offering the first holistic review of memory mechanisms in LLM-based agents, abstracting design patterns, an... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.01744v5-b31b1b.svg)](https://arxiv.org/abs/2404.01744) | Octopus v2: On-device language model for super agent | The paper introduces Octopus v2, an on-device language model with 2B parameters that significantly improves accuracy and reduces latency compared to G... | The work differs by introducing functional tokens for efficient function calling, optimizing latency and accuracy for on-device use, and achieving cos... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.12373v3-b31b1b.svg)](https://arxiv.org/abs/2406.12373) | WebCanvas: Benchmarking Web Agents in Online Environments | The paper introduces WebCanvas, an online evaluation framework for web agents that addresses the dynamic nature of web environments. Key contributions... | Unlike static benchmarks, WebCanvas focuses on dynamic web environments with real-time task evaluation, introduces a live dataset with intermediate st... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.07973-b31b1b.svg)](https://arxiv.org/abs/2404.07973) | Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language... | Ferret-v2 enhances referring and grounding capabilities in MLLMs through high-resolution image processing, multi-granularity visual encoding with DINO... | Unlike prior MLLMs reliant on low-resolution encoders (e.g., CLIP), Ferret-v2 introduces DINOv2 for multi-granularity visual encoding and a three-stag... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.16054v2-b31b1b.svg)](https://arxiv.org/abs/2404.16054) | LlamaTouch: A Faithful and Scalable Testbed for Mobile UI Task Automation | LlamaTouch introduces a novel testbed for evaluating mobile UI task automation agents by focusing on faithful and scalable task execution evaluation.... | Unlike prior work relying on static datasets or human validation, LlamaTouch introduces a dynamic benchmark that evaluates agents based on traversing... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.10819v1-b31b1b.svg)](https://arxiv.org/abs/2406.10819) | GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents | Introduces GUI-World, a comprehensive dataset for GUI-oriented multimodal LLM agents, emphasizing dynamic and sequential tasks across diverse GUI scen... | GUI-World addresses gaps in prior work by covering dynamic GUI content, multi-window interactions, and diverse operating environments. It provides a b... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.20309-b31b1b.svg)](https://arxiv.org/abs/2405.20309) | Large Language Models Can Self-Improve At Web Agent Tasks | The paper explores LLM self-improvement for web agent tasks using synthetic training data, achieves a 31% task completion rate improvement on WebArena... | This work differs by focusing on self-improvement through fine-tuning on model-generated data for long-horizon web agent tasks, and by introducing com... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.08184v1-b31b1b.svg)](https://arxiv.org/abs/2406.08184) | MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents | The paper introduces MobileAgentBench, a benchmark for evaluating mobile LLM agents on Android devices. It addresses the lack of standardized testing... | This work differs from related work by focusing on benchmarking rather than model development. While prior studies explored LLM/MLLM-based agents and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.09992v1-b31b1b.svg)](https://arxiv.org/abs/2404.09992) | MMInA: Benchmarking Multihop Multimodal Internet Agents | The paper introduces MMInA, a benchmark for evaluating embodied agents in multihop, multimodal web tasks. Key contributions include evolving real-worl... | MMInA addresses gaps in existing benchmarks by focusing on real-world evolving websites, multihop tasks, and multimodal reasoning. It introduces a hol... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.09411-b31b1b.svg)](https://arxiv.org/abs/2406.09411) | MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding | Introduces MuirBench, a comprehensive benchmark for evaluating multi-image understanding in multimodal LLMs, highlighting challenges in spatial-tempor... | MuirBench differs from prior work by providing a broader, more comprehensive assessment of multi-image reasoning capabilities, including 12 diverse ta... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.16660v2-b31b1b.svg)](https://arxiv.org/abs/2404.16660) | Benchmarking Mobile Device Control Agents across Diverse Configurations | The paper introduces B-MoCA, a novel benchmark for evaluating mobile device control agents across diverse device configurations. It emphasizes tasks m... | Unlike prior work focused on specific agent types or limited task sets, B-MoCA provides a unified benchmark with diverse device configurations and 131... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.19263v2-b31b1b.svg)](https://arxiv.org/abs/2406.19263) | Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Groundi... | This work introduces the Screen Point-and-Read (ScreenPR) task, a novel GUI-referring task involving screen reading based on user-indicated points. It... | This work differs from related work by focusing on the ScreenPR task, which is underexplored in GUI understanding. It introduces the ToL grounding mec... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.13046-b31b1b.svg)](https://arxiv.org/abs/2404.13046) | MoVA: Adapting Mixture of Vision Experts to Multimodal Context | The paper addresses the limitations of single vision encoders in MLLMs by introducing MoVA, a novel framework that adaptively routes and fuses task-sp... | MoVA differs from prior work by introducing a context-aware expert routing strategy guided by LLMs and a fine-grained MoV-Adapter for task-specific kn... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.01549v1-b31b1b.svg)](https://arxiv.org/abs/2404.01549) | Octopus: On-device language model for function calling of software APIs | The paper introduces Octopus, an on-device LLM fine-tuned for software API function calling, with techniques like conditional masking and a novel benc... | This work differs by focusing on on-device LLMs for API interactions, introducing conditional masking for format alignment, and creating a specialized... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.13013-b31b1b.svg)](https://arxiv.org/abs/2404.13013) | Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Mod... | Groma introduces localized visual tokenization for grounded multimodal understanding, enabling region-level tasks like captioning and grounding. It in... | Groma differs by embedding localization into image tokenization rather than relying on external modules or LLM-based coordinate outputs, enabling unif... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.20095-b31b1b.svg)](https://arxiv.org/abs/2406.20095) | LLaRA: Supercharging Robot Learning Data for Vision-Language Policy | LLaRA introduces a framework for adapting pretrained Vision-Language Models (VLMs) into Vision-Language-Action (VLA) models for robotic control. It ge... | LLaRA differs from related work by focusing on visuomotor instruction tuning for robotics, generating task-specific conversation-style datasets from e... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.10739-b31b1b.svg)](https://arxiv.org/abs/2405.10739) | Efficient Multimodal Large Language Models: A Survey | The paper provides a comprehensive survey of efficient Multimodal Large Language Models (MLLMs), summarizing their development timelines, efficient st... | This work differs from related work by offering a systematic review of efficient MLLMs, highlighting scalability challenges, and proposing future dire... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.04619-b31b1b.svg)](https://arxiv.org/abs/2404.04619) | Do We Really Need a Complex Agent System? Distill Embodied Agent into a Single M... | The paper introduces STEVE-2, a hierarchical knowledge distillation framework for open-ended embodied agents, addressing limitations in multi-LLM syst... | STEVE-2 differs from prior work by distilling complex multi-agent systems into a single model with hierarchical architecture, enabling dynamic adaptat... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.17820-b31b1b.svg)](https://arxiv.org/abs/2405.17820) | Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vi... | The paper identifies 'blind tokens' in LVLMs that cause hallucinations due to excessive attention on irrelevant image regions. It introduces AvisC, a... | Unlike prior output-level methods that contrast whole-image representations or use external models, AvisC focuses on internal attention patterns to id... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.10227v1-b31b1b.svg)](https://arxiv.org/abs/2406.10227) | VideoGUI: A Benchmark for GUI Automation from Instructional Videos | Introduces VideoGUI, a multi-modal benchmark for advanced GUI automation tasks derived from instructional videos, emphasizing visual-centric tasks, hi... | Differently from prior benchmarks limited to simple text-instruction tasks, VideoGUI targets complex visual-centric GUI tasks via instructional videos... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.05902-b31b1b.svg)](https://arxiv.org/abs/2404.05902) | WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents | Wilbur introduces adaptive in-context learning for web agents, combining a differentiable ranking model, instruction synthesis, and an intelligent bac... | Wilbur differs from prior work by introducing backtracking to recover from mistakes, synthesizing both positive and negative task demonstrations, and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.09204-b31b1b.svg)](https://arxiv.org/abs/2404.09204) | TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Langua... | The paper presents TextHawk, a Multimodal Large Language Model (MLLM) specifically designed for document-oriented tasks, featuring four novel componen... | TextHawk differs from prior work by introducing a novel architecture with ReSA, SPEs, QPN, and MLCA for efficient fine-grained document perception, al... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.10887v1-b31b1b.svg)](https://arxiv.org/abs/2404.10887) | Search Beyond Queries: Training Smaller Language Models for Web Interactions via... | The paper introduces GLAINTEL, a novel agent for intelligent web navigation that leverages reinforcement learning and smaller language models to addre... | GLAINTEL differs from related work by focusing on smaller language models trained via reinforcement learning and unsupervised methods, demonstrating s... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.08755-b31b1b.svg)](https://arxiv.org/abs/2404.08755) | Training a Vision Language Model as Smartphone Assistant | This research introduces a Vision Language Model (VLM) designed to control mobile devices through UI interactions, leveraging sequences of past screen... | The work differs from prior methods by directly interacting with the UI through visual inputs and action sequences, rather than relying on API calls o... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.20098-b31b1b.svg)](https://arxiv.org/abs/2406.20098) | Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Mul... | The paper introduces Web2Code, a large-scale dataset and evaluation framework for webpage-to-code generation tasks, addressing the gap in MLLMs' abili... | Unlike existing datasets that lack instruction information or focus on general visual tasks, Web2Code integrates instruction tuning, diverse QA pairs,... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.09187-b31b1b.svg)](https://arxiv.org/abs/2406.09187) | GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasonin... | GuardAgent introduces a novel guardrail framework for LLM agents, enabling dynamic safety checks through knowledge-enabled reasoning and code executio... | Unlike prior guardrails focused on input/output moderation, GuardAgent introduces a task-planning and code-execution framework for safeguarding LLM ag... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.16698-b31b1b.svg)](https://arxiv.org/abs/2404.16698) | Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM... | The paper introduces GovSim, a novel simulation platform to study cooperative decision-making in LLM agents, highlighting challenges in achieving sust... | This work differs by focusing on multi-agent cooperation in dynamic, realistic scenarios (GovSim) rather than single-agent benchmarks. It introduces a... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.01623-b31b1b.svg)](https://arxiv.org/abs/2406.01623) | WebSuite: Systematically Evaluating Why Web Agents Fail | The paper introduces WebSuite, a diagnostic benchmark for web agents that systematically evaluates failure patterns by categorizing web actions into a... | Unlike existing benchmarks that only measure task success or failure, WebSuite introduces a taxonomy of web actions and an extensible benchmark suite... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.11120-b31b1b.svg)](https://arxiv.org/abs/2405.11120) | Latent State Estimation Helps UI Agents to Reason | The paper explores how LLMs can estimate latent states in UI agents through zero-shot prompting, demonstrating significant improvements in task comple... | This work differs from related work by formalizing the use of pre-trained LLMs to estimate latent states in a textual space without task-specific fine... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.12718-b31b1b.svg)](https://arxiv.org/abs/2406.12718) | Mitigating Object Hallucinations in Large Vision-Language Models with Assembly o... | The paper addresses object hallucinations in Large Vision-Language Models (LVLMs) by identifying attention deficiency as a root cause. It introduces A... | This work differs from related work by focusing on attention mechanism design rather than instruction tuning or post-hoc revisers. It proposes a dual-... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.16375-b31b1b.svg)](https://arxiv.org/abs/2404.16375) | List Items One by One: A New Data Source and Learning Paradigm for Multimodal LL... | The paper introduces a new learning paradigm called 'list items one by one' to enhance Multimodal Large Language Models' (MLLMs) ability to understand... | This work differs from related studies by focusing on a novel training paradigm ('list items one by one') and a curated dataset to enable SoM promptin... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.19296v1-b31b1b.svg)](https://arxiv.org/abs/2404.19296) | Octopus v4: Graph of language models | The paper introduces Octopus v4, a system that integrates multiple open-source language models using functional tokens and a graph structure to optimi... | This work differs from related work by introducing a graph-based coordination framework with functional tokens to dynamically route and reformat queri... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.11459v2-b31b1b.svg)](https://arxiv.org/abs/2404.11459) | Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent | The paper introduces a compact, on-device multimodal AI agent (Octopus v3) optimized for edge devices with <1B parameters, featuring functional tokens... | Octopus v3 differs by introducing functional tokens for AI agent-specific reasoning, optimizing for sub-1B parameters for edge deployment, and demonst... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.18082v1-b31b1b.svg)](https://arxiv.org/abs/2406.18082) | Octo-planner: On-device Language Model for Planner-Action Agents | The paper introduces an on-device Planner-Action framework for AI agents, separating planning and action execution. It utilizes Phi-3 Mini (a 3.8B par... | The work differs from related work by focusing on on-device execution with fine-tuned LLMs (Phi-3 Mini) and multi-LoRA for multi-domain adaptability,... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.15341-b31b1b.svg)](https://arxiv.org/abs/2405.15341) | V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal... | Introduces V-Zen, a novel Multimodal Large Language Model (MLLM) specialized for GUI understanding and grounding, along with the GUIDE dataset. The wo... | V-Zen improves GUI grounding precision with dual-resolution image encoders and a dedicated grounding module, addressing limitations in existing MLLMs.... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.13719v1-b31b1b.svg)](https://arxiv.org/abs/2406.13719) | GUI Action Narrator: Where and When Did That Action Take Place? | The paper introduces a GUI video captioning benchmark (Act2Cap) with 4,189 samples, addressing GUI-specific challenges like dense information and rapi... | This work differs from related work by introducing a specialized GUI video captioning benchmark and framework that explicitly addresses GUI-specific c... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.11106-b31b1b.svg)](https://arxiv.org/abs/2405.11106) | LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions | The paper surveys existing LLM-based single-agent and multi-agent reinforcement learning frameworks, emphasizing communication strategies, coordinatio... | This work differs from related research by focusing on the integration of LLMs into MARL frameworks, emphasizing language-conditioned communication, c... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.16048-b31b1b.svg)](https://arxiv.org/abs/2404.16048) | GUIDE: Graphical User Interface Data for Execution | The paper introduces GUIDE, a novel dataset for MLLMs in RPA, emphasizing multi-platform adaptability and diverse website coverage. It presents V-Zen,... | GUIDE differs from related work by providing annotated data with action grounding, multi-platform adaptability, and diverse website coverage, while V-... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.14596-b31b1b.svg)](https://arxiv.org/abs/2406.14596) | VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Prog... | The paper introduces ICAL, a method that refines suboptimal trajectories into high-quality multimodal programs of thought using VLM self-refinement an... | Unlike prior text-based methods that lack visual cues or introspection, ICAL leverages VLMs to generate causal reasoning and subgoals from noisy demon... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.16386-b31b1b.svg)](https://arxiv.org/abs/2406.16386) | Automatically Generating UI Code from Screenshot: A Divide-and-Conquer-Based App... | The paper introduces DCGen, a divide-and-conquer approach for generating UI code from screenshots, addressing challenges like element omission, distor... | DCGen differs from related work by introducing a segment-aware divide-and-conquer framework specifically tailored for MLLMs, improving upon CNN-based... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.10179-b31b1b.svg)](https://arxiv.org/abs/2404.10179) | Scaling Instructable Agents Across Many Simulated Worlds | The paper introduces the SIMA project, which develops agents capable of following arbitrary language instructions across diverse 3D environments. Key... | Unlike prior work focused on specific games or environments, SIMA emphasizes training agents across a wide range of 3D worlds (including commercial ga... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.11633-b31b1b.svg)](https://arxiv.org/abs/2406.11633) | DocGenome: An Open Large-scale Scientific Document Benchmark for Training and Te... | DocGenome introduces a large-scale, multi-modal scientific document benchmark addressing limitations in multi-page document extraction, understanding... | DocGenome differs from related work by offering the first comprehensive dataset with structured multi-modal data (including LaTeX, layout attributes,... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.09981-b31b1b.svg)](https://arxiv.org/abs/2405.09981) | Adversarial Robustness for Visual Grounding of Multimodal Large Language Models | This paper addresses the adversarial robustness of visual grounding in Multi-modal Large Language Models (MLLMs) by proposing three novel adversarial... | This work differs from related studies by focusing specifically on adversarial robustness in visual grounding tasks, whereas prior research primarily... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.13264v2-b31b1b.svg)](https://arxiv.org/abs/2406.13264) | WONDERBREAD: A Benchmark for Evaluating Multimodal Foundation Models on Business... | The paper introduces WONDERBREAD, a benchmark for evaluating multimodal foundation models (FMs) on business process management (BPM) tasks beyond auto... | Unlike existing benchmarks focused on workflow automation, WONDERBREAD addresses underexplored BPM tasks like documentation and knowledge transfer. It... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.08948-b31b1b.svg)](https://arxiv.org/abs/2404.08948) | Large Language Models for Mobile GUI Text Input Generation: An Empirical Study | The paper evaluates the effectiveness of nine state-of-the-art LLMs in generating text inputs for Android GUI testing, demonstrating varying page-pass... | This work differs from related studies by conducting the first large-scale empirical evaluation of LLMs for Android text-input generation in GUI testi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.06947-b31b1b.svg)](https://arxiv.org/abs/2406.06947) | CAAP: Context-Aware Action Planning Prompting to Solve Computer Tasks with Front... | This paper introduces a novel LLM-based GUI agent that operates solely on screenshot inputs, eliminating reliance on HTML/DOM data. It proposes Contex... | Unlike prior work relying on DOM/HTML inputs or integrated architectures, this work uses modular design with LLM-driven action planning via CAAP promp... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.02475-b31b1b.svg)](https://arxiv.org/abs/2404.02475) | Prompt2Task: Automating UI Tasks on Smartphones from Textual Prompts | Prompt2Task introduces a multi-agent system for UI task automation on smartphones, translating textual prompts into operation sequences without requir... | Unlike prior UI automation systems that rely on predefined flows or detailed step-by-step instructions, Prompt2Task employs a text-prompt-driven appro... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.09911-b31b1b.svg)](https://arxiv.org/abs/2404.09911) | ChatShop: Interactive Information Seeking with Language Agents | The paper introduces ChatShop, a novel task framework for evaluating language agents' strategic information-seeking capabilities in interactive scenar... | Unlike prior work focused on static information retrieval or limited reference games, ChatShop introduces a realistic, interactive benchmark that eval... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.14314-b31b1b.svg)](https://arxiv.org/abs/2405.14314) | Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration | The paper introduces ReAd, a framework for efficient LLM grounding in multi-agent collaboration through Reinforced Advantage feedback. It addresses in... | Unlike prior work relying on physical verification or self-reflection, ReAd introduces a principled advantage-based feedback mechanism grounded in mul... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.17418-b31b1b.svg)](https://arxiv.org/abs/2405.17418) | A Self-Correcting Vision-Language-Action Model for Fast and Slow System Manipula... | The paper introduces SC-VLA, a self-correcting vision-language-action model combining fast and slow systems for robust robotic manipulation. It addres... | SC-VLA differs from prior work by explicitly addressing two key limitations: direct correction of low-level SE(3) poses and learning from correction f... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.18243-b31b1b.svg)](https://arxiv.org/abs/2404.18243) | LEGENT: Open Platform for Embodied Agents | Introduces LEGENT, an open platform for embodied agents integrating LLMs and LMMs, offering a 3D environment, data generation pipeline, and demonstrat... | LEGENT differs from related work by providing an open, scalable platform that combines LMMs with embodied training, addressing limitations of existing... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.09295-b31b1b.svg)](https://arxiv.org/abs/2406.09295) | AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language M... | The paper introduces AlignMMBench, a comprehensive Chinese multimodal alignment benchmark addressing gaps in existing benchmarks by focusing on nuance... | AlignMMBench differs from related work by being the first benchmark specifically designed for Chinese visual contexts, incorporating thirteen tasks ac... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.12107-b31b1b.svg)](https://arxiv.org/abs/2405.12107) | Imp: Highly Capable Large Multimodal Models for Mobile Devices | Systematic exploration of lightweight LMMs through model architecture, training strategy, and training data; development of Imp models (2B-4B paramete... | Combines architectural innovations, optimized training strategies, and curated data to achieve high performance in lightweight LMMs, enabling efficien... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.18860-b31b1b.svg)](https://arxiv.org/abs/2405.18860) | Empowering Embodied Manipulation: A Bimanual-Mobile Robot Manipulation Dataset f... | The paper introduces BRMData, a comprehensive bimanual-mobile robot manipulation dataset for household tasks, addressing limitations in existing datas... | BRMData differs from related work by providing the first dataset combining bimanual manipulation with mobile mobility, diverse task difficulty levels,... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.14056-b31b1b.svg)](https://arxiv.org/abs/2406.14056) | VGA: Vision GUI Assistant -- Minimizing Hallucinations through Image-Centric Fin... | The paper introduces VGA, a fine-tuned model for GUI comprehension that addresses hallucinations through image-centric fine-tuning. Key contributions... | Unlike prior work that relies on traditional GUI comprehension methods or LVLMs with limited visual alignment, VGA introduces image-centric fine-tunin... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.14250v3-b31b1b.svg)](https://arxiv.org/abs/2406.14250) | E-ANT: A Large-Scale Dataset for Efficient Automatic GUI NavigaTion | The paper introduces E-ANT, the first Chinese GUI navigation dataset with real human behavior and high-quality annotations, aiming to improve MLLM dec... | E-ANT differs from prior work by providing a large-scale, Chinese-focused dataset with human-traced interactions and high-quality screenshots, enablin... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.13631-b31b1b.svg)](https://arxiv.org/abs/2406.13631) | On AI-Inspired UI-Design | The paper explores three AI approaches to enhance UI design creativity: using LLMs for direct UI generation, VLMs for searching app screenshot dataset... | This work differs from related work by proposing an integrated AI-inspired design process that combines LLMs, VLMs, and DMs to address UI design chall... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.02208-b31b1b.svg)](https://arxiv.org/abs/2406.02208) | Why Only Text: Empowering Vision-and-Language Navigation with Multi-modal Prompt... | Introduces Vision-and-Language Navigation with Multi-modal Prompts (VLN-MP) to enhance navigation by integrating visual and textual instructions, ensu... | This work differs from related work by introducing multi-modal instructions (text + images) in VLN, creating a new benchmark with diverse datasets and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2404.03570-b31b1b.svg)](https://arxiv.org/abs/2404.03570) | Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity | The paper introduces a modular AI system for bi-arm robots that integrates Large Language Models (LLMs) for task planning, Vision-Language Models (VLM... | The work differs from related work by combining existing models into a modular architecture with integrated safety constraints and zero-shot capabilit... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.09385-b31b1b.svg)](https://arxiv.org/abs/2406.09385) | Towards Vision-Language Geo-Foundation Model: A Survey | The paper provides a comprehensive survey of Vision-Language Geo-Foundation Models (VLGFMs), highlighting their development in handling geospatial dat... | This work is the first comprehensive literature review of VLGFMs, synthesizing existing research on geospatial vision-language models, while identifyi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.08100-b31b1b.svg)](https://arxiv.org/abs/2406.08100) | Multimodal Table Understanding | The paper introduces multimodal table understanding, a novel problem addressing direct table comprehension via visual information rather than text con... | This work differs by directly processing table images instead of relying on text-based representations, introducing MMTab for comprehensive evaluation... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.13381-b31b1b.svg)](https://arxiv.org/abs/2406.13381) | CoAct: A Global-Local Hierarchy for Autonomous Agent Collaboration | The paper introduces the CoAct framework, which applies hierarchical planning and collaboration mechanisms inspired by human society to LLMs. It addre... | CoAct differs from related work by introducing a global-local hierarchical agent architecture that enables collaborative task execution and failure re... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.06822-b31b1b.svg)](https://arxiv.org/abs/2406.06822) | An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injec... | The paper introduces CodeBreaker, an LLM-assisted backdoor attack framework targeting code completion models. It innovates by using LLMs (e.g., GPT-4)... | CodeBreaker differs from prior work by leveraging LLMs to disguise malicious payloads within essential code sections, ensuring evasion of detection wh... |
| [![arXiv](https://img.shields.io/badge/arXiv-2406.11247-b31b1b.svg)](https://arxiv.org/abs/2406.11247) | STEVE Series: Step-by-Step Construction of Agent Systems in Minecraft | The STEVE Series introduces a hierarchical multi-agent system in Minecraft integrating large language models (LLMs) with vision encoders, action codeb... | The work differs by combining LLMs with vision/action modules, introducing a hierarchical multi-agent architecture, and leveraging a custom dataset (S... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.14868-b31b1b.svg)](https://arxiv.org/abs/2405.14868) | Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis | The paper introduces GCD, a monocular dynamic novel view synthesis framework that generates videos from arbitrary camera perspectives without requirin... | Unlike prior methods reliant on multi-view inputs or limited camera motion, GCD achieves dynamic view synthesis from a single video using diffusion mo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2405.00516v1-b31b1b.svg)](https://arxiv.org/abs/2405.00516) | Navigating WebAI: Training Agents to Complete Web Tasks with Large Language Mode... | The paper combines supervised learning (SL) and reinforcement learning (RL) to improve web navigation tasks, addresses HTML structure understanding li... | This work differs from prior methods by integrating SL and RL techniques to mitigate overfitting and improve HTML comprehension, while emphasizing the... |


### 2024: Q3

| arXiv | Title | Summary | Contributions |
|-------|-------|---------|---------------|
| [![arXiv](https://img.shields.io/badge/arXiv-2407.01476-b31b1b.svg)](https://arxiv.org/abs/2407.01476) | Tree Search for Language Model Agents | The paper introduces a tree search algorithm for language model (LM) agents to enhance multi-step planning and exploration in interactive web environm... | This work differs from related work by introducing the first tree search algorithm specifically tailored for LM agents in realistic web tasks, achievi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.07199v1-b31b1b.svg)](https://arxiv.org/abs/2408.07199) | Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents | The paper introduces a framework combining guided Monte Carlo Tree Search (MCTS) with self-critique mechanisms and an off-policy DPO algorithm for aut... | The work differs by integrating MCTS search with self-critique and off-policy DPO for trajectory learning, enabling agents to generalize in multi-step... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.00203v1-b31b1b.svg)](https://arxiv.org/abs/2408.00203) | OmniParser for Pure Vision Based GUI Agent | This paper introduces OmniParser, a pure vision-based approach for parsing UI screenshots into structured elements to enhance GUI agents. It addresses... | OmniParser differs from prior work by providing a pure vision-based solution without requiring HTML or view hierarchy dependencies, curating domain-sp... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.16741-b31b1b.svg)](https://arxiv.org/abs/2407.16741) | OpenHands: An Open Platform for AI Software Developers as Generalist Agents | The paper introduces OpenHands, a platform enabling AI agents to perform tasks akin to human developers by integrating code writing, CLI interaction,... | OpenHands differs from related work by providing a unified platform for developing and evaluating AI agents across diverse tasks (software engineering... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.08264v2-b31b1b.svg)](https://arxiv.org/abs/2409.08264) | Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale | Introduces Windows Agent Arena, a scalable benchmark for evaluating multi-modal agents in real Windows OS environments, featuring 154 diverse tasks re... | Focuses on Windows-specific OS interactions and real-world task complexity, expanding beyond prior benchmarks (e.g., OSWorld's Linux focus) with scala... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.17490v1-b31b1b.svg)](https://arxiv.org/abs/2407.17490) | AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents | The paper introduces AMEX, a large-scale multi-annotation dataset for mobile GUI agents with three levels of annotations (GUI element grounding, funct... | AMEX differs from prior datasets by providing multi-level annotations (element grounding, functionality, and instructions) with stepwise action chains... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.07429-b31b1b.svg)](https://arxiv.org/abs/2409.07429) | Agent Workflow Memory | The paper introduces Agent Workflow Memory (AWM), a method enabling agents to learn and reuse task workflows from past experiences to improve performa... | AWM differs from related work by explicitly inducing reusable task workflows from agent trajectories, enabling flexible adaptation to new tasks withou... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.06327v1-b31b1b.svg)](https://arxiv.org/abs/2408.06327) | VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents | Introduces VisualAgentBench (VAB), a comprehensive benchmark for evaluating Large Multimodal Models (LMMs) as visual foundation agents across Embodied... | Differently from prior work focused on task-specific training or static datasets, VAB offers a holistic benchmark and training framework for LMMs as v... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.11824-b31b1b.svg)](https://arxiv.org/abs/2408.11824) | AppAgent v2: Advanced Agent for Flexible Mobile Interactions | The paper introduces AppAgent v2, a novel LLM-based multimodal framework for mobile devices that enhances GUI interaction through a structured knowled... | Unlike prior work, AppAgent v2 introduces a structured knowledge base with RAG-enabled retrieval, a two-phase exploration-deployment framework, and co... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.15978-b31b1b.svg)](https://arxiv.org/abs/2408.15978) | WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution w... | The paper introduces WebPilot, a multi-agent system that combines global and local optimization strategies to enhance adaptability in complex web envi... | WebPilot differs from related work by employing a dual optimization strategy (global task decomposition and local MCTS refinement) to overcome rigid p... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.20566-b31b1b.svg)](https://arxiv.org/abs/2409.20566) | MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning | The paper introduces MM1.5, a family of multimodal LLMs with enhanced capabilities in text-rich image understanding, visual grounding, and multi-image... | Unlike prior work, MM1.5 focuses on systematic data mixture exploration for training, including OCR data, synthetic captions, and optimized visual ins... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.12637-b31b1b.svg)](https://arxiv.org/abs/2408.12637) | Building and better understanding vision-language models: insights and future di... | The paper provides a comprehensive tutorial on building vision-language models (VLMs), emphasizing architecture, data, and training methods. It introd... | This work differs from related research by offering a detailed tutorial on VLM development, introducing the Docmatix dataset for document understandin... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.04346v2-b31b1b.svg)](https://arxiv.org/abs/2407.04346) | MobileFlow: A Multimodal LLM For Mobile GUI Agent | MobileFlow introduces a multimodal LLM tailored for mobile GUI agents, addressing privacy risks by avoiding system API calls, supporting variable-reso... | MobileFlow differs from related work by focusing on Chinese GUI interfaces, avoiding API-based layout extraction, employing hybrid visual encoders for... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.11295-b31b1b.svg)](https://arxiv.org/abs/2409.11295) | EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage | The paper identifies privacy risks in generalist web agents through adversarial environments, proposes the Environmental Injection Attack (EIA) as a n... | This work differs from related research by focusing on adversarial privacy risks in generalist web agents, introducing EIA as a targeted attack method... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.16500-b31b1b.svg)](https://arxiv.org/abs/2408.16500) | CogVLM2: Visual Language Models for Image and Video Understanding | The paper introduces the CogVLM2 family, enhancing vision-language fusion with improved training recipes, higher-resolution support (up to 1344x1344 p... | The work advances over related work by introducing an improved visual expert architecture with efficient higher-resolution processing, automated tempo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.02544v1-b31b1b.svg)](https://arxiv.org/abs/2408.02544) | Caution for the Environment: Multimodal Agents are Susceptible to Environmental... | The paper investigates the susceptibility of multimodal GUI agents to environmental distractions, introducing a novel research question on faithfulnes... | This work shifts focus from traditional action accuracy (helpfulness) to faithfulness in multimodal agents, introducing environmental distractions as... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.03037-b31b1b.svg)](https://arxiv.org/abs/2407.03037) | Seeing is Believing: Vision-driven Non-crash Functional Bug Detection for Mobile... | The paper introduces Trident, a vision-driven multi-agent collaborative framework for detecting non-crash functional bugs in mobile apps. It addresses... | Trident introduces a novel multi-agent system (Explorer, Monitor, Detector) integrated with MLLMs, enabling semantic understanding of GUI transitions... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.13032-b31b1b.svg)](https://arxiv.org/abs/2407.13032) | Agent-E: From Autonomous Web Navigation to Foundational Design Principles in Age... | The paper introduces Agent-E, a novel web agent with architectural improvements including hierarchical architecture, flexible DOM distillation/denoisi... | The work differs from related work by integrating hierarchical planning, DOM distillation/denoising, and change observation mechanisms, along with dom... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.14818v2-b31b1b.svg)](https://arxiv.org/abs/2409.14818) | MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding | This paper introduces MobileVLM, a specialized vision-language model designed to enhance intra- and inter-UI understanding for mobile agents. It addre... | Unlike prior work focusing on single-page UI understanding or chained datasets lacking structural context, MobileVLM introduces four UI-based pre-trai... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.03215v1-b31b1b.svg)](https://arxiv.org/abs/2409.03215) | xLAM: A Family of Large Action Models to Empower AI Agent Systems | The paper introduces xLAM, a family of large action models designed for AI agent tasks, addressing challenges in dataset scarcity and protocol standar... | xLAM differs by combining dense and mixture-of-expert architectures, offering a scalable training framework that addresses dataset limitations. It emp... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.01511v2-b31b1b.svg)](https://arxiv.org/abs/2407.01511) | CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents | Introduces Crab, the first cross-environment benchmark framework for multimodal language model agents, featuring graph-based fine-grained evaluation,... | Differs from related work by addressing limitations in single-environment benchmarks through cross-environment task support, generalized evaluation me... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.14337v2-b31b1b.svg)](https://arxiv.org/abs/2409.14337) | MobileViews: A Large-Scale Mobile GUI Dataset | The paper introduces MobileViews, the largest mobile screen dataset with over 600K screenshot-view hierarchy pairs from 20K+ Android apps. It addresse... | MobileViews differs from prior work by providing a large-scale, high-fidelity dataset collected over 81,600 device-hours using automated app traversal... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.04620-b31b1b.svg)](https://arxiv.org/abs/2407.04620) | Learning to (Learn at Test Time): RNNs with Expressive Hidden States | The paper introduces Test-Time Training (TTT) layers for sequence modeling, addressing limitations of traditional RNNs in handling long contexts. Key... | This work differs from related work by framing RNNs as test-time learners where hidden states are dynamically updated via self-supervised learning dur... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.15637-b31b1b.svg)](https://arxiv.org/abs/2409.15637) | Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agent... | The paper introduces Synatra, a method to convert indirect knowledge (e.g., online tutorials) into direct demonstrations for training digital agents.... | Unlike prior work reliant on expensive human demonstrations or complex reinforcement learning setups, Synatra leverages abundant indirect knowledge so... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.00993v1-b31b1b.svg)](https://arxiv.org/abs/2407.00993) | Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents | The paper introduces Mobile-Bench, a benchmark addressing three key challenges in evaluating LLM-based mobile agents: inefficiency of UI operations, i... | Mobile-Bench differs from prior work by integrating 103 APIs to enhance UI operation efficiency, categorizing tasks into SAST/SAMT/MAMT for varying co... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.10956v1-b31b1b.svg)](https://arxiv.org/abs/2407.10956) | Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engine... | The paper introduces Spider2-V, a benchmark for evaluating multimodal agents in data science and engineering workflows. It emphasizes real-world tasks... | Spider2-V differs from related work by creating the first benchmark specifically targeting professional data workflows, combining code generation, GUI... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.09295-b31b1b.svg)](https://arxiv.org/abs/2407.09295) | Systematic Categorization, Construction and Evaluation of New Attacks against Mu... | The paper introduces a systematic threat modeling methodology for multi-modal mobile GUI agents, identifies 34 novel attacks, and proposes SecMoba, a... | This work differs from related work by focusing specifically on mobile GUI agents, addressing the unique challenges of their environment, implementati... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.08862-b31b1b.svg)](https://arxiv.org/abs/2408.08862) | Visual Agents as Fast and Slow Thinkers | The paper introduces FaST, a framework that integrates System 1 (fast, intuitive) and System 2 (slow, deliberate) thinking mechanisms into visual agen... | FaST differs from related work by explicitly modeling System 1/2 cognitive distinctions through a switch adapter, enabling adaptive problem-solving fo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.10741v1-b31b1b.svg)](https://arxiv.org/abs/2409.10741) | NaviQAte: Functionality-Guided Web Application Navigation | NaviQAte introduces a functionality-guided approach to web application navigation by framing exploration as a question-and-answer task. It leverages l... | Unlike prior work focused on task-specific instructions or deterministic state transitions, NaviQAte employs LLMs for adaptive, functionality-driven e... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.15769-b31b1b.svg)](https://arxiv.org/abs/2408.15769) | A Survey on Evaluation of Multimodal Large Language Models | The paper provides a systematic review of MLLM evaluation methods, categorizing evaluation tasks into general and domain-specific areas, summarizing b... | This work differs from related work by offering a comprehensive survey that organizes existing MLLM evaluation methodologies into structured framework... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.15441-b31b1b.svg)](https://arxiv.org/abs/2409.15441) | Steward: Natural Language Web Automation | Steward introduces an LLM-powered web automation framework that enables natural language-driven interaction with websites, addressing scalability limi... | Steward differs by integrating reactive planning with off-the-shelf LLMs and vision models, combining HTML element proposal, planning-based synthesis,... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.07061-b31b1b.svg)](https://arxiv.org/abs/2407.07061) | Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Inte... | The paper introduces the Internet of Agents (IoA) framework, addressing ecosystem isolation, distributed environment simulation, and rigid communicati... | IoA differentiates from existing frameworks by providing a flexible, scalable platform for integrating third-party agents, enabling distributed collab... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.08693-b31b1b.svg)](https://arxiv.org/abs/2407.08693) | Robotic Control via Embodied Chain-of-Thought Reasoning | The paper introduces Embodied Chain-of-Thought Reasoning (ECoT) for vision-language-action models (VLAs) to enhance robotic control by enabling iterat... | This work differs from related work by integrating task-specific, sensor-grounded chain-of-thought reasoning into VLAs, enabling policies to plan and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.06886-b31b1b.svg)](https://arxiv.org/abs/2407.06886) | Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI | The paper provides a comprehensive survey of Embodied AI, focusing on four main research areas: embodied perception, interaction, agents, and sim-to-r... | This work differs from related studies by offering a comprehensive survey specifically tailored to the era of Multi-modal Large Models (MLMs), address... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.10943-b31b1b.svg)](https://arxiv.org/abs/2407.10943) | GRUtopia: Dream General Robots in a City at Scale | The paper introduces GRUtopia, a large-scale simulated 3D environment for embodied AI research, with key contributions including GRScenes (a diverse d... | Unlike prior works focused on home environments, GRUtopia extends to service-oriented scenarios with diverse scenes and integrates LLM-driven NPCs for... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.19056v1-b31b1b.svg)](https://arxiv.org/abs/2407.19056) | OfficeBench: Benchmarking Language Agents across Multiple Applications for Offic... | OfficeBench introduces a novel benchmark for evaluating language agents in realistic office automation tasks, emphasizing multi-application interactio... | OfficeBench differs from prior benchmarks by focusing on comprehensive office workflows involving multiple applications, requiring agents to perform a... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.18807-b31b1b.svg)](https://arxiv.org/abs/2409.18807) | LLM With Tools: A Survey | The paper provides a comprehensive survey on integrating tools with large language models (LLMs) to enhance task efficiency and accuracy. It introduce... | This work differs from related research by offering a unified framework for tool integration, focusing on actionable plan execution and dynamic adapta... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.17140-b31b1b.svg)](https://arxiv.org/abs/2409.17140) | AXIS: Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents | AXIS introduces an API-first framework for LLM-based agents to enhance human-agent-computer interaction (HACI) by prioritizing API calls over UI inter... | AXIS differs from existing LLM-based UI agents by focusing on API-first interactions rather than direct UI manipulation, addressing inefficiencies in... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.09018-b31b1b.svg)](https://arxiv.org/abs/2407.09018) | AUITestAgent: Automatic Requirements Oriented GUI Function Testing | The paper introduces AUITestAgent, a natural language-driven GUI testing tool for mobile apps that automates GUI interaction and verification. Key con... | AUITestAgent differs from related work by focusing on step-oriented GUI testing with dynamic agents for interaction extraction and multi-dimensional v... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.18921-b31b1b.svg)](https://arxiv.org/abs/2407.18921) | Mobile Edge Intelligence for Large Language Models: A Contemporary Survey | The paper provides a comprehensive survey on Mobile Edge Intelligence (MEI) for Large Language Models (LLMs), highlighting challenges in cloud-based L... | This work differs from related work by proposing the MEI4LLM architecture and systematically addressing edge-specific challenges for LLMs, while provi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.08725-b31b1b.svg)](https://arxiv.org/abs/2407.08725) | MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility | The paper introduces MetaUrban, a compositional simulation platform for urban micromobility research, enabling infinite interactive urban scenes with... | MetaUrban differs from related work by focusing on urban micromobility scenarios with compositional scene generation, addressing safety and generaliza... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.13729-b31b1b.svg)](https://arxiv.org/abs/2409.13729) | MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large Language Mo... | The paper introduces MathVL, a fine-tuning dataset for mathematical problems with diverse visual information, develops MathGLM-Vision models through s... | This work differs from related work by emphasizing the importance of diverse visual information in mathematical problem-solving, creating a specialize... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.18142-b31b1b.svg)](https://arxiv.org/abs/2409.18142) | A Survey on Multimodal Benchmarks: In the Era of Large AI Models | The paper provides a systematic review of 211 multimodal benchmarks for evaluating Multimodal Large Language Models (MLLMs), analyzing task designs, e... | This work differs from related studies by offering a comprehensive survey and analysis of existing benchmarks rather than proposing new models, datase... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.16090-b31b1b.svg)](https://arxiv.org/abs/2408.16090) | EPO: Hierarchical LLM Agents with Environment Preference Optimization | The paper introduces a hierarchical framework for long-horizon decision-making using LLMs, decomposing tasks into subgoals and low-level actions. It p... | The work differs from related work by combining hierarchical LLMs with EPO for reward signal generation, leveraging environment feedback instead of re... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.01927-b31b1b.svg)](https://arxiv.org/abs/2409.01927) | From Grounding to Planning: Benchmarking Bottlenecks in Web Agents | The paper decomposes web agents into planning and grounding components, identifies planning as the primary performance bottleneck, introduces a refine... | This work differs from related work by explicitly isolating and benchmarking planning and grounding components, challenging the assumption that ground... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.06318-b31b1b.svg)](https://arxiv.org/abs/2408.06318) | Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner... | The paper investigates challenges in long-horizon planning for LLM agents using the TravelPlanner benchmark, highlighting issues with context handling... | This work introduces the TravelPlanner benchmark to evaluate long-horizon planning and proposes FAFT, a feedback-aware fine-tuning approach that lever... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.00263-b31b1b.svg)](https://arxiv.org/abs/2407.00263) | From Local Concepts to Universals: Evaluating the Multicultural Understanding of... | The paper addresses cultural biases in vision-language models (VLMs) by introducing the GlobalRG benchmark, which evaluates multicultural understandin... | This work differs from related work by introducing the GlobalRG benchmark, which expands cultural coverage to 50 countries and introduces two novel ta... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.11965-b31b1b.svg)](https://arxiv.org/abs/2407.11965) | UrbanWorld: An Urban World Model for 3D City Generation | UrbanWorld introduces a novel generative model for creating realistic, customizable, and interactive 3D urban environments. It combines urban MLLM for... | UrbanWorld differs from related work by integrating an urban MLLM with diffusion models for controllable generation, enabling the first fully automati... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.10845-b31b1b.svg)](https://arxiv.org/abs/2408.10845) | CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving | The paper introduces CoVLA, a large-scale dataset integrating vision, language, and action for autonomous driving, addressing the lack of comprehensiv... | CoVLA bridges the gap in existing datasets by combining vision, language, and action annotations with real-world driving data, enabling end-to-end VLA... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.12089-b31b1b.svg)](https://arxiv.org/abs/2409.12089) | The Impact of Element Ordering on LM Agent Performance | The paper investigates how element ordering impacts LM agent performance in visual environments, proposes dimensionality reduction for pixel-based ele... | This work differs from related work by focusing on element ordering in pixel-only environments, proposing dimensionality reduction as an effective ord... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.01147-b31b1b.svg)](https://arxiv.org/abs/2408.01147) | Actra: Optimized Transformer Architecture for Vision-Language-Action Models in R... | Actra introduces an optimized Transformer architecture for vision-language-action (VLA) models in robot learning, featuring trajectory attention and l... | Actra differs from prior work by introducing trajectory attention for segmented multi-modal sequences and learnable action queries, addressing limitat... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.14567-b31b1b.svg)](https://arxiv.org/abs/2407.14567) | Integrating Artificial Intelligence into Operating Systems: A Comprehensive Surv... | The paper provides a comprehensive survey of AI integration into operating systems, focusing on techniques like machine learning and large language mo... | This work differs from related studies by offering a holistic review of AI-OS integration, synthesizing existing methodologies, identifying gaps in cu... |
| [![arXiv](https://img.shields.io/badge/arXiv-2407.00203-b31b1b.svg)](https://arxiv.org/abs/2407.00203) | PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-ag... | The paper introduces PathGen-1.6M, a large-scale high-quality pathology image-text dataset generated via multi-agent collaboration. It proposes PathGe... | Unlike prior work relying on limited external sources (PubMed, social media), this work generates high-quality pathology data through multi-agent coll... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.02834-b31b1b.svg)](https://arxiv.org/abs/2409.02834) | CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathemat... | The paper introduces CMM-Math, a Chinese multimodal math dataset for evaluating and enhancing large multimodal models (LMMs) in mathematical reasoning... | This work differs by creating the first Chinese multimodal math dataset (CMM-Math) and benchmark, addressing the lack of non-English resources. It als... |
| [![arXiv](https://img.shields.io/badge/arXiv-2408.00765-b31b1b.svg)](https://arxiv.org/abs/2408.00765) | MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models for Integ... | The paper introduces MM-Vet v2, an enhanced benchmark for evaluating large multimodal models (LMMs) with a focus on integrated capabilities. It adds a... | This work differs from related benchmarks (e.g., MME, MMBench, MM-Vet) by introducing a new core capability for evaluating image-text sequence underst... |
| [![arXiv](https://img.shields.io/badge/arXiv-2409.03256-b31b1b.svg)](https://arxiv.org/abs/2409.03256) | E2CL: Exploration-based Error Correction Learning for Embodied Agents | The paper introduces E2CL, a novel framework that leverages exploration-induced errors and environmental feedback to enhance environment alignment for... | E2CL differs from related work by integrating both teacher-guided and teacher-free exploration to gather environmental feedback and correct errors, ad... |


### 2024: Q4

| arXiv | Title | Summary | Contributions |
|-------|-------|---------|---------------|
| [![arXiv](https://img.shields.io/badge/arXiv-2410.05243-b31b1b.svg)](https://arxiv.org/abs/2410.05243) | Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Ag... | This paper introduces UGround, a universal visual grounding model for GUI agents that enables pixel-level visual interaction without relying on text-b... | The work differs from related work by proposing a vision-only approach for GUI agents that directly maps referring expressions to pixel-level coordina... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.23218v1-b31b1b.svg)](https://arxiv.org/abs/2410.23218) | OS-ATLAS: A Foundation Action Model for Generalist GUI Agents | This paper introduces OS-Atlas, a foundational GUI action model designed to enhance GUI grounding and Out-Of-Distribution (OOD) generalization for ope... | OS-Atlas differs from prior work by providing the first open-source foundation model specifically tailored for GUI agents, combining a large-scale cro... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.05271-b31b1b.svg)](https://arxiv.org/abs/2412.05271) | Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Da... | The paper introduces InternVL 2.5, an advanced open-source multimodal large language model (MLLM) that systematically explores model scaling, test-tim... | This work differs from related research by focusing on systematic analysis of model, data, and test-time scaling factors in MLLMs, achieving state-of-... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.18279-b31b1b.svg)](https://arxiv.org/abs/2411.18279) | Large Language Model-Brained GUI Agents: A Survey | The paper provides a comprehensive survey of LLM-brained GUI agents, covering their historical evolution, core components, advanced techniques, data u... | This work systematically integrates and synthesizes existing research on GUI automation and LLM-driven agents, offering a unified framework for unders... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.00820-b31b1b.svg)](https://arxiv.org/abs/2411.00820) | AutoGLM: Autonomous Foundation Agents for GUIs | AutoGLM introduces an intermediate interface to separate planning and grounding behaviors for GUI control, and a progressive training framework for se... | AutoGLM differs from related work by addressing GUI-specific challenges through an intermediate interface for decoupled planning/grounding and a novel... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.18967v1-b31b1b.svg)](https://arxiv.org/abs/2410.18967) | Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms | Ferret-UI 2 introduces multi-platform UI understanding, high-resolution adaptive scaling, and advanced data generation with GPT-4o visual prompting. I... | Ferret-UI 2 differs from prior work by supporting multiple platforms (iPhone, Android, Web, AppleTV), enabling high-resolution adaptive perception, an... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.04454-b31b1b.svg)](https://arxiv.org/abs/2412.04454) | Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction | Aguvis introduces a unified vision-based framework for autonomous GUI agents, addressing challenges in textual representation dependency, platform-spe... | Unlike prior work reliant on text or platform-specific actions, Aguvis operates directly on screen images with cross-platform standardization and stru... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.17465-b31b1b.svg)](https://arxiv.org/abs/2411.17465) | ShowUI: One Vision-Language-Action Model for GUI Visual Agent | ShowUI introduces a vision-language-action model for GUI agents with three key innovations: (1) UI-Guided Visual Token Selection to reduce computation... | ShowUI advances GUI agents by addressing limitations of text-based APIs and prior vision-language models through novel architectural components (UI-gu... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.19723-b31b1b.svg)](https://arxiv.org/abs/2412.19723) | OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthe... | OS-Genesis introduces a novel data synthesis pipeline for GUI agent trajectory generation through reverse task synthesis, enabling interaction-driven... | OS-Genesis differs from related work by reversing conventional trajectory collection processes, using interaction-driven exploration to generate traje... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.16256-b31b1b.svg)](https://arxiv.org/abs/2412.16256) | Aria-UI: Visual Grounding for GUI Instructions | Aria-UI introduces a pure-vision approach for GUI grounding without relying on HTML or AXTree inputs, proposes a scalable data pipeline for diverse in... | Unlike prior work reliant on auxiliary inputs (HTML/AXTree) or limited instruction data, Aria-UI combines pure vision with dynamic context modeling vi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.10302-b31b1b.svg)](https://arxiv.org/abs/2412.10302) | DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal... | The paper introduces DeepSeek-VL2, an advanced MoE-based Vision-Language Model with dynamic tiling vision encoding for high-resolution images and Mult... | DeepSeek-VL2 differs from related work by introducing dynamic tiling for flexible high-resolution vision processing and Multi-head Latent Attention fo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.04890-b31b1b.svg)](https://arxiv.org/abs/2411.04890) | GUI Agents with Foundation Models: A Comprehensive Survey | The paper provides a comprehensive survey of (M)LLM-based GUI agents, focusing on data resources, frameworks, and applications. It introduces a unifie... | This work differs from related studies by offering a structured, unified taxonomy of existing research, emphasizing application-oriented trends, and i... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.14803-b31b1b.svg)](https://arxiv.org/abs/2410.14803) | DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Devi... | The paper introduces DistRL, an asynchronous distributed reinforcement learning framework designed to enhance the efficiency of online fine-tuning for... | DistRL differs from prior work by addressing the inefficiencies of synchronous multi-machine setups (e.g., DigiRL) through asynchronous distributed tr... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.08164-b31b1b.svg)](https://arxiv.org/abs/2410.08164) | Agent S: An Open Agentic Framework that Uses Computers Like a Human | Agent S introduces an open agentic framework for GUI interaction, addressing domain knowledge acquisition, long-horizon planning, and dynamic interfac... | Agent S integrates existing MLLM capabilities with novel architecture components (ACI, hierarchical planning) to enable zero-shot GUI control, outperf... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.13501-b31b1b.svg)](https://arxiv.org/abs/2412.13501) | GUI Agents: A Survey | The paper provides a comprehensive survey of GUI agents, categorizing benchmarks, evaluation metrics, architectures, and training methods. It proposes... | This work differs from related research by offering a systematic framework that unifies GUI agent capabilities and providing a structured overview of... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.24024v2-b31b1b.svg)](https://arxiv.org/abs/2410.24024) | AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents | AndroidLab introduces a systematic framework for training and benchmarking Android autonomous agents, featuring a reproducible environment with divers... | Unlike prior benchmarks that relied on static environments or focused solely on closed-source models, AndroidLab offers a unified, interactive environ... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.02391-b31b1b.svg)](https://arxiv.org/abs/2411.02391) | Attacking Vision-Language Computer Agents via Pop-ups | The paper identifies vulnerabilities in vision-language models (VLMs) used by autonomous agents, demonstrating that adversarial pop-ups can effectivel... | This work differs from related work by focusing on adversarial attacks via pop-ups rather than proposing new models, datasets, or architectures. It hi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.02907-b31b1b.svg)](https://arxiv.org/abs/2410.02907) | NNetNav: Unsupervised Learning of Browser Agents Through Environment Interaction... | NNetNav introduces an unsupervised method for training browser agents by generating synthetic demonstrations through environment interaction. It lever... | Unlike prior work reliant on human demonstrations or supervised fine-tuning, NNetNav generates synthetic data through interaction and uses language-ba... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.02337v1-b31b1b.svg)](https://arxiv.org/abs/2411.02337) | WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement... | Introduces WebRL, a self-evolving online curriculum reinforcement learning framework for training open LLMs as web agents. Addresses task scarcity, sp... | Proposes WebRL as a dynamic, online learning framework that evolves tasks and rewards based on agent progress, unlike static task sets in prior work.... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.05467-b31b1b.svg)](https://arxiv.org/abs/2412.05467) | The BrowserGym Ecosystem for Web Agent Research | The paper introduces the BrowserGym ecosystem, a unified framework for evaluating web agents with standardized benchmarks and AgentLab for agent devel... | This work differs from related work by unifying existing benchmarks into a cohesive ecosystem, introducing AgentLab for agent creation/testing, and co... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.13824-b31b1b.svg)](https://arxiv.org/abs/2410.13824) | Harnessing Webpage UIs for Text-Rich Visual Understanding | The paper advances text-rich visual understanding by synthesizing multimodal instructions from webpage UIs using text-based LLMs, introduces the Multi... | This work differs by leveraging webpage accessibility trees to generate structured text instructions paired with UI screenshots, enabling models to ge... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.18603-b31b1b.svg)](https://arxiv.org/abs/2410.18603) | AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generali... | AgentStore introduces a scalable platform for integrating heterogeneous agents, addressing limitations in generalization and specialization of existin... | Unlike prior work with fixed or homogeneous agent systems, AgentStore enables dynamic integration of third-party agents through its novel MetaAgent ar... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.16464-b31b1b.svg)](https://arxiv.org/abs/2410.16464) | Beyond Browsing: API-Based Web Agents | The paper introduces API-Based Agents and Hybrid Agents that leverage APIs for web tasks, outperforming traditional GUI-based browsing agents. It demo... | This work differs by focusing on API-based interactions instead of GUI-based browsing, proposing Hybrid Agents that combine API calls with web navigat... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.13825-b31b1b.svg)](https://arxiv.org/abs/2410.13825) | AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents | The paper introduces AgentOccam, an LLM-based web agent that improves performance by aligning observation and action spaces with LLM pre-training data... | Unlike prior work relying on handcrafted prompting, search algorithms, or multi-agent systems, AgentOccam focuses on optimizing observation/action spa... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.18963-b31b1b.svg)](https://arxiv.org/abs/2410.18963) | OSCAR: Operating System Control via State-Aware Reasoning and Re-Planning | OSCAR introduces a generalist agent for OS-level GUI interaction using state-aware reasoning and dynamic re-planning. It translates natural language c... | OSCAR differs from prior work by combining state-aware re-planning with code-centric control, enabling real-time OS interaction through standardized i... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.18116-b31b1b.svg)](https://arxiv.org/abs/2412.18116) | AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation | The paper introduces AutoDroid-V2, a system that leverages small language models (SLMs) and code generation for mobile GUI task automation. It address... | AutoDroid-V2 differs from prior work by shifting from cloud-based LLMs to on-device SLMs for code generation, using app-specific API documentation and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.11871-b31b1b.svg)](https://arxiv.org/abs/2410.11871) | TinyClick: Single-Turn Agent for Empowering GUI Automation | TinyClick introduces a single-turn UI agent leveraging the Florence-2-Base model for efficient GUI automation. It achieves high accuracy (73.8% on Scr... | TinyClick differs from prior work by emphasizing efficiency and cost-effectiveness through multitask training and MLLM augmentation, achieving competi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.01268-b31b1b.svg)](https://arxiv.org/abs/2412.01268) | Ponder & Press: Advancing Visual GUI Agent towards General Computer Control | The paper introduces Ponder & Press, a visual-only GUI agent framework that separates task planning (Ponder) and GUI element localization (Press) usin... | Unlike prior work relying on non-visual inputs or single-step end-to-end models, Ponder & Press employs a divide-and-conquer architecture with dedicat... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.10323-b31b1b.svg)](https://arxiv.org/abs/2411.10323) | The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use | The paper introduces a preliminary case study of Claude 3.5 Computer Use as the first public beta GUI agent, evaluates its capabilities across diverse... | This work differs from related work by focusing on a real-world evaluation of a novel GUI agent (Claude 3.5 Computer Use) with an end-to-end API-based... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.06559-b31b1b.svg)](https://arxiv.org/abs/2411.06559) | Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web... | The paper introduces WebDreamer, a model-based planning framework for web agents that uses LLMs as world models and value functions. It presents a spe... | This work differs from related work by integrating LLMs as world models for model-based planning in web agents, addressing the limitations of reactive... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.15004-b31b1b.svg)](https://arxiv.org/abs/2411.15004) | ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data | The paper introduces ScribeAgent, a specialized web agent developed by fine-tuning open-source LLMs on production-scale workflow data. It achieves sta... | This work differs from related work by focusing on fine-tuning open-source LLMs with real-world workflow data rather than relying on prompting strateg... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.19609v1-b31b1b.svg)](https://arxiv.org/abs/2410.19609) | OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Explorat... | The paper introduces an open-source framework for multimodal web agents that enable iterative real-world exploration, feedback, and optimization. It a... | This work differs from related work by focusing on real-world, multimodal web agents using open-source models, addressing the lack of ground-truth sig... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.09605-b31b1b.svg)](https://arxiv.org/abs/2412.09605) | AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials | AgentTrek introduces a scalable data synthesis pipeline for generating high-quality GUI agent trajectories by leveraging web tutorials. It combines au... | Unlike prior methods reliant on expensive human-annotated trajectories, AgentTrek innovates through automated tutorial-based data synthesis, enabling... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.17883-b31b1b.svg)](https://arxiv.org/abs/2410.17883) | Lightweight Neural App Control | The paper introduces LiMAC, a lightweight architecture for mobile app control that combines a small Action Transformer (AcT) with a fine-tuned vision-... | LiMAC differs from related work by integrating a lightweight transformer network with a fine-tuned VLM, enabling efficient mobile app control without... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.17589-b31b1b.svg)](https://arxiv.org/abs/2412.17589) | PC Agent: While You Sleep, AI Works -- A Cognitive Journey into Digital World | The paper introduces PC Agent, a framework for digital agents that transfers human cognition to perform complex tasks. Key contributions include PC Tr... | This work differs from related work by focusing on cognitive data collection and transfer, combining planning and grounding agents, and emphasizing hu... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.11872-b31b1b.svg)](https://arxiv.org/abs/2410.11872) | ClickAgent: Enhancing UI Location Capabilities of Autonomous Agents | ClickAgent introduces a hybrid framework combining MLLM-driven reasoning with a specialized UI location model to address the challenge of UI element l... | Unlike prior works that rely on XML/OCR or single-modal reasoning, ClickAgent integrates a dedicated UI location model with an MLLM, enabling more acc... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.13757-b31b1b.svg)](https://arxiv.org/abs/2410.13757) | MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task A... | MobA introduces an adaptive planning module with reflection for error recovery, a multifaceted memory module for enhanced adaptability, and MobBench,... | MobA differs from related work by integrating adaptive planning with reflection mechanisms, multifaceted memory for task execution, and a novel benchm... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.10047-b31b1b.svg)](https://arxiv.org/abs/2412.10047) | Large Action Models: From Inception to Implementation | The paper introduces a comprehensive framework for developing Large Action Models (LAMs), transitioning from traditional LLMs to action-centric models... | This work differs from related research by offering a generalized, end-to-end framework for LAM development, focusing on practical implementation and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.07755-b31b1b.svg)](https://arxiv.org/abs/2412.07755) | SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models | The paper introduces SAT, a dynamic spatial aptitude training dataset for multimodal language models (MLMs) that addresses limitations in dynamic spat... | This work differs from related work by focusing on dynamic spatial reasoning through simulation-based training and real-world evaluation, whereas prio... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.09362-b31b1b.svg)](https://arxiv.org/abs/2412.09362) | Falcon-UI: Understanding GUI Before Following User Instructions | This paper introduces Falcon-UI, a GUI agent model that decouples GUI context understanding from instruction-following capabilities. It proposes the I... | Unlike prior work that couples GUI navigation with instruction-following during training, this paper decouples these components. It introduces Insight... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.19100v1-b31b1b.svg)](https://arxiv.org/abs/2410.19100) | VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understandin... | Introduces VideoWebArena (VideoWA), a benchmark for evaluating long-context multimodal agents on video understanding tasks. Focuses on skill retention... | This work addresses gaps in existing benchmarks by creating VideoWebArena, a comprehensive dataset and benchmark specifically designed to evaluate lon... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.13232-b31b1b.svg)](https://arxiv.org/abs/2410.13232) | Web Agents with World Models: Learning and Leveraging Environment Dynamics in We... | This paper introduces a world-model-augmented (WMA) web agent that simulates action outcomes to improve decision-making in web navigation. It addresse... | This work differs from prior research by integrating world models into LLM-based web agents, enabling foresight of environmental dynamics. Unlike tria... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.13194-b31b1b.svg)](https://arxiv.org/abs/2412.13194) | Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation Model I... | The paper introduces Proposer-Agent-Evaluator (PAE), a framework enabling foundation model agents to autonomously discover and refine skills through c... | PAE differs from prior work by replacing static human-curated task templates with an autonomous context-aware task proposer, integrating VLM-based suc... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.05261-b31b1b.svg)](https://arxiv.org/abs/2410.05261) | TextHawk2: A Large Vision-Language Model Excels in Bilingual OCR and Grounding w... | The paper introduces TextHawk2, a bilingual LVLM with 16x fewer image tokens for efficient fine-grained perception, excelling in OCR, grounding, and m... | Unlike prior work, TextHawk2 achieves 16x token compression without sacrificing performance, employs a unified visual encoder for OCR and grounding, a... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.06703v2-b31b1b.svg)](https://arxiv.org/abs/2410.06703) | ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness in Web A... | The paper introduces ST-WebAgentBench, a benchmark focused on evaluating safety and trustworthiness in web agents for enterprise settings. It addresse... | This work differs from related work by introducing ST-WebAgentBench, the first benchmark specifically designed to evaluate safety, trustworthiness, an... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.02006-b31b1b.svg)](https://arxiv.org/abs/2411.02006) | Foundations and Recent Trends in Multimodal Mobile Agents: A Survey | The paper provides a comprehensive survey of multimodal mobile agents, focusing on real-time adaptability, multimodal interaction, and recent advancem... | This work differs from related research by offering a structured survey of multimodal mobile agents, emphasizing their integration with foundation mod... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.18426-b31b1b.svg)](https://arxiv.org/abs/2412.18426) | GUI Testing Arena: A Unified Benchmark for Advancing Autonomous GUI Testing Agen... | The paper introduces GTArena, a unified benchmark for evaluating autonomous GUI testing agents, focusing on test intention generation, test task execu... | This work differs from related work by proposing a holistic benchmark (GTArena) that integrates test case generation, execution, and defect detection... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.10342-b31b1b.svg)](https://arxiv.org/abs/2412.10342) | Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining | The paper introduces Iris, a visual agent addressing GUI complexity through Information-Sensitive Cropping (ISC) for efficient high-resolution process... | Unlike prior work relying on structured data or fixed-resolution processing, Iris introduces ISC to dynamically allocate computational resources based... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.19054-b31b1b.svg)](https://arxiv.org/abs/2410.19054) | Infogent: An Agent-Based Framework for Web Information Aggregation | The paper introduces Infogent, a modular framework for web information aggregation that addresses the limitations of existing methods by focusing on e... | Infogent differs from related work by explicitly addressing information aggregation from multiple sources, rather than linear goal-oriented tasks. It... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.09604-b31b1b.svg)](https://arxiv.org/abs/2410.09604) | EmbodiedCity: A Benchmark Platform for Embodied Agent in Real-world City Environ... | The paper introduces EmbodiedCity, a benchmark platform for embodied agents in real-world city environments. It combines high-fidelity 3D simulations... | Unlike prior works focused on indoor or fictional city environments with limited realism, EmbodiedCity provides a high-fidelity, real-world city simul... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.04468-b31b1b.svg)](https://arxiv.org/abs/2411.04468) | Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks | Magentic-One introduces a modular multi-agent system with an Orchestrator agent for task planning and error recovery, specialized agents for GUI, file... | Magentic-One differentiates by using a structured ledger-based Orchestrator for multi-agent coordination, modular extensibility, and a novel benchmark... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.24164-b31b1b.svg)](https://arxiv.org/abs/2410.24164) | $œÄ_0$: A Vision-Language-Action Flow Model for General Robot Control | The paper introduces œÄ‚ÇÄ, a vision-language-action flow model for general robot control, combining pre-trained vision-language models (VLMs) with flow... | The work differs from prior vision-language-action (VLA) models by employing flow matching for action generation instead of autoregressive discretizat... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.10840-b31b1b.svg)](https://arxiv.org/abs/2412.10840) | Attention-driven GUI Grounding: Leveraging Pretrained Multimodal Large Language... | The paper introduces a tuning-free attention-driven grounding method (TAG) for GUI elements using pretrained MLLMs, leveraging attention patterns with... | This work differs from related work by proposing a tuning-free approach that utilizes inherent attention mechanisms of pretrained MLLMs for GUI ground... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.10345-b31b1b.svg)](https://arxiv.org/abs/2412.10345) | TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for General... | Introduces visual trace prompting to enhance spatial-temporal awareness in VLA models for robotic manipulation, develops the TraceVLA model and a comp... | Distinguishes from related work by explicitly encoding historical movement trajectories as visual prompts for VLA models, enabling better spatial-temp... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.22552-b31b1b.svg)](https://arxiv.org/abs/2410.22552) | Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language... | Auto-Intent introduces a method for adapting pre-trained LLMs to web navigation tasks without fine-tuning by discovering concise intents from demonstr... | This work differs from related work by proposing automated intent discovery from demonstrations and self-exploration with multiple semantically varied... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.19461-b31b1b.svg)](https://arxiv.org/abs/2410.19461) | EDGE: Enhanced Grounded GUI Understanding with Enriched Multi-Granularity Synthe... | The paper introduces EDGE, a data synthesis framework for generating multi-granularity synthetic data to enhance LVLMs' GUI understanding. It addresse... | Unlike prior work relying on text metadata or environment-specific backends, EDGE synthesizes large-scale, multi-granularity GUI data from webpages, e... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.00467-b31b1b.svg)](https://arxiv.org/abs/2410.00467) | Dynamic Planning for LLM-based Graphical User Interface Automation | The paper introduces Dynamic Planning of Thoughts (D-PoT) for LLM-based GUI agents, addressing the challenge of dynamic plan adaptation in GUI automat... | Unlike prior work focusing on GUI perception and multimodal understanding, this work introduces a novel planning mechanism (D-PoT) that dynamically ad... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.13591-b31b1b.svg)](https://arxiv.org/abs/2411.13591) | Improved GUI Grounding via Iterative Narrowing | The paper introduces an iterative narrowing framework for improving GUI grounding in Vision-Language Models (VLMs) by refining predictions through pro... | This work differs from related work by proposing an iterative visual prompting approach that refines predictions through localized cropping, avoiding... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.13451-b31b1b.svg)](https://arxiv.org/abs/2411.13451) | AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from Human Dem... | The paper introduces AdaptAgent, a framework enabling multimodal web agents to adapt to new websites/domains with few human demonstrations. It emphasi... | AdaptAgent differs from prior work by focusing on data-efficient few-shot adaptation using human demonstrations (instead of relying solely on large pr... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.10742-b31b1b.svg)](https://arxiv.org/abs/2412.10742) | WEPO: Web Element Preference Optimization for LLM-based Web Navigation | WEPO introduces a novel approach to LLM-based web navigation by leveraging unsupervised preference learning with distance-based non-salient HTML eleme... | WEPO differs from related work by focusing on HTML element-based contrastive learning for preference optimization, rather than relying on reinforcemen... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.09006-b31b1b.svg)](https://arxiv.org/abs/2410.09006) | From Interaction to Impact: Towards Safer AI Agents Through Understanding and Ev... | The paper introduces a comprehensive taxonomy of mobile UI action impacts through expert workshops, conducts a data synthesis study to gather impactfu... | This work differs from related studies by focusing on the consequences of UI actions rather than just navigation or automation. It introduces a novel... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.15100-b31b1b.svg)](https://arxiv.org/abs/2411.15100) | XGrammar: Flexible and Efficient Structured Generation Engine for Large Language... | XGrammar introduces a structured generation engine for LLMs that addresses efficiency in constrained decoding for complex applications. Key contributi... | XGrammar differs from related work by combining a pushdown automaton (PDA) with an adaptive token mask cache for efficient CFG execution, reducing run... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.03572-b31b1b.svg)](https://arxiv.org/abs/2412.03572) | Navigation World Models | The paper introduces a Navigation World Model (NWM) that combines video prediction with navigation planning, enabling agents to simulate and evaluate... | Unlike prior supervised navigation policies with fixed behavior, NWM uses a world model approach to simulate environments, dynamically incorporate con... |
| [![arXiv](https://img.shields.io/badge/arXiv-2412.13817-b31b1b.svg)](https://arxiv.org/abs/2412.13817) | Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via Hall... | This paper addresses object hallucinations in large vision-language models (LVLMs) by introducing HalluSpace, a low-rank subspace representing the dis... | Unlike prior works that rely on fine-tuning or post-processing, Nullu introduces a novel approach by identifying and projecting away HalluSpace‚Äîa subs... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.17520v1-b31b1b.svg)](https://arxiv.org/abs/2410.17520) | MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile Device Contr... | The paper introduces MobileSafetyBench, a benchmark for evaluating the safety of autonomous agents in mobile device control. It emphasizes safety-spec... | This work differs from related research by introducing the first benchmark specifically designed to evaluate safety in mobile device-control agents, d... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.15164v1-b31b1b.svg)](https://arxiv.org/abs/2410.15164) | SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation | SPA-Bench introduces a comprehensive benchmark for evaluating (M)LLM-based smartphone agents, featuring diverse tasks in English and Chinese, a plug-a... | SPA-Bench differs from prior work by combining action and state-based evaluation in a hybrid pipeline, offering cross-app and multilingual tasks, and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.11758-b31b1b.svg)](https://arxiv.org/abs/2410.11758) | Latent Action Pretraining from Videos | Introduces Latent Action Pretraining (LAPA) for Vision-Language-Action (VLA) models, enabling unsupervised pretraining from internet-scale videos with... | Differently from prior work reliant on labeled action data, LAPA leverages unsupervised pretraining from web-scale videos using a two-stage approach:... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.03314-b31b1b.svg)](https://arxiv.org/abs/2411.03314) | MME-Finance: A Multimodal Finance Benchmark for Expert-level Understanding and R... | The paper introduces MME-Finance, a bilingual multimodal benchmark tailored for financial domain tasks, emphasizing expert-level understanding of fina... | This work differs from related work by focusing on the financial domain, which requires specialized knowledge and handling of technical financial visu... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.06234-b31b1b.svg)](https://arxiv.org/abs/2410.06234) | TEOChat: A Large Vision-Language Assistant for Temporal Earth Observation Data | The paper introduces TEOChat, a vision-language assistant specialized for temporal Earth observation (EO) data, enabling conversational reasoning over... | TEOChat addresses limitations of prior vision-language models by specializing in temporal EO data, outperforming existing models on temporal tasks and... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.15461-b31b1b.svg)](https://arxiv.org/abs/2410.15461) | EVA: An Embodied World Model for Future Video Anticipation | The paper introduces RoG (Reflection of Generation) for enhancing video prediction in embodied scenarios, proposes the EVA model for high-fidelity vid... | This work differs from related work by combining pre-trained vision-language models (VLM) and video generation models (VDM) with intermediate reasonin... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.05474-b31b1b.svg)](https://arxiv.org/abs/2410.05474) | R-Bench: Are your Large Multimodal Model Robust to Real-world Corruptions? | R-Bench introduces a comprehensive benchmark for evaluating the real-world robustness of Large Multimodal Models (LMMs) by modeling the end-to-end cor... | Unlike prior benchmarks that focus on adversarial attacks or limited corruption types, R-Bench comprehensively models real-world corruption across the... |
| [![arXiv](https://img.shields.io/badge/arXiv-2411.02704-b31b1b.svg)](https://arxiv.org/abs/2411.02704) | RT-Affordance: Affordances are Versatile Intermediate Representations for Robot... | The paper introduces RT-Affordance, a hierarchical model using affordances as intermediate representations for robot manipulation. It addresses limita... | RT-Affordance differs from related work by using affordances as a lightweight, user-specifiable intermediate representation, avoiding over-specified c... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.08356-b31b1b.svg)](https://arxiv.org/abs/2410.08356) | SummAct: Uncovering User Intentions Through Interactive Behaviour Summarisation | Introduces SummAct, a hierarchical method for summarizing interactive behavior into high-level intentions using large language models and UI element a... | Differ from prior work by using hierarchical summarization and UI element attention to capture context-aware intentions, avoiding predefined categorie... |
| [![arXiv](https://img.shields.io/badge/arXiv-2410.08021-b31b1b.svg)](https://arxiv.org/abs/2410.08021) | OneRef: Unified One-tower Expression Grounding and Segmentation with Mask Referr... | This paper introduces OneRef, a unified one-tower framework for visual grounding and segmentation tasks that merges visual and linguistic feature spac... | Unlike prior methods relying on separate modality encoders or complex fusion decoders, OneRef unifies vision and language in a single transformer towe... |


### 2025: Q1

| arXiv | Title | Summary | Contributions |
|-------|-------|---------|---------------|
| [![arXiv](https://img.shields.io/badge/arXiv-2502.13923-b31b1b.svg)](https://arxiv.org/abs/2502.13923) | Qwen2.5-VL Technical Report | Qwen2.5-VL introduces enhanced visual recognition, precise object localization via bounding boxes/points, robust document parsing, and long-video comp... | Unlike prior work, Qwen2.5-VL introduces dynamic-resolution ViT and Window Attention for efficient native resolution processing, absolute time encodin... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.12326-b31b1b.svg)](https://arxiv.org/abs/2501.12326) | UI-TARS: Pioneering Automated GUI Interaction with Native Agents | UI-TARS introduces a native GUI agent model that achieves state-of-the-art performance through enhanced perception, unified action modeling, system-2... | UI-TARS differs from related work by being a pure-vision, end-to-end native agent model that eliminates reliance on commercial models, textual represe... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.11733-b31b1b.svg)](https://arxiv.org/abs/2501.11733) | Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks | Introduces Mobile-Agent-E, a hierarchical multi-agent framework with self-evolution capabilities for complex mobile tasks. Key innovations include Tip... | Differently from prior work, Mobile-Agent-E introduces hierarchical multi-agent architecture with dedicated roles (Manager, Perceptor, Operator, etc.)... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.04575-b31b1b.svg)](https://arxiv.org/abs/2501.04575) | InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflec... | InfiGUIAgent introduces a two-stage supervised fine-tuning pipeline to enhance GUI understanding, hierarchical reasoning, and expectation-reflection r... | Unlike prior work reliant on accessibility trees or single-step reasoning, InfiGUIAgent employs synthesized data for hierarchical reasoning and reflec... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.21620-b31b1b.svg)](https://arxiv.org/abs/2503.21620) | UI-R1: Enhancing Efficient Action Prediction of GUI Agents by Reinforcement Lear... | The paper introduces UI-R1, a framework that applies rule-based reinforcement learning (RL) to enhance multimodal large language models (MLLMs) for GU... | UI-R1 differs from prior work by extending rule-based RL to GUI action prediction, introducing a task-specific reward function for action type and arg... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.01149-b31b1b.svg)](https://arxiv.org/abs/2501.01149) | A3: Android Agent Arena for Mobile GUI Agents | A3 introduces a comprehensive evaluation platform for mobile GUI agents, addressing limitations of existing static-frame datasets by providing real-wo... | A3 differs by focusing on real-world, dynamic tasks with automated evaluation, unlike existing static-frame datasets and benchmarks that lack comprehe... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.18906-b31b1b.svg)](https://arxiv.org/abs/2502.18906) | VEM: Environment-Free Exploration for Training GUI Agent with Value Environment... | The paper introduces an environment-free reinforcement learning framework for GUI agents using a pretrained Value Environment Model (VEM) that estimat... | This work differs from related work by proposing a semantic-aware value estimation framework (VEM) that eliminates the need for environment interactio... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.10893-b31b1b.svg)](https://arxiv.org/abs/2501.10893) | Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realisti... | The paper introduces Learn-by-interact, a data-centric framework for adapting LLM agents to realistic environments without human annotations. Key cont... | Learn-by-interact differs from prior work by autonomously synthesizing high-quality agent data through backward construction, eliminating reliance on... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.06395-b31b1b.svg)](https://arxiv.org/abs/2502.06395) | AppVLM: A Lightweight Vision Language Model for Online App Control | The paper introduces AppVLM, a lightweight Vision-Language Model (VLM) designed for efficient app control on smartphones. It addresses limitations of... | AppVLM differs from related work by combining offline dataset fine-tuning with online environment data collection for policy refinement, achieving bot... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.13130-b31b1b.svg)](https://arxiv.org/abs/2502.13130) | Magma: A Foundation Model for Multimodal AI Agents | Magma introduces a foundation model for multimodal AI agents that integrates verbal intelligence (vision-language understanding) with spatial-temporal... | Magma differs from related work by unifying multimodal understanding with spatial-temporal action planning through SoM/ToM pretraining, enabling gener... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.13053-b31b1b.svg)](https://arxiv.org/abs/2502.13053) | Evaluating the Robustness of Multimodal Agents Against Active Environmental Inje... | The paper introduces the concept of Active Environment Injection Attacks (AEIA) targeting multimodal agents interacting with Android OS. It identifies... | This work differs from related research by focusing on AEIA as a novel threat model that exploits environmental interaction vulnerabilities rather tha... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.18356-b31b1b.svg)](https://arxiv.org/abs/2502.18356) | WebGames: Challenging General-Purpose Web-Browsing AI Agents | Introduces WebGames, a comprehensive benchmark suite for evaluating web-browsing AI agents through 50+ interactive challenges, highlighting significan... | WebGames differs from related work by providing a hermetic, controlled environment without external dependencies, offering reproducible evaluations wi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.02268-b31b1b.svg)](https://arxiv.org/abs/2503.02268) | AppAgentX: Evolving GUI Agents as Proficient Smartphone Users | The paper introduces an evolutionary framework for GUI agents that enhances efficiency by abstracting high-level actions from task execution history,... | Unlike traditional rule-based systems or LLM-based agents relying on step-by-step reasoning, AppAgentX introduces an evolutionary architecture with me... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.02982-b31b1b.svg)](https://arxiv.org/abs/2502.02982) | MobileA3gent: Training Mobile GUI Agents Using Decentralized Self-Sourced Data f... | The paper introduces MobileA3gent, a framework for training mobile GUI agents using decentralized self-sourced data from diverse users. Key contributi... | This work differs from related work by proposing a decentralized data collection paradigm and automatic annotation to replace costly human labeling, a... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.11069-b31b1b.svg)](https://arxiv.org/abs/2503.11069) | API Agents vs. GUI Agents: Divergence and Convergence | The paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, analyzing their architectural differences, develop... | This work differs from related work by systematically comparing API-first and GUI-first LLM agents, highlighting their divergence in operational princ... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.17352-b31b1b.svg)](https://arxiv.org/abs/2503.17352) | OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles | Introduces OpenVLThinker, an open-source large vision-language model (LVLM) that combines iterative supervised fine-tuning (SFT) and reinforcement lea... | Proposes an iterative SFT-RL framework for LVLMs, overcoming challenges of visual grounding and search space inefficiencies in reasoning tasks. Unlike... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.15760-b31b1b.svg)](https://arxiv.org/abs/2502.15760) | Digi-Q: Learning Q-Value Functions for Training Device-Control Agents | Digi-Q introduces a method to train VLM-based action-value Q-functions using offline temporal-difference learning on frozen VLM features, enabling pol... | Digi-Q differs from related work by using offline TD learning with frozen VLM features instead of on-policy reinforcement learning or fine-tuning, ena... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.00401-b31b1b.svg)](https://arxiv.org/abs/2503.00401) | Smoothing Grounding and Reasoning for MLLM-Powered GUI Agents with Query-Oriente... | The paper addresses the gap between coordinate-oriented grounding and action-oriented reasoning in resource-constrained GUI agents by introducing quer... | This work differs from related work by proposing query inference, a query-oriented approach that bridges grounding and reasoning tasks without requiri... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.06470-b31b1b.svg)](https://arxiv.org/abs/2503.06470) | Think Twice, Click Once: Enhancing GUI Grounding via Fast and Slow Systems | The paper introduces Focus, a dual-system GUI grounding framework inspired by human cognitive processes, combining fast prediction with systematic ana... | Focus differs from related work by integrating a dual-system architecture that simulates human dual-process cognition, enabling structured decompositi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.07572-b31b1b.svg)](https://arxiv.org/abs/2501.07572) | WebWalker: Benchmarking LLMs in Web Traversal | The paper introduces WebWalkerQA, a benchmark for evaluating LLMs in web traversal tasks, and proposes WebWalker, a multi-agent framework using an exp... | This work differs from related work by combining RAG with a novel multi-agent framework (WebWalker) for systematic web traversal, addressing limitatio... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.17709-b31b1b.svg)](https://arxiv.org/abs/2503.17709) | GUI-Xplore: Empowering Generalizable GUI Agents with One Exploration | GUI-Xplore introduces a novel dataset and framework to address cross-app and cross-task generalization in GUI agents. The dataset incorporates explora... | This work differs from related work by addressing critical limitations in existing benchmarks through a dataset that captures developer-induced struct... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.16411-b31b1b.svg)](https://arxiv.org/abs/2501.16411) | PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World... | The paper introduces PhysBench, a comprehensive benchmark for evaluating Vision-Language Models (VLMs) in physical world understanding, and proposes P... | This work differs from related work by introducing PhysBench as a specialized benchmark for physical world understanding, along with PhysAgent, which... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.12485-b31b1b.svg)](https://arxiv.org/abs/2501.12485) | R2D2: Remembering, Replaying and Dynamic Decision Making with a Reflective Agent... | The paper introduces the R2D2 framework, which integrates 'Remember' and 'Reflect' paradigms to enhance web agents' navigation and decision-making. Ke... | R2D2 differs from prior work by combining memory-enhanced navigation (replay buffer for environment mapping) and reflective learning (error analysis/s... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.17110-b31b1b.svg)](https://arxiv.org/abs/2502.17110) | Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational... | The paper introduces Mobile-Agent-V, a video-guided framework for injecting operational knowledge into mobile automation, addressing limitations in ex... | Unlike prior work reliant on manual textual knowledge or limited training data, Mobile-Agent-V leverages raw video to extract procedural knowledge, in... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.15937-b31b1b.svg)](https://arxiv.org/abs/2503.15937) | Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment | V-Droid introduces a verifier-driven paradigm for mobile GUI agents, employing LLMs as verifiers rather than generators. Key contributions include dis... | Unlike prior LLM-powered agents that use generators for action creation, V-Droid uses LLMs as verifiers to evaluate candidate actions, combined with n... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.10074-b31b1b.svg)](https://arxiv.org/abs/2501.10074) | SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and Chain-o... | The paper introduces SpatialCoT, a novel approach that enhances spatial reasoning in Vision-Language Models (VLMs) through spatial coordinate bi-direc... | SpatialCoT differs from related work by integrating spatial coordinate alignment with chain-of-thought reasoning, overcoming the limitations of langua... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.13685-b31b1b.svg)](https://arxiv.org/abs/2502.13685) | MoM: Linear Sequence Modeling with Mixture-of-Memories | Introduces Mixture-of-Memories (MoM), a novel architecture inspired by neuroscience principles to enhance memory capacity and reduce interference in l... | MoM differs from related work by introducing multiple memory states with a router mechanism to mitigate memory interference, unlike existing linear mo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.11859-b31b1b.svg)](https://arxiv.org/abs/2502.11859) | Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Persp... | The paper introduces a psychometric framework to evaluate five Basic Spatial Abilities (BSAs) in Visual Language Models (VLMs), benchmarks 13 mainstre... | This work differs from related studies by establishing a psychometric framework grounded in Gardner's Theory of Multiple Intelligences, systematically... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.14021-b31b1b.svg)](https://arxiv.org/abs/2503.14021) | MP-GUI: Modality Perception with MLLMs for GUI Understanding | MP-GUI introduces a specialized MLLM for GUI understanding, addressing spatial structure modeling challenges through three modality-specific perceiver... | Unlike prior work, MP-GUI explicitly models spatial structures in GUIs through specialized perceivers and a fusion gate, enabling task-specific prefer... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.07949-b31b1b.svg)](https://arxiv.org/abs/2502.07949) | Advancing Autonomous VLM Agents via Variational Subgoal-Conditioned Reinforcemen... | The paper introduces Variational Subgoal-Conditioned Reinforcement Learning (VSC-RL) to enhance VLM agents for complex decision-making tasks. It addre... | Unlike existing RL-based agents, VSC-RL introduces a variational subgoal-conditioned framework with SGC-ELBO, enabling efficient learning through subg... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.08047-b31b1b.svg)](https://arxiv.org/abs/2502.08047) | WorldGUI: An Interactive Benchmark for Desktop GUI Automation from Any Starting... | This work introduces WorldGUI, a benchmark addressing dynamic GUI automation challenges by simulating diverse initial states. It proposes WorldGUI-Age... | Unlike prior benchmarks focusing on static initial/final states, WorldGUI introduces dynamic testing with intermediate starting states and contextual... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.02403-b31b1b.svg)](https://arxiv.org/abs/2503.02403) | AutoEval: A Practical Framework for Autonomous Evaluation of Mobile Agents | AutoEval introduces an autonomous evaluation framework for mobile agents, focusing on automated task reward signal generation via Structured Substate... | This work differs from related work by automating the generation of task reward signals and evaluation processes, eliminating manual effort required i... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.15661-b31b1b.svg)](https://arxiv.org/abs/2503.15661) | UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction | Introduces UI-Vision, a comprehensive benchmark for evaluating GUI agents in desktop environments with dense annotations, three fine-to-coarse grained... | UI-Vision addresses data collection challenges in desktop environments by providing the first license-permissive, offline benchmark with high-quality... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.05143-b31b1b.svg)](https://arxiv.org/abs/2503.05143) | FedMABench: Benchmarking Mobile Agents on Decentralized Heterogeneous User Data | The paper introduces FedMABench, the first benchmark for federated mobile agent training, addressing the lack of standardized benchmarks in decentrali... | FedMABench fills the gap in federated mobile agent research by providing the first comprehensive benchmark with diverse datasets, federated algorithms... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.18470-b31b1b.svg)](https://arxiv.org/abs/2503.18470) | MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse | The paper introduces MetaSpatial, an RL-based framework enhancing 3D spatial reasoning in VLMs for realistic 3D scene generation. It addresses limitat... | MetaSpatial differs by employing reinforcement learning for dynamic spatial reasoning, integrating physics-aware constraints, and using multi-turn ref... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.13896-b31b1b.svg)](https://arxiv.org/abs/2501.13896) | GUI-Bee: Align GUI Action Grounding to Novel Environments via Autonomous Explora... | This work addresses the challenge of GUI action grounding in novel environments by introducing GUI-Bee, an autonomous agent that collects environment-... | Unlike prior works relying on pre-existing datasets, this paper introduces an autonomous agent (GUI-Bee) that dynamically collects environment-specifi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.02955-b31b1b.svg)](https://arxiv.org/abs/2502.02955) | ReachAgent: Enhancing Mobile Agent via Page Reaching and Operation | This work introduces ReachAgent, a two-stage framework for mobile AI agents that addresses task completion by decomposing tasks into page reaching and... | ReachAgent differs from prior work by explicitly decomposing tasks into page reaching and operation subtasks, introducing a two-stage framework with r... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.16465-b31b1b.svg)](https://arxiv.org/abs/2503.16465) | OS-Kairos: Adaptive Interaction for MLLM-Powered GUI Agents | Addresses over-execution in MLLM-powered GUI agents by introducing OS-Kairos, which predicts confidence levels for adaptive human-agent collaboration.... | OS-Kairos introduces confidence-based adaptive interaction to mitigate over-execution, differing from prior work focused on autonomous execution witho... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.09572-b31b1b.svg)](https://arxiv.org/abs/2503.09572) | Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks | The paper introduces Plan-and-Act, a framework that separates high-level planning from low-level execution for long-horizon tasks. It addresses planni... | Plan-and-Act introduces a scalable synthetic data generation method for training planners, a simpler 2-agent architecture compared to prior works with... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.15558-b31b1b.svg)](https://arxiv.org/abs/2503.15558) | Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning | The paper introduces Cosmos-Reason1, a family of multimodal large language models specialized in physical world understanding and reasoning. Key contr... | This work differs from related work by explicitly defining ontologies for physical common sense (Space, Time, Fundamental Physics) and embodied reason... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.00217-b31b1b.svg)](https://arxiv.org/abs/2501.00217) | The Potential of LLMs in Automating Software Testing: From Generation to Reporti... | The paper introduces an LLM-powered agent framework for automated software testing, focusing on test generation, call graph visualization, and automat... | This work differs from related ABST methods by integrating Large Language Models (LLMs) into an agent-based framework for testing, enabling end-to-end... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.08226-b31b1b.svg)](https://arxiv.org/abs/2502.08226) | TRISHUL: Towards Region Identification and Screen Hierarchy Understanding for La... | TRISHUL introduces a training-free framework for holistic GUI comprehension by integrating action grounding and GUI referring tasks. It addresses cros... | TRISHUL differs by combining action grounding and GUI referring in a single framework without training, leveraging spatial-semantic reasoning via HSP... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.03971-b31b1b.svg)](https://arxiv.org/abs/2502.03971) | RWKV-UI: UI Understanding with Enhanced Perception and Reasoning | RWKV-UI enhances UI understanding through layout detection prompts, Chain-of-Thought (CoT) based visual prompts, and a three-encoder architecture for... | RWKV-UI differs from related work by integrating layout detection and CoT-based visual prompts for enhanced reasoning, employing a three-encoder archi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.01977-b31b1b.svg)](https://arxiv.org/abs/2502.01977) | AutoGUI: Scaling GUI Grounding with Automatic Functionality Annotations from LLM... | The paper introduces AutoGUI, an automatic annotation pipeline for generating large-scale, detailed functionality annotations of UI elements. It addre... | AutoGUI differs from related work by providing the first large-scale, contextually rich UI element functionality annotations generated automatically v... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.04730-b31b1b.svg)](https://arxiv.org/abs/2503.04730) | WinClick: GUI Grounding with Multimodal Large Language Models | Introduces WinClick, a novel visual GUI agent for Windows that operates on raw screenshots without relying on structured data formats. Develops WinSpo... | This work differs from related work by focusing on desktop environments (specifically Windows) where structured data is inaccessible, introducing WinS... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.03196-b31b1b.svg)](https://arxiv.org/abs/2503.03196) | SpiritSight Agent: Advanced GUI Agent with One Look | The paper introduces SpiritSight, a vision-based end-to-end GUI agent with enhanced GUI navigation capabilities. Key contributions include the creatio... | This work differs from related work by addressing element grounding limitations in vision-based GUI agents through the GUI-Lasagne dataset and the UBP... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.02863-b31b1b.svg)](https://arxiv.org/abs/2501.02863) | Beyond Pass or Fail: Multi-Dimensional Benchmarking of Foundation Models for Goa... | The paper introduces Sphinx, a multi-dimensional benchmark for evaluating foundation models (FMs) in mobile UI navigation, addressing limitations of e... | Sphinx differs from related work by providing a comprehensive, multi-dimensional evaluation framework for FMs in UI navigation, incorporating real-wor... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.09780-b31b1b.svg)](https://arxiv.org/abs/2503.09780) | AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents | This work introduces AgentDAM, a benchmark to evaluate AI web agents' adherence to data minimization principles. It highlights privacy risks in autono... | Unlike prior work that probes LLMs directly, this study provides an end-to-end benchmark simulating real web interactions, offering a more realistic a... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.04957-b31b1b.svg)](https://arxiv.org/abs/2503.04957) | SafeArena: Evaluating the Safety of Autonomous Web Agents | The paper introduces SafeArena, the first benchmark for evaluating the safety of autonomous web agents through deliberate misuse scenarios. It systema... | This work differs from related research by focusing specifically on deliberate malicious use cases of web agents, introducing a structured benchmark w... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.07919-b31b1b.svg)](https://arxiv.org/abs/2503.07919) | BEARCUBS: A benchmark for computer-using web agents | Introduces BEARCUBS, a benchmark evaluating web agents' real-world capabilities through live web interactions and multimodal tasks. Highlights challen... | BEARCUBS differs from prior work by using live web content and requiring diverse multimodal interactions (e.g., video understanding, 3D navigation) th... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.02189-b31b1b.svg)](https://arxiv.org/abs/2501.02189) | A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark,... | The paper provides a comprehensive survey of Vision Language Models (VLMs), covering their architectures, alignment methods, benchmarks, and challenge... | This work differs from related work by providing the first comprehensive survey specifically focused on multi-modal VLMs, addressing alignment, benchm... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.11170-b31b1b.svg)](https://arxiv.org/abs/2503.11170) | DeskVision: Large Scale Desktop Region Captioning for Advanced GUI Agents | Introduces AutoCaptioner for automated GUI data generation, creates DeskVision (first large-scale desktop dataset) and DeskVision-Eval (largest deskto... | Focuses on desktop GUI data gap by creating the first large-scale real-world desktop dataset (DeskVision) and benchmark (DeskVision-Eval), along with... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.11357-b31b1b.svg)](https://arxiv.org/abs/2502.11357) | Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web... | The paper introduces a scalable framework for synthesizing diverse web trajectories, develops a large-scale multimodal dataset (94K trajectories, 720K... | Unlike prior work relying on human-annotated data or limited synthetic generation, Explorer employs exploration-driven trajectory synthesis to dynamic... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.24180-b31b1b.svg)](https://arxiv.org/abs/2503.24180) | Navi-plus: Managing Ambiguous GUI Navigation Tasks with Follow-up | Introduces the Self-Correction GUI Navigation task, develops the Navi-plus dataset with follow-up QA pairs, and proposes Dual-Stream Trajectory Evalua... | Differently addresses ambiguous user tasks by enabling GUI agents to actively request clarifying follow-up questions, contrasting with prior works foc... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.06776-b31b1b.svg)](https://arxiv.org/abs/2502.06776) | InSTA: Towards Internet-Scale Training For Agents | The paper introduces the InSTA pipeline for internet-scale training of web navigation agents, leveraging LLMs for task annotation, trajectory generati... | The work differs by automating data curation through LLMs for task generation and filtering, creating a large-scale dataset of 150k websites with safe... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.04747-b31b1b.svg)](https://arxiv.org/abs/2502.04747) | Every Software as an Agent: Blueprint and Case Study | The paper proposes a whitebox approach for software agents by integrating large language models (LLMs) with software internals (source code and runtim... | This work differs from related work by advocating a whitebox paradigm where LLMs access software internals for dynamic code generation and execution,... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.23434-b31b1b.svg)](https://arxiv.org/abs/2503.23434) | Towards Trustworthy GUI Agents: A Survey | The paper provides a comprehensive survey on the trustworthiness of GUI agents, examining security vulnerabilities, reliability in dynamic environment... | This work differs from related studies by systematically addressing the multidimensional trustworthiness of GUI agents, highlighting previously overlo... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.16150-b31b1b.svg)](https://arxiv.org/abs/2501.16150) | A Comprehensive Survey of Agents for Computer Use: Foundations, Challenges, and... | The paper provides a comprehensive taxonomy for agents for computer use (ACUs) across three dimensions (domain, interaction, agent perspectives), anal... | This work differs from related work by offering a unifying taxonomy for ACUs, systematically analyzing existing approaches through multi-dimensional p... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.07942-b31b1b.svg)](https://arxiv.org/abs/2502.07942) | Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths of Larg... | The paper introduces AgentSymbiotic, an iterative framework combining large and small LLMs for web agents, leveraging their complementary strengths. K... | This work differs by integrating iterative data synthesis with task performance, addressing off-policy bias and small LLM bottlenecks through novel di... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.02950-b31b1b.svg)](https://arxiv.org/abs/2503.02950) | LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications | LiteWebAgent introduces an extensible framework for VLM-based web agents, combining a production-ready core with modular research components (planning... | LiteWebAgent differentiates itself by providing a unified framework that integrates VLMs with web agents through decoupled action generation and groun... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.19537-b31b1b.svg)](https://arxiv.org/abs/2503.19537) | Agent-Initiated Interaction in Phone UI Automation | The paper introduces a task formulation for detecting user interaction needs in phone UI automation agents, generates appropriate messages for user en... | This work differs from related work by focusing on agent-initiated user interaction as a critical yet underexplored aspect of UI automation, providing... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.14282-b31b1b.svg)](https://arxiv.org/abs/2502.14282) | PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Au... | The paper introduces PC-Agent, a hierarchical multi-agent framework for complex PC task automation, featuring an Active Perception Module (APM) for en... | Unlike prior work focused on mobile/web or limited PC tasks, PC-Agent addresses cross-app workflows and inter-subtask dependencies via a hierarchical... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.09263-b31b1b.svg)](https://arxiv.org/abs/2503.09263) | COLA: A Scalable Multi-Agent Framework For Windows UI Task Automation | COLA introduces a collaborative multi-agent framework for Windows UI automation, addressing static architecture limitations and workflow fault toleran... | Unlike prior work like UFO and MMAC, COLA introduces a dynamic task decomposition system with a decision agent pool, memory units for agent evolution,... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.18525-b31b1b.svg)](https://arxiv.org/abs/2502.18525) | Programming with Pixels: Computer-Use Meets Software Engineering | Introduces PwP, a unified environment for computer-use agents interacting with IDEs via visual perception, typing, and clicking, and PwP-Bench, a benc... | Shifts from tool-based paradigms to computer-use agents, unifying diverse SWE benchmarks into PwP-Bench, and highlighting the potential of direct IDE... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.12532-b31b1b.svg)](https://arxiv.org/abs/2503.12532) | STEVE: A Step Verification Pipeline for Computer-use Agent Training | STEVE introduces a step verification pipeline for training computer-use agents, leveraging GPT-4o for binary step labeling and Kahneman and Tversky Op... | STEVE differs by integrating step-wise verification with GPT-4o and KTO optimization, enabling scalable training with both positive/negative trajector... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.13843-b31b1b.svg)](https://arxiv.org/abs/2503.13843) | WebNav: An Intelligent Agent for Voice-Controlled Web Navigation | Introduces WebNav, a dual LLM architecture for voice-controlled web navigation, combining vision-based context and dynamic DOM labeling. Key innovatio... | WebNav differs from related work by employing a modular two-stage LLM architecture (Controller for strategy, Assistant for precise actions) and integr... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.17903-b31b1b.svg)](https://arxiv.org/abs/2502.17903) | Towards Sustainable Web Agents: A Plea for Transparency and Dedicated Metrics fo... | The paper emphasizes the environmental impact of web agents, advocates for transparency in model parameters and processes, and introduces the necessit... | This work differs from related work by focusing on sustainability and energy efficiency in web agents, highlighting the environmental costs of current... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.18492-b31b1b.svg)](https://arxiv.org/abs/2503.18492) | Safeguarding Mobile GUI Agent via Logic-based Action Verification | Introduces VeriSafe Agent (VSA), a logic-based verification system for Mobile GUI Agents to ensure alignment with user intent. Key contributions inclu... | VSA introduces the first formal verification framework for GUI agents, combining autoformalization with logic-based verification to address LFM unpred... |
| [![arXiv](https://img.shields.io/badge/arXiv-2502.17812-b31b1b.svg)](https://arxiv.org/abs/2502.17812) | Can Multimodal LLMs Perform Time Series Anomaly Detection? | This paper investigates the potential of multimodal large language models (MLLMs) for time series anomaly detection (TSAD). It introduces the VisualTi... | This work is the first comprehensive study exploring MLLMs for TSAD, extending beyond univariate scenarios to multivariate and irregular time series.... |
| [![arXiv](https://img.shields.io/badge/arXiv-2501.02699-b31b1b.svg)](https://arxiv.org/abs/2501.02699) | EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in Instructional Multi... | The paper introduces EAGLE, a method to reduce hallucinations in instructional multimodal models by enhancing visual grounding through a reformulated... | EAGLE differs from related work by directly enhancing the visual encoder's fine-grained grounding through a post-pretraining tuning strategy, rather t... |
| [![arXiv](https://img.shields.io/badge/arXiv-2503.20978-b31b1b.svg)](https://arxiv.org/abs/2503.20978) | ScreenLLM: Stateful Screen Schema for Efficient Action Understanding and Predict... | The paper introduces stateful screen schema for efficient GUI interaction representation and proposes ScreenLLM, a multimodal large language model spe... | Unlike prior work, ScreenLLM combines stateful screen schema with MLLMs to explicitly model temporal user actions and intentions, overcoming limitatio... |


### 2025: Q2

| arXiv | Title | Summary | Contributions |
|-------|-------|---------|---------------|
| [![arXiv](https://img.shields.io/badge/arXiv-2504.00906-b31b1b.svg)](https://arxiv.org/abs/2504.00906) | Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agent... | Agent S2 introduces a compositional generalist-specialist framework for computer use agents, addressing GUI grounding, long-horizon planning, and task... | Unlike prior work relying on single generalist models, Agent S2 introduces a compositional architecture that delegates tasks between generalist and sp... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.10458-b31b1b.svg)](https://arxiv.org/abs/2504.10458) | GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents | The paper introduces GUI-R1, a reinforcement learning framework for GUI agents that addresses limitations of supervised fine-tuning by using unified a... | GUI-R1 differs from prior work by employing reinforcement fine-tuning (RFT) with a unified action space, enabling better generalization and performanc... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.01382-b31b1b.svg)](https://arxiv.org/abs/2504.01382) | An Illusion of Progress? Assessing the Current State of Web Agents | The paper critically assesses current web agents, identifies benchmark shortcomings, introduces Online-Mind2Web benchmark, develops LLM-as-a-Judge aut... | This work differs from related work by introducing a realistic online benchmark (Online-Mind2Web) and an LLM-based automatic evaluation method with hi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.07491-b31b1b.svg)](https://arxiv.org/abs/2504.07491) | Kimi-VL Technical Report | Kimi-VL introduces an efficient MoE-based vision-language model with a 128K context window, native-resolution MoonViT encoder for high-resolution visi... | The work advances over prior open-source VLMs by integrating MoE for scalable efficiency, native-resolution vision encoding, and long-context reasonin... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.07079-b31b1b.svg)](https://arxiv.org/abs/2504.07079) | SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills | SkillWeaver introduces a skill-centric framework for autonomous web agents to self-improve through skill discovery, practice, and API synthesis. It em... | Unlike prior work, SkillWeaver focuses on autonomous skill abstraction and composition as reusable APIs, enabling agents to iteratively enhance capabi... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.13805-b31b1b.svg)](https://arxiv.org/abs/2504.13805) | LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration Benchmark | The paper introduces LearnGUI, the first comprehensive dataset for demonstration-based learning in mobile GUI agents, and LearnAct, a multi-agent fram... | Unlike prior static datasets and dynamic benchmarks, LearnGUI provides high-quality human demonstrations for both offline and online tasks, while Lear... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.09848-b31b1b.svg)](https://arxiv.org/abs/2504.09848) | A Survey of Large Language Model-Powered Spatial Intelligence Across Scales: Adv... | The paper provides a comprehensive survey of spatial intelligence across disciplines and scales, integrating human spatial cognition research with lar... | This work differs from related work by systematically bridging human spatial cognition research with LLM advancements, offering a multi-scale framewor... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.10127-b31b1b.svg)](https://arxiv.org/abs/2504.10127) | Breaking the Data Barrier -- Building GUI Agents Through Task Generalization | The paper introduces a method to enhance GUI agents through task generalization by leveraging non-GUI tasks like mathematical reasoning and textual re... | This work differs from related work by demonstrating that non-GUI tasks (e.g., mathematical reasoning) significantly improve GUI agent performance thr... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.13936-b31b1b.svg)](https://arxiv.org/abs/2504.13936) | ViMo: A Generative Visual GUI World Model for App Agents | ViMo introduces a generative visual GUI world model for App agents, addressing long-horizon planning challenges by predicting future GUI observations... | ViMo differs from prior work by introducing a visual modality for GUI prediction (vs. text-based descriptions), employing a novel STR architecture for... |
| [![arXiv](https://img.shields.io/badge/arXiv-2505.19897-b31b1b.svg)](https://arxiv.org/abs/2505.19897) | ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Wo... | Introduces ScienceBoard, a realistic multi-domain environment with dynamic scientific workflows and a benchmark of 169 validated tasks for evaluating... | ScienceBoard differs from prior work by creating a domain-specific, rigorously validated benchmark and environment tailored for scientific workflows,... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.19838-b31b1b.svg)](https://arxiv.org/abs/2504.19838) | LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects | The paper provides a comprehensive survey of LLM-driven phone GUI agents, addressing challenges like limited generality, high maintenance, and weak in... | This work differs from related work by offering a structured taxonomy of LLM-powered GUI agents, proposing standardized benchmarks and evaluation prot... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.14603-b31b1b.svg)](https://arxiv.org/abs/2504.14603) | UFO2: The Desktop AgentOS | UFO2 introduces a system-level AgentOS for Windows desktop automation, combining deep OS integration, hybrid GUI detection, speculative multi-action p... | Unlike prior CUAs focused on visual grounding or language reasoning, UFO2 introduces a modular AgentOS architecture with deep OS and application integ... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.02357-b31b1b.svg)](https://arxiv.org/abs/2504.02357) | ReuseDroid: A VLM-empowered Android UI Test Migrator Boosted by Active Feedback | This paper introduces REUSEDROID, a multiagent framework leveraging Large Vision-Language Models (VLMs) for GUI test migration. It addresses limitatio... | REUSEDROID differs by employing VLMs for multi-stage test migration, focusing on core logic alignment across apps with differing operational logic, an... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.11675-b31b1b.svg)](https://arxiv.org/abs/2504.11675) | VLM-Fuzz: Vision Language Model Assisted Recursive Depth-first Search Exploratio... | VLM-Fuzz introduces a novel approach combining Vision Language Models (VLMs) with heuristic-based depth-first search (DFS) for Android UI testing. It... | VLM-Fuzz differs from related work by integrating VLMs for visual reasoning with DFS exploration, addressing limitations in state tracking and widget... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.07981-b31b1b.svg)](https://arxiv.org/abs/2504.07981) | ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use | This paper introduces ScreenSpot-Pro, a benchmark for evaluating GUI grounding in high-resolution professional environments, and proposes ScreenSeekeR... | The work differs from related work by focusing on high-resolution professional applications, introducing a rigorous benchmark (ScreenSpot-Pro) with ex... |
| [![arXiv](https://img.shields.io/badge/arXiv-2505.07062-b31b1b.svg)](https://arxiv.org/abs/2505.07062) | Seed1.5-VL Technical Report | Seed1.5-VL introduces a vision-language foundation model with a 532M vision encoder and 20B MoE LLM, achieving state-of-the-art performance on 38/60 b... | Seed1.5-VL differs through its compact yet powerful architecture combining a vision encoder with MoE LLM, enabling superior performance in visual reas... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.08942-b31b1b.svg)](https://arxiv.org/abs/2504.08942) | AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories | The paper introduces AgentRewardBench, the first benchmark for evaluating LLM-based automatic evaluations of web agent trajectories. It highlights the... | This work differs from related work by creating the first benchmark (AgentRewardBench) to systematically evaluate LLM judges for web agent trajectory... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.10445-b31b1b.svg)](https://arxiv.org/abs/2504.10445) | RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users | Introduces RealWebAssist, a benchmark for evaluating long-horizon web assistance with real-world users through sequential instruction-following. Focus... | RealWebAssist differs from prior benchmarks by using real-world user instructions collected during natural web interactions, supporting sequential tas... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.11257-b31b1b.svg)](https://arxiv.org/abs/2504.11257) | UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis | The paper addresses GUI instruction grounding challenges through a large-scale synthetic data synthesis pipeline (UI-E2I-Synth) and introduces a new b... | This work differs from related work by proposing a large-scale synthetic data generation approach using GPT-4o instead of manual annotation, and by in... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.14239-b31b1b.svg)](https://arxiv.org/abs/2504.14239) | InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative... | The paper introduces InfiGUI-R1, a multimodal GUI agent that transitions from reactive acting to deliberative reasoning through the Actor2Reasoner fra... | This work differs from related work by introducing a two-stage training framework (Actor2Reasoner) that systematically evolves GUI agents from reactiv... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.11543-b31b1b.svg)](https://arxiv.org/abs/2504.11543) | REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websit... | Introduces REAL, a benchmark and framework for evaluating autonomous agents on deterministic simulations of real websites, featuring high-fidelity web... | Differ from related work by providing a deterministic, real-world website simulation benchmark with task-specific evaluations, enabling safe, reproduc... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.16419-b31b1b.svg)](https://arxiv.org/abs/2504.16419) | PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels | PixelWeb introduces a novel automated annotation approach for GUI datasets, combining visual feature extraction and DOM structure analysis to address... | Unlike prior work, PixelWeb introduces pixel-level precise annotations through channel derivation and layer analysis modules, resolving issues like oc... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.09352-b31b1b.svg)](https://arxiv.org/abs/2504.09352) | Explorer: Robust Collection of Interactable GUI Elements | The paper introduces Explorer, a system for robustly collecting and understanding interactable GUI elements (buttons, text fields) through live applic... | Explorer differs from prior work by using live app data for training, avoiding platform-specific APIs, and integrating auto-labeling and state mapping... |
| [![arXiv](https://img.shields.io/badge/arXiv-2506.03143-b31b1b.svg)](https://arxiv.org/abs/2506.03143) | GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents | The paper introduces GUI-Actor, a coordinate-free visual grounding framework for GUI agents that addresses limitations in existing coordinate-based me... | GUI-Actor differs from related work by eliminating coordinate generation in favor of a coordinate-free approach, leveraging an <ACTOR> token for direc... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.20464-b31b1b.svg)](https://arxiv.org/abs/2504.20464) | A Survey on GUI Agents with Foundation Models Enhanced by Reinforcement Learning | The paper provides a structured survey of GUI agents enhanced by reinforcement learning, formalizing tasks as Markov Decision Processes, reviewing mod... | This work differs from related work by offering a comprehensive taxonomy of (M)LLM-based GUI agent architectures and training methods, emphasizing the... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.13865-b31b1b.svg)](https://arxiv.org/abs/2504.13865) | A Survey on (M)LLM-Based GUI Agents | The paper provides a comprehensive survey of LLM-based GUI Agents, identifying four fundamental components (perception, exploration, planning, interac... | This work systematically synthesizes the state-of-the-art in GUI Agent research, offering a structured analysis of architectural components and evalua... |
| [![arXiv](https://img.shields.io/badge/arXiv-2505.00684-b31b1b.svg)](https://arxiv.org/abs/2505.00684) | Visual Test-time Scaling for GUI Agent Grounding | The paper introduces RegionFocus, a visual test-time scaling approach for GUI agents that dynamically zooms into relevant interface regions to reduce... | Unlike prior text-based or full-screen vision-based approaches, RegionFocus introduces a novel visual test-time scaling framework that separates plann... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.06821-b31b1b.svg)](https://arxiv.org/abs/2504.06821) | Inducing Programmatic Skills for Agentic Tasks | The paper introduces Agent Skill Induction (ASI), a framework enabling agents to dynamically induce, verify, and utilize programmatic skills for web n... | Unlike prior text-based skill representations, ASI employs executable programs for skills, offering verifiability and composability. This enables high... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.19298-b31b1b.svg)](https://arxiv.org/abs/2504.19298) | AndroidGen: Building an Android Language Agent under Data Scarcity | The paper introduces AndroidGen, a framework for enhancing LLM-based Android agents under data scarcity. It addresses challenges in data collection fo... | AndroidGen differs from related work by focusing on data scarcity through automated trajectory collection and training of open-source LLMs, incorporat... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.18575-b31b1b.svg)](https://arxiv.org/abs/2504.18575) | WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks | The paper introduces WASP, a benchmark for evaluating web agent security against prompt injection attacks, highlighting the vulnerability of AI models... | Unlike prior work that simplifies threats or focuses on single-step tasks, WASP provides a realistic, end-to-end benchmark for assessing web agent sec... |
| [![arXiv](https://img.shields.io/badge/arXiv-2505.00416-b31b1b.svg)](https://arxiv.org/abs/2505.00416) | ScaleTrack: Scaling and back-tracking Automated GUI Agents | ScaleTrack addresses limitations in GUI agent training by introducing a framework that combines scaled GUI grounding with backtracking planning. It im... | ScaleTrack differs from related work by integrating scaled GUI grounding with backtracking planning, addressing data insufficiency and ignoring histor... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.16073-b31b1b.svg)](https://arxiv.org/abs/2504.16073) | Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation | The paper introduces a process reward model to guide VLM agents during GUI navigation, enabling step-by-step action optimization. It addresses limitat... | This work differs from related work by introducing process-level reward guidance during inference, enabling real-time action optimization rather than... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.04716-b31b1b.svg)](https://arxiv.org/abs/2504.04716) | On the Robustness of GUI Grounding Models Against Image Attacks | The paper systematically evaluates the robustness of GUI grounding models against natural noise, untargeted adversarial attacks, and targeted adversar... | This work focuses on GUI grounding robustness, addressing unique challenges like low-resolution inputs and adversarial attacks, which are underexplore... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.11281-b31b1b.svg)](https://arxiv.org/abs/2504.11281) | The Obvious Invisible Threat: LLM-Powered GUI Agents' Vulnerability to Fine-Prin... | The paper identifies six types of attacks exploiting GUI agents' vulnerabilities to fine-print injections, introduces Fine-Print Injection as a novel... | This work differs from related work by focusing on contextually embedded threats and introducing Fine-Print Injection, a novel attack exploiting agent... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.17934-b31b1b.svg)](https://arxiv.org/abs/2504.17934) | Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Age... | The paper identifies three key risks of LLM-powered GUI agents (privacy, security, and human oversight limitations), highlights gaps in existing evalu... | This work differs from related research by focusing on human-centered evaluation frameworks for GUI agents rather than technical advancements in model... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.15434-b31b1b.svg)](https://arxiv.org/abs/2504.15434) | AGI Is Coming... Right After AI Learns to Play Wordle | The paper evaluates OpenAI's Computer-User Agent (CUA) on the Wordle game, highlighting challenges in color recognition and multi-step task reasoning.... | This work differs from related work by focusing on specific limitations of existing agents (e.g., color processing in Wordle) and advocating for targe... |
| [![arXiv](https://img.shields.io/badge/arXiv-2505.15277-b31b1b.svg)](https://arxiv.org/abs/2505.15277) | Web-Shepherd: Advancing PRMs for Reinforcing Web Agents | Introduces Web-Shepherd, the first process reward model (PRM) for web navigation, along with WebPRM Collection (a large-scale step-level preference da... | Differently from prior work relying on MLLMs as evaluators, this work proposes a specialized PRM (Web-Shepherd) with a curated dataset and benchmark,... |
| [![arXiv](https://img.shields.io/badge/arXiv-2504.00983-b31b1b.svg)](https://arxiv.org/abs/2504.00983) | WorldScore: A Unified Evaluation Benchmark for World Generation | The paper introduces WorldScore, a unified benchmark for world generation that addresses limitations in existing benchmarks by enabling multi-scene ge... | WorldScore differs from related work by providing the first unified benchmark for multi-scene world generation, incorporating camera trajectory specif... |
| [![arXiv](https://img.shields.io/badge/arXiv-2505.11015-b31b1b.svg)](https://arxiv.org/abs/2505.11015) | WildDoc: How Far Are We from Achieving Comprehensive and Robust Document Underst... | Introduces WildDoc, a new benchmark for real-world document understanding with diverse real-world conditions, highlighting the performance gap between... | WildDoc addresses the gap in existing benchmarks by providing a real-world document understanding benchmark with manually captured images under varied... |

