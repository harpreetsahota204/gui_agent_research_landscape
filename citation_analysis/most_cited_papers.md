# Top 20 Most Cited Papers

|   Rank | ArXiv                                                                                                  | Title                                                                                                |   Year |   Citations |   PageRank | Platforms                      | Summary                                                                                                                                                  | Contributions                                                                                                                                            |
|--------|--------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|--------|-------------|------------|--------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|
|      1 | [![arXiv](https://img.shields.io/badge/arXiv-2303.08774-b31b1b.svg)](https://arxiv.org/abs/2303.08774) | GPT-4 Technical Report                                                                               |   2023 |         726 |   0.001044 | iOS, Mobile                    | GPT-4 is a large-scale multimodal model with human-level performance on professional and academic benchmarks, improved post-training alignment for...    | Differs from prior work by achieving human-level performance on complex benchmarks (e.g., simulated bar exams), demonstrating multimodal capabilities... |
|      2 | [![arXiv](https://img.shields.io/badge/arXiv-2308.12966-b31b1b.svg)](https://arxiv.org/abs/2308.12966) | Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond |   2023 |         374 |   0.001044 | None                           | The paper introduces Qwen-VL, a series of vision-language models with enhanced visual perception capabilities through a visual receptor, 3-stage...      | The work differs from related work by integrating a meticulously designed 3-stage training pipeline, visual receptor, and input-output interface,...     |
|      3 | [![arXiv](https://img.shields.io/badge/arXiv-2112.09332-b31b1b.svg)](https://arxiv.org/abs/2112.09332) | WebGPT: Browser-assisted question-answering with human feedback                                      |   2021 |         204 |   0.001044 | None                           | The paper introduces a browser-assisted question-answering system that integrates web browsing with GPT-3, utilizing imitation learning and human...     | This work differs from related work by explicitly combining web-browsing capabilities with large language models, leveraging human feedback for...       |
|      4 | [![arXiv](https://img.shields.io/badge/arXiv-2305.16291-b31b1b.svg)](https://arxiv.org/abs/2305.16291) | Voyager: An Open-Ended Embodied Agent with Large Language Models                                     |   2023 |         165 |   0.002929 | iOS, Mobile                    | Voyager introduces an LLM-powered embodied lifelong learning agent for open-ended environments like Minecraft, featuring an automatic...                 | Unlike prior works that rely on fixed curricula or require human interaction, Voyager enables open-ended exploration through a bottom-up...              |
|      5 | [![arXiv](https://img.shields.io/badge/arXiv-2307.13854-b31b1b.svg)](https://arxiv.org/abs/2307.13854) | WebArena: A Realistic Web Environment for Building Autonomous Agents                                 |   2023 |         124 |   0.001549 | Mobile, iOS                    | The paper introduces WebArena, a highly realistic and reproducible web environment for autonomous agents, featuring functional websites from four...     | WebArena differs from prior work by providing a realistic web environment with dynamic, functional websites and a benchmark focused on functional...     |
|      6 | [![arXiv](https://img.shields.io/badge/arXiv-2303.11366-b31b1b.svg)](https://arxiv.org/abs/2303.11366) | Reflexion: Language Agents with Verbal Reinforcement Learning                                        |   2023 |         121 |   0.001118 | Mobile, Web                    | Reflexion introduces a novel framework for language agents that uses verbal reinforcement learning through self-reflective feedback. It leverages...     | Reflexion differentiates from related work by using verbal feedback as a 'semantic gradient' instead of traditional reinforcement learning, enabling...  |
|      7 | [![arXiv](https://img.shields.io/badge/arXiv-2310.09478-b31b1b.svg)](https://arxiv.org/abs/2310.09478) | MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning      |   2023 |         117 |   0.001044 | None                           | The paper introduces MiniGPT-v2, a unified interface leveraging large language models for vision-language tasks. Key contributions include...            | Unlike prior work, MiniGPT-v2 introduces task-specific identifiers during training to enhance task distinction and learning efficiency, combined with... |
|      8 | [![arXiv](https://img.shields.io/badge/arXiv-2310.11441-b31b1b.svg)](https://arxiv.org/abs/2310.11441) | Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V                             |   2023 |          99 |   0.001044 | None                           | The paper introduces Set-of-Mark (SoM), a novel visual prompting method that enhances visual grounding capabilities of large multimodal models like...   | This work differs from related work by focusing on prompt engineering rather than model architecture or training methods, enabling zero-shot visual...   |
|      9 | [![arXiv](https://img.shields.io/badge/arXiv-2312.08914-b31b1b.svg)](https://arxiv.org/abs/2312.08914) | CogAgent: A Visual Language Model for GUI Agents                                                     |   2023 |          99 |   0.001802 | Android, iOS                   | CogAgent introduces a specialized visual language model (VLM) for GUI agents, achieving state-of-the-art performance on VQA benchmarks and GUI...        | CogAgent differs by directly processing GUI screenshots (not HTML/OCR) with a high-resolution VLM architecture, enabling human-level GUI...              |
|     10 | [![arXiv](https://img.shields.io/badge/arXiv-2307.15818-b31b1b.svg)](https://arxiv.org/abs/2307.15818) | RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control                        |   2023 |          93 |   0.001044 | None                           | The paper introduces RT-2, a vision-language-action (VLA) model that integrates large-scale web data with robotic control through co-fine-tuning. It...  | This work differs by co-fine-tuning vision-language models on both robotic trajectory data and internet-scale vision-language tasks, treating actions... |
|     11 | [![arXiv](https://img.shields.io/badge/arXiv-2404.16821-b31b1b.svg)](https://arxiv.org/abs/2404.16821) | How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites    |   2024 |          88 |   0.004198 | Mobile, iOS                    | The paper introduces three key improvements: (1) a strong vision encoder with continuous learning for better visual understanding, (2) dynamic...        | InternVL 1.5 addresses gaps in parameter scale, image resolution, and multilingual capability compared to proprietary models by integrating a...         |
|     12 | [![arXiv](https://img.shields.io/badge/arXiv-2401.10935-b31b1b.svg)](https://arxiv.org/abs/2401.10935) | SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents                                    |   2024 |          75 |   0.001999 | Android, and Android platforms | The paper introduces SeeClick, a visual GUI agent that automates tasks using screenshots instead of structured data, addresses the GUI grounding...      | SeeClick differs from prior work by eliminating reliance on structured text (e.g., HTML) and GUI metadata, leveraging LVLMs for direct...                |
|     13 | [![arXiv](https://img.shields.io/badge/arXiv-2311.07562-b31b1b.svg)](https://arxiv.org/abs/2311.07562) | GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation                |   2023 |          67 |   0.001044 | None                           | The paper introduces MM-Navigator, a GPT-4V-based agent for zero-shot smartphone GUI navigation, demonstrating high accuracy in action description...    | This work differs from related work by leveraging GPT-4V's advanced screen interpretation and action reasoning capabilities for zero-shot GUI...         |
|     14 | [![arXiv](https://img.shields.io/badge/arXiv-2307.12856-b31b1b.svg)](https://arxiv.org/abs/2307.12856) | A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis               |   2023 |          66 |   0.002438 | Mobile, Web                    | The paper introduces WebAgent, an LLM-driven autonomous agent for real-world web automation that addresses open-domainness, long-context HTML...         | Unlike prior works relying on simulated environments or single LLMs, WebAgent combines HTML-T5 (specialized for HTML with novel attention mechanisms)... |
|     15 | [![arXiv](https://img.shields.io/badge/arXiv-2401.13649-b31b1b.svg)](https://arxiv.org/abs/2401.13649) | VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks                           |   2024 |          66 |   0.002379 | Mobile, Web                    | Introduces VisualWebArena, a benchmark for evaluating multimodal agents on visually grounded web tasks, emphasizing integration of visual and textual... | VisualWebArena fills the gap in evaluating multimodal agents on visually grounded tasks, offering a comprehensive benchmark with real-world tasks and... |
|     16 | [![arXiv](https://img.shields.io/badge/arXiv-2404.07972-b31b1b.svg)](https://arxiv.org/abs/2404.07972) | OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments           |   2024 |          63 |   0.001044 | Linux, Windows                 | The paper introduces OSWorld, a real computer environment for evaluating multimodal agents in open-ended tasks across multiple operating systems. It...  | OSWorld differs from prior work by providing a scalable, real-world interactive environment and benchmark that captures the diversity and complexity...  |
|     17 | [![arXiv](https://img.shields.io/badge/arXiv-2401.16158-b31b1b.svg)](https://arxiv.org/abs/2401.16158) | Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception                      |   2024 |          62 |   0.001095 | Mobile, iOS                    | The paper introduces Mobile-Agent, a multi-modal agent that uses visual perception to operate mobile apps without relying on XML metadata. It...         | Mobile-Agent differs from prior work by employing a vision-centric approach without requiring XML or system metadata, introducing Mobile-Eval as a...    |
|     18 | [![arXiv](https://img.shields.io/badge/arXiv-2306.06070-b31b1b.svg)](https://arxiv.org/abs/2306.06070) | Mind2Web: Towards a Generalist Agent for the Web                                                     |   2023 |          60 |   0.001408 | Mobile, macOS                  | The paper introduces Mind2Web, the first dataset for generalist web agents, emphasizing real-world websites, diverse domains/tasks, and user...          | Differs from prior work by using real-world websites instead of simulations, providing diverse tasks across 31 domains, and integrating LLMs with a...   |
|     19 | [![arXiv](https://img.shields.io/badge/arXiv-2403.05525-b31b1b.svg)](https://arxiv.org/abs/2403.05525) | DeepSeek-VL: Towards Real-World Vision-Language Understanding                                        |   2024 |          58 |   0.002145 | Mobile, iOS                    | DeepSeek-VL introduces a hybrid vision encoder for efficient high-resolution image processing, a comprehensive real-world dataset with diverse...        | DeepSeek-VL differentiates from prior work by combining vision and language pretraining with a hybrid vision encoder, addressing limitations of...       |
|     20 | [![arXiv](https://img.shields.io/badge/arXiv-2401.13919-b31b1b.svg)](https://arxiv.org/abs/2401.13919) | WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models                            |   2024 |          56 |   0.001471 | Mobile, iOS                    | This paper introduces WebVoyager, a multimodal web agent leveraging large multimodal models (LMMs) to interact with real-world websites end-to-end....   | WebVoyager differs from prior work by enabling real-world web navigation through multimodal inputs (screenshots and text), utilizing a novel...          |

---
*Generated on: 2025-08-15 09:32:10*
*Total entries: 20*
