# Top 20 Papers That Introduce Datasets

|   Rank | ArXiv                                                                                                  | Title                                                                                                     |   Year |   Citations | Summary                                                                                                                                                  | Contributions                                                                                                                                            |
|--------|--------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|--------|-------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|
|      1 | [![arXiv](https://img.shields.io/badge/arXiv-2311.07562-b31b1b.svg)](https://arxiv.org/abs/2311.07562) | GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation                     |   2023 |          67 | The paper introduces MM-Navigator, a GPT-4V-based agent for zero-shot smartphone GUI navigation, demonstrating high accuracy in action description...    | This work differs from related work by leveraging GPT-4V's advanced screen interpretation and action reasoning capabilities for zero-shot GUI...         |
|      2 | [![arXiv](https://img.shields.io/badge/arXiv-2404.07972-b31b1b.svg)](https://arxiv.org/abs/2404.07972) | OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments                |   2024 |          63 | The paper introduces OSWorld, a real computer environment for evaluating multimodal agents in open-ended tasks across multiple operating systems. It...  | OSWorld differs from prior work by providing a scalable, real-world interactive environment and benchmark that captures the diversity and complexity...  |
|      3 | [![arXiv](https://img.shields.io/badge/arXiv-2306.06070-b31b1b.svg)](https://arxiv.org/abs/2306.06070) | Mind2Web: Towards a Generalist Agent for the Web                                                          |   2023 |          60 | The paper introduces Mind2Web, the first dataset for generalist web agents, emphasizing real-world websites, diverse domains/tasks, and user...          | Differs from prior work by using real-world websites instead of simulations, providing diverse tasks across 31 domains, and integrating LLMs with a...   |
|      4 | [![arXiv](https://img.shields.io/badge/arXiv-2401.13919-b31b1b.svg)](https://arxiv.org/abs/2401.13919) | WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models                                 |   2024 |          56 | This paper introduces WebVoyager, a multimodal web agent leveraging large multimodal models (LMMs) to interact with real-world websites end-to-end....   | WebVoyager differs from prior work by enabling real-world web navigation through multimodal inputs (screenshots and text), utilizing a novel...          |
|      5 | [![arXiv](https://img.shields.io/badge/arXiv-2405.14573-b31b1b.svg)](https://arxiv.org/abs/2405.14573) | AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents                                    |   2024 |          53 | The paper introduces AndroidWorld, a dynamic benchmarking environment for autonomous agents on Android, featuring 116 programmatic tasks across 20...    | AndroidWorld differs from related work by providing the first comprehensive mobile benchmark with dynamically generated, parameterized tasks across...   |
|      6 | [![arXiv](https://img.shields.io/badge/arXiv-1710.99999-b31b1b.svg)](https://arxiv.org/abs/1710.99999) | Rico: A Mobile App Dataset for Building Data-Driven Design Applications                                   |   2017 |          47 | The paper introduces Rico, the largest mobile app design dataset, enabling data-driven applications in design search, UI layout generation, code...      | Unlike static datasets like ImageNet, Rico dynamically captures UI interactions and states through runtime mining, supports deep learning with...        |
|      7 | [![arXiv](https://img.shields.io/badge/arXiv-2307.10088-b31b1b.svg)](https://arxiv.org/abs/2307.10088) | Android in the Wild: A Large-Scale Dataset for Android Device Control                                     |   2023 |          40 | The paper introduces a large-scale dataset (AITW) for device-control research, emphasizing visual and language interaction, multi-step tasks, and...     | This work differs from related work by providing an order-of-magnitude larger dataset with diverse Android versions, device types, and complex...        |
|      8 | [![arXiv](https://img.shields.io/badge/arXiv-2305.11854-b31b1b.svg)](https://arxiv.org/abs/2305.11854) | Multimodal Web Navigation with Instruction-Finetuned Foundation Models                                    |   2023 |          37 | This work introduces WebGUM, a multimodal agent for web navigation that combines vision-language foundation models with instruction-finetuning. It...    | Unlike prior works reliant on domain-specific architectures and online RL, this work leverages instruction-finetuned vision-language foundation...       |
|      9 | [![arXiv](https://img.shields.io/badge/arXiv-2404.05719-b31b1b.svg)](https://arxiv.org/abs/2404.05719) | Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs                                          |   2024 |          36 | Ferret-UI introduces a specialized multimodal large language model (MLLM) for mobile UI understanding, addressing limitations in existing models...      | This work differs from related work by explicitly addressing UI-specific challenges (e.g., elongated aspect ratios, small objects) through a...          |
|     10 | [![arXiv](https://img.shields.io/badge/arXiv-2402.05930-b31b1b.svg)](https://arxiv.org/abs/2402.05930) | WebLINX: Real-World Website Navigation with Multi-Turn Dialogue                                           |   2024 |          36 | The paper introduces WEBLINX, a large-scale benchmark for conversational web navigation, and proposes a retrieval-inspired model to address the...       | This work differs from related work by introducing a novel benchmark (WEBLINX) and a retrieval-inspired architecture tailored for web navigation...      |
|     11 | [![arXiv](https://img.shields.io/badge/arXiv-2403.02713-b31b1b.svg)](https://arxiv.org/abs/2403.02713) | Android in the Zoo: Chain-of-Action-Thought for GUI Agents                                                |   2024 |          36 | This work introduces Chain-of-Action-Thought (CoAT) for GUI agents, emphasizing semantic reasoning through screen context, action thinking, targets,...  | Unlike prior works focusing solely on coordinate-based actions or separating element recognition from action inference, CoAT integrates semantic...      |
|     12 | [![arXiv](https://img.shields.io/badge/arXiv-2005.03776-b31b1b.svg)](https://arxiv.org/abs/2005.03776) | Mapping Natural Language Instructions to Mobile UI Action Sequences                                       |   2020 |          33 | The paper introduces a novel problem of grounding natural language instructions to mobile UI actions, develops three new datasets including...           | This work differs from related work by focusing specifically on mobile UI interaction, creating task-specific datasets with grounded action...           |
|     13 | [![arXiv](https://img.shields.io/badge/arXiv-2402.17553-b31b1b.svg)](https://arxiv.org/abs/2402.17553) | OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web |   2024 |          30 | Introduces OmniACT, the first dataset and benchmark for evaluating autonomous agents' ability to generate executable scripts for both desktop and web... | OmniACT differs from prior work by combining desktop and web tasks, requiring executable script generation rather than just action prediction, and...    |
|     14 | [![arXiv](https://img.shields.io/badge/arXiv-2307.08581-b31b1b.svg)](https://arxiv.org/abs/2307.08581) | BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs                                                    |   2023 |          28 | BuboGPT enhances multi-modal LLMs with fine-grained visual grounding, enabling precise cross-modal interactions between vision, audio, and language....  | Unlike prior works that rely on coarse-grained mappings, BuboGPT achieves fine-grained visual grounding by integrating a SAM-based module and a novel... |
|     15 | [![arXiv](https://img.shields.io/badge/arXiv-2410.23218-b31b1b.svg)](https://arxiv.org/abs/2410.23218) | OS-ATLAS: A Foundation Action Model for Generalist GUI Agents                                             |   2024 |          27 | This paper introduces OS-Atlas, a foundational GUI action model designed to enhance GUI grounding and Out-Of-Distribution (OOD) generalization for...    | OS-Atlas differs from prior work by providing the first open-source foundation model specifically tailored for GUI agents, combining a large-scale...    |
|     16 | [![arXiv](https://img.shields.io/badge/arXiv-2406.03679-b31b1b.svg)](https://arxiv.org/abs/2406.03679) | On the Effects of Data Scale on UI Control Agents                                                         |   2024 |          27 | The paper investigates the scalability of fine-tuning LLMs for UI control agents, introduces the AndroidControl dataset with high/low-level...           | The work introduces AndroidControl, the most diverse UI control dataset with both high/low-level instructions, and provides systematic analysis of...    |
|     17 | [![arXiv](https://img.shields.io/badge/arXiv-2408.00203-b31b1b.svg)](https://arxiv.org/abs/2408.00203) | OmniParser for Pure Vision Based GUI Agent                                                                |   2024 |          26 | This paper introduces OmniParser, a pure vision-based approach for parsing UI screenshots into structured elements to enhance GUI agents. It...          | OmniParser differs from prior work by providing a pure vision-based solution without requiring HTML or view hierarchy dependencies, curating...          |
|     18 | [![arXiv](https://img.shields.io/badge/arXiv-2402.04615-b31b1b.svg)](https://arxiv.org/abs/2402.04615) | ScreenAI: A Vision-Language Model for UI and Infographics Understanding                                   |   2024 |          26 | ScreenAI introduces a vision-language model specialized in understanding UIs and infographics, combining PaLI architecture with Pix2Struct's flexible... | ScreenAI differs from related work by unifying UI and infographic understanding through a novel visual language model architecture, leveraging a...      |
|     19 | [![arXiv](https://img.shields.io/badge/arXiv-2406.08451-b31b1b.svg)](https://arxiv.org/abs/2406.08451) | GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices                       |   2024 |          26 | This work introduces GUI Odyssey, a comprehensive dataset for cross-app GUI navigation, and develops OdysseyAgent, a multimodal navigation agent. It...  | This work differs from related work by focusing on cross-app navigation tasks, which previous datasets and models primarily ignored. It introduces...    |
|     20 | [![arXiv](https://img.shields.io/badge/arXiv-2406.11317-b31b1b.svg)](https://arxiv.org/abs/2406.11317) | GUICourse: From General Vision Language Models to Versatile GUI Agents                                    |   2024 |          26 | The paper introduces GUICourse, a comprehensive suite of datasets (GUIEnv, GUIAct, GUIChat) to enhance Vision Language Models (VLMs) for GUI agent...    | This work differs from related work by explicitly addressing VLM limitations in OCR, grounding, and GUI-specific knowledge through purpose-built...      |


### Temporal Evolution and Thematic Waves

#### **Wave 1: Foundation Dataset Era (2017-2020)**
**Rico (2017)** and **Mapping Natural Language Instructions to Mobile UI Action Sequences (2020)** established the foundational paradigm:
- **Rico's Innovation**: Unlike static datasets like ImageNet, Rico dynamically captured UI interactions and states through runtime mining, employing content-agnostic similarity heuristics and autoencoder-generated embeddings for scalable real-world UI analysis without source code access.
- **Mobile UI Grounding**: Created the first comprehensive framework for mapping natural language to mobile actions, introducing PIXELHELP dataset and dual-Transformer architecture for contextual UI object representation.

#### **Wave 2: Multimodal Integration Era (2023)**
**GPT-4V in Wonderland (2023)**, **Mind2Web (2023)**, **Android in the Wild (2023)**, **Multimodal Web Navigation (2023)**, and **BuboGPT (2023)** ushered in the multimodal revolution:
- **GPT-4V in Wonderland**: Pioneered zero-shot smartphone GUI navigation using large multimodal models, establishing benchmarks for mobile interaction without task-specific training.
- **Mind2Web**: Created the first large-scale dataset for generalist web agents, capturing diverse user interactions on real-world websites with comprehensive task coverage.
- **Android in the Wild**: Provided large-scale Android device control data, enabling real-world mobile automation research.

#### **Wave 3: Comprehensive Benchmarking Era (2024)**
**OSWorld**, **WebVoyager**, **AndroidWorld**, **WebLINX**, **Android in the Zoo**, **OmniACT**, **OS-ATLAS**, **OmniParser**, **ScreenAI**, **GUI Odyssey**, and **GUICourse** represent the maturation of dataset-driven research:
- **OSWorld**: Differs from prior work by providing a scalable, real-world interactive environment across multiple operating systems (Ubuntu, Windows, macOS), enabling reliable and reproducible evaluation of agents' GUI grounding and operational knowledge.
- **AndroidWorld**: Introduced dynamic benchmarking with real Android environments, moving beyond static evaluation to interactive assessment.

### Key Innovation Patterns

#### **1. Real-World Complexity Capture**
Modern dataset papers consistently emphasize capturing real-world complexity:
- **OSWorld**: 369 tasks involving real applications, file I/O, and cross-application workflows
- **AndroidWorld**: Dynamic environments with real Android devices
- **WebLINX**: Real-world website navigation with multi-turn dialogue
- **GUI Odyssey**: Cross-app navigation scenarios on mobile devices

#### **2. Multimodal Integration**
Dataset papers progressively incorporated multiple modalities:
- **Visual + Language**: All papers integrate visual understanding with natural language instructions
- **Action Sequences**: Temporal action modeling becomes standard
- **Context Awareness**: Screen state, application context, and user intent modeling

#### **3. Evaluation Methodology Innovation**
Advanced evaluation frameworks emerged:
- **Ferret-UI**: Dual-subimage encoding strategy addressing UI-specific challenges (elongated aspect ratios, small objects)
- **OmniACT**: Cross-platform evaluation (desktop and web)
- **ScreenAI**: Specialized infographics and UI understanding metrics

#### **4. Scale and Diversity**
Consistent scaling in dataset size and diversity:
- **Rico**: 66k+ UI screens from 9.3k apps
- **Mind2Web**: 2k+ websites with diverse interaction patterns
- **Android in the Wild**: Large-scale device control scenarios
- **OSWorld**: 369 comprehensive computer tasks

### Methodological Contributions

#### **Data Collection Innovation**
- **Runtime Mining**: Rico's approach to dynamic UI capture without code modification
- **Crowdsourcing**: Systematic human annotation and verification
- **Automated Augmentation**: LLM-based data synthesis and expansion
- **Real-World Deployment**: Moving from simulation to actual device/environment interaction

#### **Evaluation Framework Development**
- **Multi-Dimensional Assessment**: Beyond task completion to process evaluation
- **Cross-Platform Validation**: Ensuring generalization across different environments
- **Temporal Evaluation**: Long-horizon task assessment and error recovery
- **Human-AI Comparison**: Establishing human performance baselines

#### **Benchmark Standardization**
- **Reproducible Protocols**: Standardized evaluation procedures
- **Open-Source Availability**: Accessible datasets and evaluation tools
- **Community Standards**: Establishing common metrics and baselines

### Impact on Current Research

#### **Enabling Modern GUI Agents**
These datasets directly enable current state-of-the-art GUI agents by providing:
- **Training Data**: Large-scale, diverse interaction examples
- **Evaluation Standards**: Consistent benchmarking protocols
- **Real-World Grounding**: Authentic task scenarios and environments

#### **Driving Architectural Innovation**
Dataset characteristics drove architectural developments:
- **Multimodal Architectures**: Vision-language model requirements
- **Temporal Modeling**: Sequence-to-sequence action prediction
- **Context Integration**: Multi-screen, multi-app understanding
- **Error Recovery**: Robust planning and replanning capabilities

#### **Research Direction Influence**
Dataset papers shaped research priorities:
- **Real-World Deployment**: Focus on practical applicability
- **Cross-Platform Generalization**: Universal GUI understanding
- **Long-Horizon Planning**: Complex, multi-step task execution
- **Human-AI Collaboration**: Interactive assistance paradigms

### Unique Contributions Analysis

#### **Platform Specialization**
- **Mobile-Focused**: Rico, Ferret-UI, Android in the Wild, AndroidWorld, GUI Odyssey
- **Web-Centric**: Mind2Web, WebVoyager, WebLINX, Multimodal Web Navigation
- **Cross-Platform**: OSWorld, OmniACT, OS-ATLAS, OmniParser, ScreenAI, GUICourse

#### **Task Complexity Progression**
- **Elementary Tasks**: Icon recognition, text finding, widget listing
- **Intermediate Tasks**: Single-app workflows, form completion
- **Advanced Tasks**: Cross-app coordination, complex planning, error recovery

#### **Evaluation Sophistication**
- **Static Assessment**: Screenshot-based evaluation
- **Dynamic Evaluation**: Real-time interaction assessment
- **Process Evaluation**: Step-by-step action validation
- **Outcome Evaluation**: Task completion verification

### Challenges Addressed

#### **Data Quality and Scale**
- **Annotation Consistency**: Human verification and quality control
- **Coverage Completeness**: Comprehensive task and domain representation
- **Scalability Solutions**: Automated data generation and augmentation

#### **Real-World Applicability**
- **Environment Authenticity**: Real devices, applications, and websites
- **Task Relevance**: User-driven scenario selection
- **Performance Gaps**: Highlighting human-AI performance differences

#### **Evaluation Reliability**
- **Reproducibility**: Standardized evaluation environments
- **Metric Validity**: Meaningful performance indicators
- **Generalization Assessment**: Cross-domain and cross-platform validation

### Future Directions Indicated

#### **Emerging Trends**
1. **Dynamic Adaptation**: Real-time environment changes and updates
2. **Personalization**: User-specific interaction patterns and preferences
3. **Collaborative Intelligence**: Human-AI cooperative task execution
4. **Ethical Considerations**: Privacy, security, and responsible AI deployment

#### **Technical Challenges**
1. **Scalability**: Handling increasing complexity and diversity
2. **Generalization**: Cross-domain and cross-platform robustness
3. **Efficiency**: Real-time performance requirements
4. **Reliability**: Consistent and predictable behavior

### Conclusion

The dataset papers represent more than just data collection effortsâ€”they embody a fundamental transformation in GUI agent research methodology. From Rico's pioneering runtime mining to OSWorld's comprehensive real-world evaluation, these works establish the empirical foundation that enables modern multimodal agents.

Their collective contribution lies not only in providing training data but in defining evaluation standards, driving architectural innovation, and establishing research priorities that continue to shape the field. The progression from simple UI understanding to complex, multi-platform, long-horizon task execution demonstrates how thoughtful dataset design can accelerate an entire research domain.

As GUI agents move toward practical deployment, these foundational datasets remain crucial infrastructure, providing the benchmarks against which progress is measured and the training grounds where future capabilities are developed. The field's current trajectory toward more capable, reliable, and generalizable agents is directly enabled by the comprehensive, high-quality datasets established by these seminal works.

---

*Analysis based on contribution field extracts from the keyword_filtered_enriched dataset, examining the top 20 papers that introduce datasets ranked by citation count.*

*Generated on: 2025-08-15*
*Total entries: 20*
