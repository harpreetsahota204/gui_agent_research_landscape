# Top 20 Frontier Papers (Emerging Influential Work)

|   Rank | ArXiv                                                                                                  | Title                                                                                                      |   Year |   Frontier Score |   Citations |   Age |   Citations/Year |   PageRank | Summary                                                                                                                                                  | Contributions                                                                                                                                            |
|--------|--------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|--------|------------------|-------------|-------|------------------|------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|
|      1 | [![arXiv](https://img.shields.io/badge/arXiv-2401.10935-b31b1b.svg)](https://arxiv.org/abs/2401.10935) | SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents                                          |   2024 |           0.5968 |          75 |     1 |             75   |   0.001999 | The paper introduces SeeClick, a visual GUI agent that automates tasks using screenshots instead of structured data, addresses the GUI grounding...      | SeeClick differs from prior work by eliminating reliance on structured text (e.g., HTML) and GUI metadata, leveraging LVLMs for direct...                |
|      2 | [![arXiv](https://img.shields.io/badge/arXiv-2404.07972-b31b1b.svg)](https://arxiv.org/abs/2404.07972) | OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments                 |   2024 |           0.5119 |          63 |     1 |             63   |   0.001044 | The paper introduces OSWorld, a real computer environment for evaluating multimodal agents in open-ended tasks across multiple operating systems. It...  | OSWorld differs from prior work by providing a scalable, real-world interactive environment and benchmark that captures the diversity and complexity...  |
|      3 | [![arXiv](https://img.shields.io/badge/arXiv-2401.13649-b31b1b.svg)](https://arxiv.org/abs/2401.13649) | VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks                                 |   2024 |           0.4719 |          66 |     1 |             66   |   0.002379 | Introduces VisualWebArena, a benchmark for evaluating multimodal agents on visually grounded web tasks, emphasizing integration of visual and textual... | VisualWebArena fills the gap in evaluating multimodal agents on visually grounded tasks, offering a comprehensive benchmark with real-world tasks and... |
|      4 | [![arXiv](https://img.shields.io/badge/arXiv-2401.16158-b31b1b.svg)](https://arxiv.org/abs/2401.16158) | Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception                            |   2024 |           0.4644 |          62 |     1 |             62   |   0.001095 | The paper introduces Mobile-Agent, a multi-modal agent that uses visual perception to operate mobile apps without relying on XML metadata. It...         | Mobile-Agent differs from prior work by employing a vision-centric approach without requiring XML or system metadata, introducing Mobile-Eval as a...    |
|      5 | [![arXiv](https://img.shields.io/badge/arXiv-2405.14573-b31b1b.svg)](https://arxiv.org/abs/2405.14573) | AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents                                     |   2024 |           0.4519 |          53 |     1 |             53   |   0.003855 | The paper introduces AndroidWorld, a dynamic benchmarking environment for autonomous agents on Android, featuring 116 programmatic tasks across 20...    | AndroidWorld differs from related work by providing the first comprehensive mobile benchmark with dynamically generated, parameterized tasks across...   |
|      6 | [![arXiv](https://img.shields.io/badge/arXiv-2401.13919-b31b1b.svg)](https://arxiv.org/abs/2401.13919) | WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models                                  |   2024 |           0.4145 |          56 |     1 |             56   |   0.001471 | This paper introduces WebVoyager, a multimodal web agent leveraging large multimodal models (LMMs) to interact with real-world websites end-to-end....   | WebVoyager differs from prior work by enabling real-world web navigation through multimodal inputs (screenshots and text), utilizing a novel...          |
|      7 | [![arXiv](https://img.shields.io/badge/arXiv-2402.07456-b31b1b.svg)](https://arxiv.org/abs/2402.07456) | OS-Copilot: Towards Generalist Computer Agents with Self-Improvement                                       |   2024 |           0.3096 |          43 |     1 |             43   |   0.001044 | The paper introduces OS-Copilot, a framework for building generalist computer agents capable of interacting with diverse OS elements. It presents...     | This work differs from related work by focusing on generalist agents for OS interactions, introducing self-improvement mechanisms, and demonstrating...  |
|      8 | [![arXiv](https://img.shields.io/badge/arXiv-2403.02713-b31b1b.svg)](https://arxiv.org/abs/2403.02713) | Android in the Zoo: Chain-of-Action-Thought for GUI Agents                                                 |   2024 |           0.2697 |          36 |     1 |             36   |   0.003898 | This work introduces Chain-of-Action-Thought (CoAT) for GUI agents, emphasizing semantic reasoning through screen context, action thinking, targets,...  | Unlike prior works focusing solely on coordinate-based actions or separating element recognition from action inference, CoAT integrates semantic...      |
|      9 | [![arXiv](https://img.shields.io/badge/arXiv-2402.05930-b31b1b.svg)](https://arxiv.org/abs/2402.05930) | WebLINX: Real-World Website Navigation with Multi-Turn Dialogue                                            |   2024 |           0.2697 |          36 |     1 |             36   |   0.001044 | The paper introduces WEBLINX, a large-scale benchmark for conversational web navigation, and proposes a retrieval-inspired model to address the...       | This work differs from related work by introducing a novel benchmark (WEBLINX) and a retrieval-inspired architecture tailored for web navigation...      |
|     10 | [![arXiv](https://img.shields.io/badge/arXiv-2407.01476-b31b1b.svg)](https://arxiv.org/abs/2407.01476) | Tree Search for Language Model Agents                                                                      |   2024 |           0.2672 |          35 |     1 |             35   |   0.005273 | The paper introduces a tree search algorithm for language model (LM) agents to enhance multi-step planning and exploration in interactive web...         | This work differs from related work by introducing the first tree search algorithm specifically tailored for LM agents in realistic web tasks,...        |
|     11 | [![arXiv](https://img.shields.io/badge/arXiv-2404.05719-b31b1b.svg)](https://arxiv.org/abs/2404.05719) | Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs                                           |   2024 |           0.2672 |          36 |     1 |             36   |   0.001044 | Ferret-UI introduces a specialized multimodal large language model (MLLM) for mobile UI understanding, addressing limitations in existing models...      | This work differs from related work by explicitly addressing UI-specific challenges (e.g., elongated aspect ratios, small objects) through a...          |
|     12 | [![arXiv](https://img.shields.io/badge/arXiv-2406.11896-b31b1b.svg)](https://arxiv.org/abs/2406.11896) | DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning                  |   2024 |           0.2547 |          29 |     1 |             29   |   0.001996 | The paper introduces DigiRL, an autonomous reinforcement learning framework for training device-control agents in real-world GUI environments. Key...    | DigiRL differs from prior work by combining autonomous offline-to-online RL with pre-trained VLMs, addressing real-world stochasticity and...            |
|     13 | [![arXiv](https://img.shields.io/badge/arXiv-2307.13854-b31b1b.svg)](https://arxiv.org/abs/2307.13854) | WebArena: A Realistic Web Environment for Building Autonomous Agents                                       |   2023 |           0.2472 |         124 |     2 |             62   |   0.001549 | The paper introduces WebArena, a highly realistic and reproducible web environment for autonomous agents, featuring functional websites from four...     | WebArena differs from prior work by providing a realistic web environment with dynamic, functional websites and a benchmark focused on functional...     |
|     14 | [![arXiv](https://img.shields.io/badge/arXiv-2406.01014-b31b1b.svg)](https://arxiv.org/abs/2406.01014) | Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration |   2024 |           0.2397 |          29 |     1 |             29   |   0.004148 | The paper introduces a multi-agent architecture (planning, decision, reflection agents) for mobile device operation tasks, addressing navigation...      | This work differs from related work by proposing a specialized multi-agent collaboration framework tailored for mobile device GUI operations,...         |
|     15 | [![arXiv](https://img.shields.io/badge/arXiv-2406.16860-b31b1b.svg)](https://arxiv.org/abs/2406.16860) | Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs                                    |   2024 |           0.2247 |          42 |     1 |             42   |   0.001044 | The paper introduces Cambrian-1, a vision-centric multimodal LLM family that evaluates diverse visual representations through visual instruction...      | Unlike prior work, Cambrian-1 focuses on vision-centric design and integrates spatial awareness via SVA to enhance visual grounding. It introduces...    |
|     16 | [![arXiv](https://img.shields.io/badge/arXiv-2410.23218-b31b1b.svg)](https://arxiv.org/abs/2410.23218) | OS-ATLAS: A Foundation Action Model for Generalist GUI Agents                                              |   2024 |           0.2197 |          27 |     1 |             27   |   0.003113 | This paper introduces OS-Atlas, a foundational GUI action model designed to enhance GUI grounding and Out-Of-Distribution (OOD) generalization for...    | OS-Atlas differs from prior work by providing the first open-source foundation model specifically tailored for GUI agents, combining a large-scale...    |
|     17 | [![arXiv](https://img.shields.io/badge/arXiv-2406.03679-b31b1b.svg)](https://arxiv.org/abs/2406.03679) | On the Effects of Data Scale on UI Control Agents                                                          |   2024 |           0.2172 |          27 |     1 |             27   |   0.001044 | The paper investigates the scalability of fine-tuning LLMs for UI control agents, introduces the AndroidControl dataset with high/low-level...           | The work introduces AndroidControl, the most diverse UI control dataset with both high/low-level instructions, and provides systematic analysis of...    |
|     18 | [![arXiv](https://img.shields.io/badge/arXiv-2406.11317-b31b1b.svg)](https://arxiv.org/abs/2406.11317) | GUICourse: From General Vision Language Models to Versatile GUI Agents                                     |   2024 |           0.2172 |          26 |     1 |             26   |   0.004478 | The paper introduces GUICourse, a comprehensive suite of datasets (GUIEnv, GUIAct, GUIChat) to enhance Vision Language Models (VLMs) for GUI agent...    | This work differs from related work by explicitly addressing VLM limitations in OCR, grounding, and GUI-specific knowledge through purpose-built...      |
|     19 | [![arXiv](https://img.shields.io/badge/arXiv-2310.11441-b31b1b.svg)](https://arxiv.org/abs/2310.11441) | Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V                                   |   2023 |           0.2097 |          99 |     2 |             49.5 |   0.001044 | The paper introduces Set-of-Mark (SoM), a novel visual prompting method that enhances visual grounding capabilities of large multimodal models like...   | This work differs from related work by focusing on prompt engineering rather than model architecture or training methods, enabling zero-shot visual...   |
|     20 | [![arXiv](https://img.shields.io/badge/arXiv-2312.08914-b31b1b.svg)](https://arxiv.org/abs/2312.08914) | CogAgent: A Visual Language Model for GUI Agents                                                           |   2023 |           0.2085 |          99 |     2 |             49.5 |   0.001802 | CogAgent introduces a specialized visual language model (VLM) for GUI agents, achieving state-of-the-art performance on VQA benchmarks and GUI...        | CogAgent differs by directly processing GUI screenshots (not HTML/OCR) with a high-resolution VLM architecture, enabling human-level GUI...              |

---
*Generated on: 2025-08-15 09:32:10*
*Total entries: 20*
