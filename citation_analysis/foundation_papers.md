# Top 20 Foundation Papers (Sustained Influence Over Time)

|   Rank | ArXiv                                                                                                  | Title                                                                                                                       |   Year |   Foundation Score |   Citations |   Age |   PageRank | Summary                                                                                                                                                  | Contributions                                                                                                                                            |
|--------|--------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|--------|--------------------|-------------|-------|------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|
|      1 | [![arXiv](https://img.shields.io/badge/arXiv-2310.17722-b31b1b.svg)](https://arxiv.org/abs/2310.17722) | Large Language Models as Generalizable Policies for Embodied Tasks                                                          |   2023 |             1      |           2 |     2 |   0.003646 | The paper introduces LLaRP, a method that adapts large language models (LLMs) as generalizable policies for embodied visual tasks through...             | LLaRP differs from prior work by using reinforcement learning directly on pre-trained LLMs for embodied tasks without requiring task-specific...         |
|      2 | [![arXiv](https://img.shields.io/badge/arXiv-1610.99999-b31b1b.svg)](https://arxiv.org/abs/1610.99999) | ERICA: Interaction Mining Mobile Apps                                                                                       |   2016 |             0.9077 |           1 |     9 |   0.001044 | The paper introduces ERICA, a scalable system for interaction mining in Android apps that captures both static (UI layouts) and dynamic (user flows)...  | ERICA differs from related work by being the first scalable system for mining dynamic design data from mobile apps without code modifications. It...     |
|      3 | [![arXiv](https://img.shields.io/badge/arXiv-2302.01560-b31b1b.svg)](https://arxiv.org/abs/2302.01560) | Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents    |   2023 |             0.8839 |          56 |     2 |   0.003223 | The paper introduces DEPS, an interactive planning framework leveraging Large Language Models (LLMs) to address long-term reasoning and sub-task...      | DEPS differs from prior work by integrating self-explanation and feedback loops for error correction, introducing a trainable goal selector to...        |
|      4 | [![arXiv](https://img.shields.io/badge/arXiv-2305.11854-b31b1b.svg)](https://arxiv.org/abs/2305.11854) | Multimodal Web Navigation with Instruction-Finetuned Foundation Models                                                      |   2023 |             0.8837 |          37 |     2 |   0.003223 | This work introduces WebGUM, a multimodal agent for web navigation that combines vision-language foundation models with instruction-finetuning. It...    | Unlike prior works reliant on domain-specific architectures and online RL, this work leverages instruction-finetuned vision-language foundation...       |
|      5 | [![arXiv](https://img.shields.io/badge/arXiv-1705.07962-b31b1b.svg)](https://arxiv.org/abs/1705.07962) | pix2code: Generating Code from a Graphical User Interface Screenshot                                                        |   2017 |             0.8591 |           3 |     8 |   0.001044 | The paper introduces an end-to-end deep learning model that generates code from GUI screenshots with high accuracy across multiple platforms. It...      | This work differs from related work by leveraging deep learning for direct code generation from screenshots, whereas prior methods often relied on...    |
|      6 | [![arXiv](https://img.shields.io/badge/arXiv-1710.99999-b31b1b.svg)](https://arxiv.org/abs/1710.99999) | Rico: A Mobile App Dataset for Building Data-Driven Design Applications                                                     |   2017 |             0.8591 |          47 |     8 |   0.001044 | The paper introduces Rico, the largest mobile app design dataset, enabling data-driven applications in design search, UI layout generation, code...      | Unlike static datasets like ImageNet, Rico dynamically captures UI interactions and states through runtime mining, supports deep learning with...        |
|      7 | [![arXiv](https://img.shields.io/badge/arXiv-2312.10763-b31b1b.svg)](https://arxiv.org/abs/2312.10763) | M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts                                                           |   2023 |             0.8339 |           1 |     2 |   0.003041 | The paper introduces M3DBench, a comprehensive multi-modal 3D instruction-following dataset that supports diverse 3D tasks, interleaved visual...        | Unlike prior 3D datasets focused on specific tasks (e.g., object detection, navigation), M3DBench unifies region and scene-level 3D tasks with...        |
|      8 | [![arXiv](https://img.shields.io/badge/arXiv-2309.11419-b31b1b.svg)](https://arxiv.org/abs/2309.11419) | KOSMOS-2.5: A Multimodal Literate Model                                                                                     |   2023 |             0.8095 |          14 |     2 |   0.002952 | KOSMOS-2.5 introduces a multimodal literate model for document-level text recognition and structured output generation, combining spatial-aware text...  | Unlike prior work focused on line-level OCR or domain-specific structured parsing, KOSMOS-2.5 unifies spatial and structural document understanding...   |
|      9 | [![arXiv](https://img.shields.io/badge/arXiv-1802.08802-b31b1b.svg)](https://arxiv.org/abs/1802.08802) | Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration                                                  |   2018 |             0.8039 |          32 |     7 |   0.001044 | The paper introduces workflow-guided exploration for reinforcement learning on web interfaces, using high-level workflows derived from expert...         | This work differs from related work by using workflow-guided exploration to constrain action sequences with expert demonstrations, rather than...        |
|     10 | [![arXiv](https://img.shields.io/badge/arXiv-2305.16291-b31b1b.svg)](https://arxiv.org/abs/2305.16291) | Voyager: An Open-Ended Embodied Agent with Large Language Models                                                            |   2023 |             0.8031 |         165 |     2 |   0.002929 | Voyager introduces an LLM-powered embodied lifelong learning agent for open-ended environments like Minecraft, featuring an automatic...                 | Unlike prior works that rely on fixed curricula or require human interaction, Voyager enables open-ended exploration through a bottom-up...              |
|     11 | [![arXiv](https://img.shields.io/badge/arXiv-1902.07257-b31b1b.svg)](https://arxiv.org/abs/1902.07257) | DOM-Q-NET: Grounded RL on Structured Language                                                                               |   2019 |             0.7402 |           1 |     6 |   0.001044 | The paper addresses challenges in web navigation for RL agents by introducing DOM-Q-NET, which handles large discrete action spaces and varying...       | DOM-Q-NET differs from related work by using a structured representation of HTML via graph neural networks and separating action-specific Q-networks...  |
|     12 | [![arXiv](https://img.shields.io/badge/arXiv-1909.01871-b31b1b.svg)](https://arxiv.org/abs/1909.01871) | Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning |   2019 |             0.7402 |           3 |     6 |   0.001044 | The paper introduces HANNA, a photo-realistic simulator for visual navigation tasks with human-like assistance. It proposes a memory-augmented neural... | This work differs from related work by integrating multimodal assistance (language + vision) with retrospective curiosity-driven imitation learning,...  |
|     13 | [![arXiv](https://img.shields.io/badge/arXiv-2305.08144-b31b1b.svg)](https://arxiv.org/abs/2305.08144) | Mobile-Env: Building Qualified Evaluation Benchmarks for LLM-GUI Interaction                                                |   2023 |             0.7317 |          14 |     2 |   0.002668 | The paper introduces Mobile-Env, a comprehensive toolkit for creating qualified GUI interaction benchmarks in Android environments. It addresses...      | Mobile-Env differs from existing benchmarks by offering a controllable and isolated environment with support for intermediate rewards and...             |
|     14 | [![arXiv](https://img.shields.io/badge/arXiv-2210.03347-b31b1b.svg)](https://arxiv.org/abs/2210.03347) | Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding                                             |   2022 |             0.711  |           9 |     3 |   0.001636 | The paper introduces Pix2Struct, a pretrained image-to-text model for visual language understanding, focusing on parsing screenshots into HTML. It...    | Pix2Struct differs from prior work by using HTML structure as a pretraining target for visual parsing, enabling OCR-free end-to-end understanding. It... |
|     15 | [![arXiv](https://img.shields.io/badge/arXiv-2311.12871-b31b1b.svg)](https://arxiv.org/abs/2311.12871) | An Embodied Generalist Agent in 3D World                                                                                    |   2023 |             0.7043 |           7 |     2 |   0.002568 | The paper introduces LEO, an embodied multi-modal generalist agent designed for 3D world interaction, addressing limitations in 3D perception,...        | LEO extends beyond existing 2D-focused generalist models by introducing a unified 3D vision-language-action architecture, a novel 3D dataset with...     |
|     16 | [![arXiv](https://img.shields.io/badge/arXiv-2307.12856-b31b1b.svg)](https://arxiv.org/abs/2307.12856) | A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis                                      |   2023 |             0.6687 |          66 |     2 |   0.002438 | The paper introduces WebAgent, an LLM-driven autonomous agent for real-world web automation that addresses open-domainness, long-context HTML...         | Unlike prior works relying on simulated environments or single LLMs, WebAgent combines HTML-T5 (specialized for HTML with novel attention mechanisms)... |
|     17 | [![arXiv](https://img.shields.io/badge/arXiv-2008.08899-b31b1b.svg)](https://arxiv.org/abs/2008.08899) | Document Visual Question Answering Challenge 2020                                                                           |   2020 |             0.6649 |           1 |     5 |   0.001044 | The paper introduces a new problem in visual question answering (VQA) focused on document images, proposing two tasks: single-image question...          | This work differs from related work by specifically addressing VQA on document images, which is distinct from general-purpose VQA tasks. It...           |
|     18 | [![arXiv](https://img.shields.io/badge/arXiv-2010.03768-b31b1b.svg)](https://arxiv.org/abs/2010.03768) | ALFWorld: Aligning Text and Embodied Environments for Interactive Learning                                                  |   2020 |             0.6649 |          45 |     5 |   0.001044 | ALFWorld bridges abstract text-based policy learning and concrete visual execution, introducing the BUTLER agent that leverages pre-learned abstract...  | Unlike prior work focused on either abstract reasoning or visual execution, ALFWorld combines both through a unified simulator, enabling agents to...    |
|     19 | [![arXiv](https://img.shields.io/badge/arXiv-2008.05132-b31b1b.svg)](https://arxiv.org/abs/2008.05132) | Object Detection for Graphical User Interface: Old Fashioned or Deep Learning or a Combination?                             |   2020 |             0.6649 |           1 |     5 |   0.001044 | The paper conducts a large-scale empirical study on GUI element detection methods, identifies limitations of existing approaches, and proposes a...      | The work differs by introducing a GUI-specific old-fashioned method with a top-down coarse-to-fine strategy, integrating it with deep learning for...    |
|     20 | [![arXiv](https://img.shields.io/badge/arXiv-2007.04954-b31b1b.svg)](https://arxiv.org/abs/2007.04954) | ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation                                                     |   2020 |             0.6649 |           7 |     5 |   0.001044 | ThreeDWorld introduces a platform for interactive multi-modal physical simulation with high-fidelity rendering, customizable agents, generative...       | Unlike prior work, ThreeDWorld provides a unified platform with real-time physics simulation, multi-modal sensory data, and customizable agents,...      |


The foundation papers in GUI agent research represent a pivotal period (2016-2020) that established the fundamental approaches, datasets, and benchmarks that continue to influence the field today. These 14 papers collectively laid the groundwork for modern GUI automation, web interaction, mobile UI understanding, and embodied AI systems. This analysis examines their unique contributions and lasting impact on the research landscape.

## Thematic Analysis of Foundation Papers

### 1. **Data-Driven GUI Understanding Revolution (2016-2017)**

**ERICA: Interaction Mining Mobile Apps (2016)** and **Rico: A Mobile App Dataset (2017)** initiated the data-driven revolution in GUI research.

- **ERICA's Innovation**: Introduced the first scalable system for mining dynamic design data from mobile apps without code modifications. Unlike previous manual approaches, ERICA provided automated interaction trace collection and machine learning-based indexing of user flows.

- **Rico's Paradigm Shift**: Unlike static datasets like ImageNet, Rico dynamically captured UI interactions and states through runtime mining. It employed content-agnostic similarity heuristics for UI clustering and autoencoder-generated embeddings, enabling scalable real-world UI analysis without source code access.

**Impact**: These papers established the foundation for large-scale, data-driven approaches to GUI understanding that dominate current research.

### 2. **Deep Learning for Code Generation (2017)**

**pix2code: Generating Code from a Graphical User Interface Screenshot (2017)** marked a watershed moment in automated code generation from visual interfaces.

- **Revolutionary Approach**: Leveraged deep learning for direct code generation from screenshots, achieving over 77% accuracy across iOS, Android, and web platforms. This differed fundamentally from prior rule-based systems or manual coding approaches.

- **End-to-End Learning**: Introduced the concept of treating GUI-to-code conversion as a computer vision problem, eliminating intermediate steps like UI element parsing.

**Legacy**: This work directly inspired modern approaches like WebSight, Sightseer, and contemporary multimodal LLMs for code generation.

### 3. **Reinforcement Learning for Web Interaction (2018)**

**Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration (2018)** addressed the critical challenge of sparse rewards in web-based RL.

- **Workflow-Guided Innovation**: Instead of behavioral cloning (prone to overfitting), introduced workflow constraints derived from expert demonstrations to guide exploration. This approach pruned bad exploration directions while maintaining learning flexibility.

- **Architectural Contribution**: Presented a novel neural policy designed specifically for semi-structured websites, achieving 100x improvement in sample efficiency over behavioral cloning.

**Significance**: Established the foundation for structured exploration in web agents, influencing current approaches in WebArena, MiniWoB++, and modern web automation frameworks.

### 4. **Grounded Language-Action Learning (2019-2020)**

**DOM-Q-NET: Grounded RL on Structured Language (2019)** and **Mapping Natural Language Instructions to Mobile UI Action Sequences (2020)** pioneered language-grounded GUI interaction.

- **DOM-Q-NET's Contribution**: Addressed large discrete action spaces and varying action counts through specialized Q-networks for clicking and typing, using graph neural networks to model HTML structure. Achieved 2x sample efficiency improvement in multi-task settings.

- **Mobile UI Grounding**: Created the first comprehensive framework for mapping natural language to mobile actions, introducing PIXELHELP dataset and dual-Transformer architecture for contextual UI object representation.

**Impact**: These approaches directly influenced modern instruction-following GUI agents and multimodal interaction systems.

### 5. **Multimodal Assistance and Navigation (2019)**

**Help, Anna! Visual Navigation with Natural Multimodal Assistance (2019)** introduced human-AI collaboration in embodied tasks.

- **Multimodal Integration**: Combined language and visual assistance through simulated human assistants (ANNA), using retrospective curiosity-driven imitation learning.

- **Hierarchical Decision-Making**: Implemented memory-augmented neural agents with multiple decision-making levels, achieving better performance on both seen and unseen environments.

**Relevance**: Presaged current trends in AI assistant systems and human-in-the-loop learning.

### 6. **GUI Element Detection Methodology (2020)**

**Object Detection for Graphical User Interface: Old Fashioned or Deep Learning or a Combination? (2020)** provided crucial empirical foundations.

- **Systematic Evaluation**: Conducted the first large-scale empirical study of GUI element detection methods on 50k+ images, revealing limitations of computer vision approaches when applied to GUI-specific tasks.

- **Hybrid Innovation**: Introduced a GUI-specific method combining traditional image processing (top-down coarse-to-fine strategy) with deep learning for text detection, significantly advancing state-of-the-art performance.

**Foundation**: Established methodological rigor and domain-aware design principles that guide current GUI perception research.

### 7. **Embodied AI Infrastructure (2020)**

The 2020 papers established critical infrastructure for embodied AI:

**ALFWorld: Aligning Text and Embodied Environments (2020)**:
- **Bridge Building**: Uniquely combined abstract text-based policy learning (TextWorld) with concrete visual execution (ALFRED), enabling transfer of abstract knowledge to visual tasks.
- **Modular Design**: Created infrastructure for researching language understanding, planning, navigation, and visual scene understanding independently.

**ThreeDWorld: Interactive Multi-Modal Physical Simulation (2020)**:
- **Comprehensive Platform**: Provided real-time near-photo-realistic rendering, high-fidelity audio, realistic physics for diverse materials (cloths, liquids, deformable objects), and VR support.
- **Research Enabler**: Facilitated research in multi-modal physical scene understanding, dynamics prediction, and multi-agent interactions.

**robosuite: Modular Simulation Framework and Benchmark (2020)**:
- **Standardization**: Introduced modular design with standardized benchmarks, high-quality controller implementations, and reproducible evaluation protocols.
- **Accessibility**: Lowered entry barriers for robotics research through comprehensive tooling and documentation.

### 8. **Specialized Benchmarking (2020)**

**Document Visual Question Answering Challenge 2020** and **DocVQA: A Dataset for VQA on Document Images** established document understanding benchmarks.

- **Domain Specialization**: Focused specifically on document images, requiring structural understanding beyond general VQA tasks.
- **Challenge Definition**: Introduced both single-document and retrieval-based tasks, highlighting the performance gap between models and human understanding (94.36% human accuracy).

**Rearrangement: A Challenge for Embodied AI (2020)**:
- **Task Unification**: Proposed rearrangement as a canonical task for embodied AI, providing standardized metrics and scenario characterization.
- **Framework Establishment**: Created testbeds across four simulation environments, enabling systematic research and evaluation.

## Cross-Cutting Themes and Innovations

### 1. **Data-Centric Approaches**
Foundation papers consistently emphasized the importance of high-quality, large-scale datasets. From ERICA's interaction mining to Rico's dynamic UI capture, these works established that GUI understanding requires domain-specific data collection strategies.

### 2. **Domain-Aware Design**
Rather than blindly applying computer vision techniques, foundation papers recognized GUI-specific challenges. The GUI object detection study explicitly highlighted this need, while DOM-Q-NET's graph-based HTML representation exemplified domain-aware architectural choices.

### 3. **Multimodal Integration**
Early recognition of the need to combine visual, textual, and structural information. Papers like HANNA's multimodal assistance and ALFWorld's text-visual alignment presaged current multimodal LLM approaches.

### 4. **Structured Exploration and Learning**
Foundation papers moved beyond naive approaches to incorporate structure. Workflow-guided exploration, hierarchical decision-making, and modular architectures became recurring themes that continue to influence modern research.

### 5. **Benchmark and Infrastructure Development**
Recognizing that progress requires standardized evaluation, foundation papers invested heavily in creating benchmarks, datasets, and simulation platforms that continue to serve the research community.

## Lasting Impact and Current Relevance

### Methodological Foundations
- **End-to-End Learning**: pix2code's approach directly influenced modern screenshot-to-code systems
- **Structured Exploration**: Workflow-guided RL principles appear in current web agents
- **Domain-Aware Design**: GUI-specific architectural choices remain crucial
- **Multimodal Integration**: Early multimodal approaches presaged current LLM capabilities

### Data and Benchmarking Legacy
- **Rico Dataset**: Continues to be used for mobile UI research
- **MiniWoB++**: Remains a standard web interaction benchmark
- **ALFWorld**: Influences current embodied AI evaluation
- **robosuite**: Active platform for robotics research

### Architectural Innovations
- **Graph-based HTML Representation**: Adopted in modern web agents
- **Dual-network Architectures**: Influences current GUI interaction models
- **Memory-augmented Agents**: Appears in contemporary AI assistant systems
- **Modular Simulation Design**: Standard practice in current platforms

## Gaps and Evolution

### What Foundation Papers Missed
1. **Large Language Model Integration**: Pre-LLM era approaches focused on specialized architectures
2. **Vision-Language Model Capabilities**: Limited multimodal understanding compared to modern VLMs
3. **Real-world Deployment Challenges**: Primarily simulation-focused
4. **Scalability to Modern Web Complexity**: Static approaches vs. dynamic modern web applications

### How Current Research Builds Upon Foundations
1. **LLM-Powered Agents**: Combine foundation paper insights with large-scale language model capabilities
2. **Vision-Language Integration**: Modern VLMs address multimodal challenges identified by foundation papers
3. **Real-world Applications**: Current research addresses deployment challenges foundation papers identified but couldn't solve
4. **Dynamic Adaptation**: Modern agents handle the complexity and variability that foundation papers recognized as key challenges

## Conclusion

The foundation papers of 2016-2020 established GUI agent research as a distinct field with its own methodologies, benchmarks, and architectural principles. Their emphasis on data-driven approaches, domain-aware design, structured learning, and comprehensive evaluation continues to guide current research. While modern LLMs and VLMs have transformed the field's capabilities, the fundamental insights about GUI understanding, interaction modeling, and evaluation established by these foundation papers remain highly relevant.

The field's current trajectory—toward more capable multimodal agents, real-world deployment, and human-AI collaboration—directly builds upon the conceptual and methodological foundations established by these seminal works. Understanding their contributions provides crucial context for appreciating both how far the field has progressed and the enduring principles that continue to drive innovation in GUI agent research.

*Generated on: 2025-08-15 09:32:10*
*Total entries: 20*
