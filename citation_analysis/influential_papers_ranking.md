# Top 20 Most Influential Papers by PageRank Score

|   Rank | ArXiv                                                                                                  | Title                                                                                                                     |   Year |   Citations |   PageRank |   Betweenness | Platforms                         | Summary                                                                                                                                                  | Contributions                                                                                                                                           |
|--------|--------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|--------|-------------|------------|---------------|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|
|      1 | [![arXiv](https://img.shields.io/badge/arXiv-2412.05271-b31b1b.svg)](https://arxiv.org/abs/2412.05271) | Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling                 |   2024 |          22 |   0.012942 |        0.0015 | Mobile, Desktop                   | The paper introduces InternVL 2.5, an advanced open-source multimodal large language model (MLLM) that systematically explores model scaling,...         | This work differs from related research by focusing on systematic analysis of model, data, and test-time scaling factors in MLLMs, achieving...         |
|      2 | [![arXiv](https://img.shields.io/badge/arXiv-2502.17110-b31b1b.svg)](https://arxiv.org/abs/2502.17110) | Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation |   2025 |           3 |   0.009771 |        0      | Android, iOS                      | The paper introduces Mobile-Agent-V, a video-guided framework for injecting operational knowledge into mobile automation, addressing limitations in...   | Unlike prior work reliant on manual textual knowledge or limited training data, Mobile-Agent-V leverages raw video to extract procedural knowledge,...  |
|      3 | [![arXiv](https://img.shields.io/badge/arXiv-2503.09572-b31b1b.svg)](https://arxiv.org/abs/2503.09572) | Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks                                                         |   2025 |           2 |   0.009207 |        0      | Mobile, Web                       | The paper introduces Plan-and-Act, a framework that separates high-level planning from low-level execution for long-horizon tasks. It addresses...       | Plan-and-Act introduces a scalable synthetic data generation method for training planners, a simpler 2-agent architecture compared to prior works...    |
|      4 | [![arXiv](https://img.shields.io/badge/arXiv-2503.05143-b31b1b.svg)](https://arxiv.org/abs/2503.05143) | FedMABench: Benchmarking Mobile Agents on Decentralized Heterogeneous User Data                                           |   2025 |           2 |   0.00907  |        0      | Mobile, iOS                       | The paper introduces FedMABench, the first benchmark for federated mobile agent training, addressing the lack of standardized benchmarks in...           | FedMABench fills the gap in federated mobile agent research by providing the first comprehensive benchmark with diverse datasets, federated...          |
|      5 | [![arXiv](https://img.shields.io/badge/arXiv-2504.10127-b31b1b.svg)](https://arxiv.org/abs/2504.10127) | Breaking the Data Barrier -- Building GUI Agents Through Task Generalization                                              |   2025 |           2 |   0.00748  |        0.0001 | Android, Web and Android (Mobile) | The paper introduces a method to enhance GUI agents through task generalization by leveraging non-GUI tasks like mathematical reasoning and textual...   | This work differs from related work by demonstrating that non-GUI tasks (e.g., mathematical reasoning) significantly improve GUI agent performance...   |
|      6 | [![arXiv](https://img.shields.io/badge/arXiv-2410.18967-b31b1b.svg)](https://arxiv.org/abs/2410.18967) | Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms                                            |   2024 |          17 |   0.007382 |        0.0035 | Android, iOS                      | Ferret-UI 2 introduces multi-platform UI understanding, high-resolution adaptive scaling, and advanced data generation with GPT-4o visual prompting....  | Ferret-UI 2 differs from prior work by supporting multiple platforms (iPhone, Android, Web, AppleTV), enabling high-resolution adaptive perception,...  |
|      7 | [![arXiv](https://img.shields.io/badge/arXiv-2504.13805-b31b1b.svg)](https://arxiv.org/abs/2504.13805) | LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration Benchmark                                                |   2025 |           3 |   0.007173 |        0      | Mobile, iOS                       | The paper introduces LearnGUI, the first comprehensive dataset for demonstration-based learning in mobile GUI agents, and LearnAct, a multi-agent...     | Unlike prior static datasets and dynamic benchmarks, LearnGUI provides high-quality human demonstrations for both offline and online tasks, while...    |
|      8 | [![arXiv](https://img.shields.io/badge/arXiv-2504.13936-b31b1b.svg)](https://arxiv.org/abs/2504.13936) | ViMo: A Generative Visual GUI World Model for App Agents                                                                  |   2025 |           2 |   0.006917 |        0      | Mobile, Web                       | ViMo introduces a generative visual GUI world model for App agents, addressing long-horizon planning challenges by predicting future GUI observations... | ViMo differs from prior work by introducing a visual modality for GUI prediction (vs. text-based descriptions), employing a novel STR architecture...   |
|      9 | [![arXiv](https://img.shields.io/badge/arXiv-2404.04619-b31b1b.svg)](https://arxiv.org/abs/2404.04619) | Do We Really Need a Complex Agent System? Distill Embodied Agent into a Single Model                                      |   2024 |           6 |   0.006597 |        0      | iOS, Mobile                       | The paper introduces STEVE-2, a hierarchical knowledge distillation framework for open-ended embodied agents, addressing limitations in multi-LLM...     | STEVE-2 differs from prior work by distilling complex multi-agent systems into a single model with hierarchical architecture, enabling dynamic...       |
|     10 | [![arXiv](https://img.shields.io/badge/arXiv-2411.17465-b31b1b.svg)](https://arxiv.org/abs/2411.17465) | ShowUI: One Vision-Language-Action Model for GUI Visual Agent                                                             |   2024 |          15 |   0.006166 |        0.0028 | Web, Mobile                       | ShowUI introduces a vision-language-action model for GUI agents with three key innovations: (1) UI-Guided Visual Token Selection to reduce...            | ShowUI advances GUI agents by addressing limitations of text-based APIs and prior vision-language models through novel architectural components...      |
|     11 | [![arXiv](https://img.shields.io/badge/arXiv-2410.19100-b31b1b.svg)](https://arxiv.org/abs/2410.19100) | VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks                               |   2024 |           4 |   0.006159 |        0.0001 | Mobile, macOS                     | Introduces VideoWebArena (VideoWA), a benchmark for evaluating long-context multimodal agents on video understanding tasks. Focuses on skill...          | This work addresses gaps in existing benchmarks by creating VideoWebArena, a comprehensive dataset and benchmark specifically designed to evaluate...   |
|     12 | [![arXiv](https://img.shields.io/badge/arXiv-2404.10179-b31b1b.svg)](https://arxiv.org/abs/2404.10179) | Scaling Instructable Agents Across Many Simulated Worlds                                                                  |   2024 |           3 |   0.005929 |        0      | Mobile, Desktop                   | The paper introduces the SIMA project, which develops agents capable of following arbitrary language instructions across diverse 3D environments. Key... | Unlike prior work focused on specific games or environments, SIMA emphasizes training agents across a wide range of 3D worlds (including commercial...  |
|     13 | [![arXiv](https://img.shields.io/badge/arXiv-2503.15937-b31b1b.svg)](https://arxiv.org/abs/2503.15937) | Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment                                           |   2025 |           3 |   0.005925 |        0.0002 | Windows, Mobile                   | V-Droid introduces a verifier-driven paradigm for mobile GUI agents, employing LLMs as verifiers rather than generators. Key contributions include...    | Unlike prior LLM-powered agents that use generators for action creation, V-Droid uses LLMs as verifiers to evaluate candidate actions, combined with... |
|     14 | [![arXiv](https://img.shields.io/badge/arXiv-2502.13130-b31b1b.svg)](https://arxiv.org/abs/2502.13130) | Magma: A Foundation Model for Multimodal AI Agents                                                                        |   2025 |           4 |   0.005725 |        0.0001 | Mobile, Robotics                  | Magma introduces a foundation model for multimodal AI agents that integrates verbal intelligence (vision-language understanding) with...                 | Magma differs from related work by unifying multimodal understanding with spatial-temporal action planning through SoM/ToM pretraining, enabling...     |
|     15 | [![arXiv](https://img.shields.io/badge/arXiv-2406.14596-b31b1b.svg)](https://arxiv.org/abs/2406.14596) | VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought                           |   2024 |           3 |   0.005503 |        0.0014 | Windows, Mobile                   | The paper introduces ICAL, a method that refines suboptimal trajectories into high-quality multimodal programs of thought using VLM self-refinement...   | Unlike prior text-based methods that lack visual cues or introspection, ICAL leverages VLMs to generate causal reasoning and subgoals from noisy...     |
|     16 | [![arXiv](https://img.shields.io/badge/arXiv-2502.06395-b31b1b.svg)](https://arxiv.org/abs/2502.06395) | AppVLM: A Lightweight Vision Language Model for Online App Control                                                        |   2025 |           5 |   0.005496 |        0.0001 | Android, macOS                    | The paper introduces AppVLM, a lightweight Vision-Language Model (VLM) designed for efficient app control on smartphones. It addresses limitations of... | AppVLM differs from related work by combining offline dataset fine-tuning with online environment data collection for policy refinement, achieving...   |
|     17 | [![arXiv](https://img.shields.io/badge/arXiv-2408.11824-b31b1b.svg)](https://arxiv.org/abs/2408.11824) | AppAgent v2: Advanced Agent for Flexible Mobile Interactions                                                              |   2024 |          17 |   0.005489 |        0.001  | Mobile, iOS                       | The paper introduces AppAgent v2, a novel LLM-based multimodal framework for mobile devices that enhances GUI interaction through a structured...        | Unlike prior work, AppAgent v2 introduces a structured knowledge base with RAG-enabled retrieval, a two-phase exploration-deployment framework, and...  |
|     18 | [![arXiv](https://img.shields.io/badge/arXiv-2503.17352-b31b1b.svg)](https://arxiv.org/abs/2503.17352) | OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles                                              |   2025 |           4 |   0.005304 |        0      | Mobile                            | Introduces OpenVLThinker, an open-source large vision-language model (LVLM) that combines iterative supervised fine-tuning (SFT) and reinforcement...    | Proposes an iterative SFT-RL framework for LVLMs, overcoming challenges of visual grounding and search space inefficiencies in reasoning tasks....      |
|     19 | [![arXiv](https://img.shields.io/badge/arXiv-2407.01476-b31b1b.svg)](https://arxiv.org/abs/2407.01476) | Tree Search for Language Model Agents                                                                                     |   2024 |          35 |   0.005273 |        0.0039 | Mobile, macOS                     | The paper introduces a tree search algorithm for language model (LM) agents to enhance multi-step planning and exploration in interactive web...         | This work differs from related work by introducing the first tree search algorithm specifically tailored for LM agents in realistic web tasks,...       |
|     20 | [![arXiv](https://img.shields.io/badge/arXiv-2504.07981-b31b1b.svg)](https://arxiv.org/abs/2504.07981) | ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use                                               |   2025 |           2 |   0.005251 |        0.0001 | Mobile, Web                       | This paper introduces ScreenSpot-Pro, a benchmark for evaluating GUI grounding in high-resolution professional environments, and proposes...             | The work differs from related work by focusing on high-resolution professional applications, introducing a rigorous benchmark (ScreenSpot-Pro) with...  |

---
*Generated on: 2025-08-15 09:32:10*
*Total entries: 20*
